<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title></title>
      <link href="/2024/12/14/%E6%97%A5%E7%A8%8B/%E6%89%93%E5%8D%A1/"/>
      <url>/2024/12/14/%E6%97%A5%E7%A8%8B/%E6%89%93%E5%8D%A1/</url>
      
        <content type="html"><![CDATA[<h1 id="打卡">打卡</h1><h2 id="算法训练营-滴答清单-10-30-开始">算法训练营（滴答清单 10.30 开始）</h2><blockquote><p>10月30日开营</p></blockquote><h2 id="天池ai训练营">天池AI训练营</h2><blockquote><p>11月3日 主动开营</p></blockquote><p><a href="https://tianchi.aliyun.com/specials/promotion/aicampdl?accounttraceid=2e4e898a17ac43dab6a0e2b5d55b4655netp">AI训练营深度学习-阿里云天池</a></p><h2 id="宿舍生活打卡-21-天">宿舍生活打卡 21 天</h2><blockquote><p>自此开始</p></blockquote><ul><li>[ ] 第11天</li></ul><h2 id="leetcode-打卡">LeetCode 打卡</h2><blockquote><p>自此开始</p></blockquote><ul><li>[x] 第33天</li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2024/12/14/%E6%97%A5%E7%A8%8B/%E6%8A%80%E6%9C%AF%E6%8A%A5%E5%91%8A/"/>
      <url>/2024/12/14/%E6%97%A5%E7%A8%8B/%E6%8A%80%E6%9C%AF%E6%8A%A5%E5%91%8A/</url>
      
        <content type="html"><![CDATA[<h1 id="reposyncer跨平台同步项目贡献赛-技术报告">RepoSyncer跨平台同步项目贡献赛 技术报告</h1><h2 id="作品设计方案">作品设计方案</h2><h3 id="1-项目背景">1. 项目背景</h3><p>本项目旨在实现 <code>GitHub</code>、<code>Gitee</code>和 <code>Gitlink </code>平台之间的仓库同步功能，包括仓库代码同步、<code>issue </code>同步和 <code>PR </code>同步等。项目主要解决在多平台协作开发中，保持代码和<code> issue</code>、<code>PR </code>状态一致的问题。目前<code>RepoSyncer</code>项目已完成代码提交的双向同步功能，还需要进一步丰富实现包括<code>PR、Issue</code>等的双向自动同步功能。</p><h3 id="2-功能需求">2. 功能需求</h3><ul><li><strong>仓库代码同步</strong>：原项目已通过<code>Git</code>实现。</li><li><strong>issue 同步</strong>：在平台间同步<code>issue</code>的创建、更新、删除等操作。</li><li><strong>PR 同步</strong>：在平台之间同步<code>PR</code>的创建、更新、合并等操作。</li></ul><h3 id="3-系统架构">3. 系统架构</h3><p>系统采用<code>FastAPI</code>框架进行后端开发，前端使用<code>AntV+React</code>，数据库使用 <code>MySQL</code>。通过定时任务或手动触发的方式进行同步操作。</p><p><img src="D:%5C%E4%BD%9C%E4%B8%9A%5C%E5%BC%80%E6%BA%90%E5%A4%A7%E8%B5%9B%5C%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E5%9B%BE.png" alt="系统架构图"></p><h2 id="实现方案">实现方案</h2><blockquote><p>经过API和Git相关的资料查阅，由于考虑到权限安全，目前Gitee、GitHub均不支持issue、PR的删除功能，所以无法实现删除功能。</p><p>刚开始未找到Gitlink的相关API，所以没有做相关实现。</p></blockquote><h3 id="1-issue-同步">1. issue 同步</h3><h4 id="实现思路">实现思路</h4><ul><li>使用 <code>Gitee API </code>和<code> PyGithub</code> 包分别拉取和更新<code> Gitee</code> 和 <code>GitHub </code>平台的<code> issue</code>。</li><li>单向同步的实现：创建<code> sync_issue</code> 数据库表，用于存储<code>issue</code>的映射关系，通过映射表完成<code>issue</code>的同步。</li></ul><img src="D:\作业\开源大赛\issue同步.png" width="600"><h4 id="数据库表结构">数据库表结构</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> sync_issue (</span><br><span class="line">    id <span class="type">INT</span> AUTO_INCREMENT <span class="keyword">PRIMARY</span> KEY,</span><br><span class="line">    title <span class="type">VARCHAR</span>(<span class="number">255</span>) <span class="keyword">NOT</span> <span class="keyword">NULL</span> COMMENT <span class="string">&#x27;issue 标题&#x27;</span>,</span><br><span class="line">    body <span class="type">VARCHAR</span>(<span class="number">255</span>) COMMENT <span class="string">&#x27;issue 内容&#x27;</span>,</span><br><span class="line">    assignee <span class="type">VARCHAR</span>(<span class="number">255</span>) COMMENT <span class="string">&#x27;issue 负责人&#x27;</span>,</span><br><span class="line">    labels <span class="type">VARCHAR</span>(<span class="number">255</span>) COMMENT <span class="string">&#x27;issue 标签&#x27;</span>,</span><br><span class="line">    inter_number <span class="type">VARCHAR</span>(<span class="number">255</span>) <span class="keyword">NOT</span> <span class="keyword">NULL</span> COMMENT <span class="string">&#x27;inter 仓库的issue id&#x27;</span>,</span><br><span class="line">    exter_number <span class="type">VARCHAR</span>(<span class="number">255</span>) <span class="keyword">NOT</span> <span class="keyword">NULL</span> COMMENT <span class="string">&#x27;exter 仓库的issue id&#x27;</span>,</span><br><span class="line">    state <span class="type">VARCHAR</span>(<span class="number">255</span>) <span class="keyword">NOT</span> <span class="keyword">NULL</span> <span class="keyword">DEFAULT</span> <span class="string">&#x27;open&#x27;</span> COMMENT <span class="string">&#x27;issue 状态&#x27;</span>,</span><br><span class="line">    repo_name <span class="type">VARCHAR</span>(<span class="number">255</span>) <span class="keyword">NOT</span> <span class="keyword">NULL</span> COMMENT <span class="string">&#x27;issue 所在同步仓库&#x27;</span></span><br><span class="line">) <span class="keyword">DEFAULT</span> <span class="type">CHARACTER</span> <span class="keyword">SET</span> <span class="operator">=</span> utf8mb4 COMMENT <span class="operator">=</span> <span class="string">&#x27;同步issue映射表&#x27;</span>;</span><br></pre></td></tr></table></figure><h4 id="功能实现">功能实现</h4><ul><li>在<code> Sync Repo</code> 接口<code>src\service\cronjob.py</code>中添加 issue 同步功能，请求体添加参数<code> sync_issue</code>，true 表示启用issue同步，false表示不启用。</li><li>在<code> sync_repo_task</code> <code>src\api\Sync_config.py</code>并列添加 <code>sync_issue_task </code>函数。</li><li>在相关 do、service 层添加 MySQL 的相关实体<code>SyncIssue</code>及其函数定义。</li></ul><h3 id="2-pr-同步">2. PR 同步</h3><h4 id="实现思路">实现思路</h4><blockquote><p>与Issue同步类似</p></blockquote><ul><li>使用 Gitee API 和 PyGithub 包分别拉取和更新 Gitee 和 GitHub 平台的 PR。</li><li>单向同步的实现：创建 sync_pr 数据库表，用于存储 PR 的映射关系。</li></ul><h4 id="数据库表结构">数据库表结构</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> sync_pullrequest (</span><br><span class="line">    id <span class="type">INT</span> AUTO_INCREMENT <span class="keyword">PRIMARY</span> KEY,</span><br><span class="line">    inter_number <span class="type">VARCHAR</span>(<span class="number">128</span>) <span class="keyword">NOT</span> <span class="keyword">NULL</span> COMMENT <span class="string">&#x27;inter 仓库的 pr number&#x27;</span>,</span><br><span class="line">    exter_number <span class="type">VARCHAR</span>(<span class="number">128</span>) <span class="keyword">NOT</span> <span class="keyword">NULL</span> COMMENT <span class="string">&#x27;exter 仓库的 pr number&#x27;</span>,</span><br><span class="line">    updated_at <span class="type">VARCHAR</span>(<span class="number">128</span>) <span class="keyword">NOT</span> <span class="keyword">NULL</span> COMMENT <span class="string">&#x27;更新时间&#x27;</span>,</span><br><span class="line">    repo_name <span class="type">VARCHAR</span>(<span class="number">128</span>) <span class="keyword">NOT</span> <span class="keyword">NULL</span> COMMENT <span class="string">&#x27;pr 所在同步仓库&#x27;</span></span><br><span class="line">) <span class="keyword">DEFAULT</span> <span class="type">CHARACTER</span> <span class="keyword">SET</span> <span class="operator">=</span> utf8mb4 COMMENT <span class="operator">=</span> <span class="string">&#x27;同步issue映射表&#x27;</span>;</span><br></pre></td></tr></table></figure><h4 id="功能实现">功能实现</h4><ul><li>在 <code>Sync Repo</code> 接口中添加 PR 同步功能，请求体添加参数 <code>sync_pr</code>，true 表示启用pr同步，false表示不启用。</li><li>在 <code>sync_repo_task </code>并列添加 <code>sync_pr_task</code> 函数。</li><li>在相关 do、service 层添加 MySQL 的相关实体 <code>SyncPr </code>及其函数定义。</li></ul><h2 id="运行效果-测试结果">运行效果/测试结果</h2><h3 id="1-issue-单向同步">1. issue 单向同步</h3><ul><li><p><strong>测试步骤</strong>：</p><ul><li>运行前端后端</li><li>创建<code>Gitee</code>、<code>Github</code>空白仓库，在前端导入同步项目配置</li><li><code>Gitee</code>上传代码、新建<code>issue</code>，前端点击同步，同步完成后检查</li><li><code>Github</code>上传代码、新建<code>issue</code>，前端修改同步方向，点击同步，同步完成后检查</li></ul></li><li><p><strong>预期结果</strong>：两个平台的 <code>issue</code> 状态保持一致。</p></li><li><p><strong>实际结果</strong>：<code>issue </code>同步功能正常工作，两个平台的 <code>issue</code> 状态保持一致。</p></li></ul><h3 id="2-pr-单向同步">2. PR 单向同步</h3><ul><li><p><strong>测试步骤</strong>：</p><ul><li>在<code>issue</code>测试的基础上进行</li><li><code>Gitee</code>新建分支<code>test</code>，创建<code>PR</code>，前端修改同步方向，点击同步，同步完成后检查</li><li><code>Gitee</code>将<code>PR</code>合并，点击同步，同步完成后检查</li></ul></li><li><p><strong>预期结果</strong>：两个平台的 <code>PR</code> 状态保持一致，并<code>PR</code>成功合并。</p></li><li><p><strong>实际结果</strong>：<code>PR</code> 同步功能正常工作，两个平台的 <code>PR</code>状态保持一致。</p></li></ul><h2 id="问题">问题</h2><h3 id="1-pr-同步合并后代码-commit-id不一致的问题">1. PR 同步合并后代码 Commit Id不一致的问题</h3><p>当你在 <code>GitHub</code> 上合并一个 <code>Pull Request (PR)</code> 时，<code>GitHub</code> 会自动生成一个新的<code> commit ID</code>，这个<code> commit</code> 代表了合并操作。这个<code>commit</code>通常称为“合并提交”（<code>merge commit</code>）。</p><img src="D:\作业\开源大赛\矛盾.png" width="600"><p>所以这里暂时提供了几种解决策略：</p><h3 id="使用单一源">使用单一源</h3><p>选择一个仓库作为主要的开发仓库（例如 GitHub），所有的开发和 PR 合并都在这个仓库进行。然后，将这个仓库的变更定期同步到另一个仓库（例如 Gitee）。这样可以确保所有的合并提交都首先在主仓库中创建，然后通过同步操作传播到次仓库。</p><h3 id="镜像仓库">镜像仓库</h3><p>设置一个仓库作为另一个的镜像，这样任何对主仓库的更改都会自动反映到镜像仓库中。GitHub 和 Gitee 都支持仓库镜像的设置。这种方法通常不会在镜像仓库中直接进行开发或合并操作，而是完全从主仓库同步。</p><h3 id="避免使用合并提交-当前策略">避免使用合并提交（当前策略）</h3><p>在合并 PR 时使用“变基”（rebase）而不是“合并提交”（merge commit）。但这需要团队成员接受提交历史可能被改写的事实。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2024/12/14/%E6%97%A5%E7%A8%8B/%E6%80%A5%E6%80%A5%E6%80%A5/"/>
      <url>/2024/12/14/%E6%97%A5%E7%A8%8B/%E6%80%A5%E6%80%A5%E6%80%A5/</url>
      
        <content type="html"><![CDATA[<h1 id="开源大赛">开源大赛</h1><h2 id="提交材料">提交材料</h2><blockquote><p>截止日期 10月30日</p></blockquote><p>作品补充要求：</p><ul><li>技术报告<ul><li>作品设计方案</li><li>实现方案</li><li>运行效果/测试结果</li><li>特色创新</li></ul></li><li>演示视频</li></ul><p>提交到官方邮箱osic@ccf.org.cn，逾期未提交将视为无效作品。</p><h2 id="赛题答辩">赛题答辩</h2><p>答辩地点：长沙理工金盆岭校区<br>时长20分钟/组（12分钟报告+8分钟问答）<br>考察内容：<br>简要介绍作品</p><ul><li>总体设计思路</li><li>代码实现及运行演示情况</li><li>赛题分析</li><li>软件代码编写</li><li>团队协作</li><li>技术方案</li><li>现场演示及答辩情况等方面能力</li></ul><h1 id="素质大赛">素质大赛</h1><h2 id="选题">选题</h2><p>自主选题，自拟题目，从信息素养的角度设计一个基于信息解决问题的案例设计作品，作品以微视频和文字说明的形式呈现。<br>注:参见《2024年大学生信息素养大赛省赛题型及评分标准》</p><h2 id="要求">要求</h2><p>【省赛决赛作品要求】<br>请结合实际，自主选题，自拟题目，从信息素养的角度设计一个基于信息解决问题的案例作品，作品以微视频和文字说明的形式呈现。<br>1.严格遵守国家法律法规，自觉践行社会主义核心价值观。作品创作须为原创成果，使用正版软件。作品必须由参赛团队独立制作完成，禁止他人代制作，不得侵犯他人知识产权，不得违反伦理道德，若出现上述禁止行为，一经发现，将取消参赛资格。<br>2.作品采用微视频形式呈现，提交的文件应包含案例微视频作品和案例文字说明两部分。<br>本科生组：微视频时长控制在4分钟左右（正负15秒范围内），大小不超过200M，有字幕，文件格式为MP4；案例文字说明控制在1000字以内，标题黑体三号居中，正文宋体四号，首行缩进两个字符，1.5倍行距。文件格式为PDF。提交文件命名格式为：学校名+案例名。<br>研究生组：微视频时长控制在5分钟左右（正负15秒范围内），大小不超过300M，有字幕，文件格式为MP4；案例文字说明控制在1500字以内，标题黑体三号居中，正文宋体四号，首行缩进两个字符，1.5倍行距。文件格式为PDF。提交文件命名格式为：学校名+案例名。<br>3.作品及文档制作中，允许使用AI工具，如果使用了AI工具和AI生成内容，包括利用AI工具收集和分析数据、制作图片、视频或图形元素、生成程序算法、文本或进行文字润色等，须在作品及提交的文档中明确说明使用了何种AI工具、使用过程，以及其作用和贡献。<br>4.微视频作品与文字说明中均不得出现学校、指导老师、学生姓名等信息。<br>5.参赛作品作为大赛成果，主办方拥有非商业性宣传展览、媒体报道、网络推广等免费使用权，不再另行征询学生本人及指导老师意见。参赛者享有署名权。<br>6.进入省赛决赛的队伍需完成全部赛程后，才能获得省赛评奖资格。</p><h1 id="组会">组会</h1><h2 id="maccai-2024-unified-multi-modal-learning-for-any-modality-combinations-in-alzheimer-s-disease-diagnosis">MACCAI 2024 Unified Multi-Modal Learning for Any Modality  Combinations in Alzheimer’s Disease Diagnosis</h2>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>神经网络基础</title>
      <link href="/2024/09/28/DL-notes/pages/week_6/"/>
      <url>/2024/09/28/DL-notes/pages/week_6/</url>
      
        <content type="html"><![CDATA[<h1 id="神经网络">神经网络</h1><p>处理和学习复杂的数据 本周学习神经网络基础知识，包括前馈神经网络与时序神经网络结构、单层神经网络与多层神经网络典型案例实践。</p><p>课程安排：</p><ol><li>掌握感知器的学习原理</li><li>掌握神经网络的模型结构</li><li>了解单层神经网络与多层神经网络的能力</li><li>掌握梯度下降算法原理与实践</li><li>掌握反向传播算法原理与实践</li><li>掌握RNN与LSTM模型结构</li><li>基于Python进行二维空间线性可分数据单/多层感知器实战</li></ol>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
          <category> 笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DL-notes </tag>
            
            <tag> DL </tag>
            
            <tag> notes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习基础</title>
      <link href="/2024/09/28/DL-notes/pages/week_5/"/>
      <url>/2024/09/28/DL-notes/pages/week_5/</url>
      
        <content type="html"><![CDATA[<h1 id="机器学习">机器学习</h1><p>本周学习机器学习基础知识，包括机器学习概念、机器学习模型分类、评估目标与优化目标、典型案例实践。</p><p>课程安排：</p><ol><li>掌握机器学习工具的基本流程</li><li>掌握特征的概念与使用</li><li>了解不同机器学习模型的分类</li><li>学会常见机器学习模型的评估方法</li><li>学会常见机器学习模型的学习优化目标</li><li>学习使用python机器学习工具sklearn</li><li>基于sklearn工具和鸢尾花数据集，进行逻辑回归实战</li></ol>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
          <category> 笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DL-notes </tag>
            
            <tag> DL </tag>
            
            <tag> notes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>常见算法</title>
      <link href="/2024/09/28/DL-notes/pages/HwAlg/"/>
      <url>/2024/09/28/DL-notes/pages/HwAlg/</url>
      
        <content type="html"><![CDATA[<h1 id="算法">算法</h1><h1 id="常见算法">常见算法</h1><h2 id="常见的激活函数">常见的激活函数</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>/(<span class="number">1</span>+np.exp(-x))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">tanh</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> np.tanh(x)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">relu</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> np.maximum(<span class="number">0</span>, x)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">leaky_relu</span>(<span class="params">x, alpha</span>):</span><br><span class="line">    <span class="keyword">return</span> np.where(x&gt;<span class="number">0</span>, x, x*alpha)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">elu</span>(<span class="params">x, alpha</span>):</span><br><span class="line">    <span class="keyword">return</span> np.where(x&gt;<span class="number">0</span>, x, alpha*(np.exp(x) - <span class="number">1</span>))</span><br></pre></td></tr></table></figure><h2 id="梯度下降">梯度下降</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lr = <span class="number">0.1</span></span><br><span class="line">x1 = <span class="number">2.5</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_iterations):</span><br><span class="line">    gredient = df(x1)</span><br><span class="line">    x1 = x1 - lr*gredient</span><br></pre></td></tr></table></figure><h2 id="概率分布模型">概率分布模型</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># 设定随机数种子，以便于复现结果</span></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 伯努利</span></span><br><span class="line">p = <span class="number">0.5</span></span><br><span class="line">bernoulli_dist = np.random.binomial(<span class="number">1</span>, p, <span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 二项</span></span><br><span class="line">n = <span class="number">10</span></span><br><span class="line">binomial_dist = np.random.binomial(n, p, <span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 正态</span></span><br><span class="line">mu, sigma= <span class="number">0</span>, <span class="number">0.1</span></span><br><span class="line">normal_dist = np.random.normal(mu, sigma, <span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 指数</span></span><br><span class="line">lambd = <span class="number">1.0</span> </span><br><span class="line">exponential_dist = np. random.exponential(<span class="number">1</span>/lambd, <span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Logistics</span></span><br><span class="line">mu, s = <span class="number">0</span>, <span class="number">1</span></span><br><span class="line">logistic_dist = np.random.logistic(mu, s, <span class="number">1000</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">data1 = np.random.randn(<span class="number">1000</span>)</span><br><span class="line">data2 = np.random.randn(<span class="number">1000</span>)</span><br><span class="line"><span class="comment"># 期望</span></span><br><span class="line">expectation1 = np.mean(data1)</span><br><span class="line">expectation2 = np.mean(data2)</span><br><span class="line"><span class="comment"># 方差</span></span><br><span class="line">variance1 = np.var(data1)</span><br><span class="line">variance2 = np.var(data2)</span><br><span class="line"><span class="comment"># 协方差</span></span><br><span class="line">covariance_matrix = np.cov(data1, data2)</span><br></pre></td></tr></table></figure><h2 id="逻辑回归模型">逻辑回归模型</h2><h2 id="反向传播算法">反向传播算法</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x_1 = <span class="number">40.0</span></span><br><span class="line">x_2 = <span class="number">80.0</span></span><br><span class="line"></span><br><span class="line">expected_output = <span class="number">60.0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 网络权重</span></span><br><span class="line">w_1 = np.full((<span class="number">2</span>,<span class="number">3</span>),<span class="number">0.5</span>)</span><br><span class="line">w_2 = np.full((<span class="number">3</span>,<span class="number">1</span>),<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">back_forward</span>(<span class="params">x_1, x_2, w_1, w_2, expected_output, loop_num, print_num</span>):</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,loop_num):</span><br><span class="line">        z_1 = x_1 * w_1[<span class="number">0</span>][<span class="number">0</span>] + x_2 * w_1[<span class="number">1</span>][<span class="number">0</span>]</span><br><span class="line">        z_2 = x_1 * w_1[<span class="number">0</span>][<span class="number">1</span>] + x_2 * w_1[<span class="number">1</span>][<span class="number">1</span>]</span><br><span class="line">        z_3 = x_1 * w_1[<span class="number">0</span>][<span class="number">2</span>] + x_2 * w_1[<span class="number">1</span>][<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">        y_pred = z_1 * w_2[<span class="number">0</span>][<span class="number">0</span>] + z_2 * w_2[<span class="number">1</span>][<span class="number">0</span>] + z_3 * w_2[<span class="number">2</span>][<span class="number">0</span>]</span><br><span class="line">        loss = <span class="number">0.5</span> * (expected_output - y_pred) ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>(i%print_num == <span class="number">0</span>):</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;前向结果：<span class="subst">&#123;y_pred&#125;</span>&quot;</span>)    </span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Loss：<span class="subst">&#123;loss&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算梯度</span></span><br><span class="line">        <span class="comment"># 输出层梯度</span></span><br><span class="line">        d_loss_predict_output = -(expected_output - y_pred)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 权重损失梯度</span></span><br><span class="line">        d_loss_w_2 = np.multiply(d_loss_predict_output,[z_1, z_2, z_3])</span><br><span class="line">        d_loss_w_1 = np.multiply(np.multiply(w_2,[x_1,x_2]),d_loss_predict_output)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># d_loss_w_1[0][0] -&gt; w_2[0][0] * x_1 </span></span><br><span class="line"></span><br><span class="line">        learning_rate = <span class="number">1e-5</span></span><br><span class="line">        w_2 -= np.multiply(d_loss_w_2, learning_rate).reshape(<span class="number">3</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        w_1 -= np.multiply(d_loss_w_1, learning_rate).reshape(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 继续前向</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;expected_output：<span class="subst">&#123;expected_output&#125;</span>&quot;</span>)</span><br><span class="line">back_forward(x_1, x_2, w_1, w_2, expected_output, <span class="number">100</span>, <span class="number">10</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
          <category> 笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DL-notes </tag>
            
            <tag> DL </tag>
            
            <tag> notes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>算法笔记</title>
      <link href="/2024/09/19/LeetCode/LeetCodeBook/"/>
      <url>/2024/09/19/LeetCode/LeetCodeBook/</url>
      
        <content type="html"><![CDATA[<h1 id="算法笔记">算法笔记</h1><h2 id="1-labuladong的算法笔记">1 <a href="https://labuladong.online/algo/home/">labuladong的算法笔记</a></h2><blockquote><p>From 2024年9月20日</p></blockquote><h3 id="单链表的基本技巧"><strong>单链表的基本技巧</strong></h3><p>1、<a href="https://leetcode.cn/problems/merge-two-sorted-lists">合并两个有序链表</a> <code>Easy</code></p><blockquote><p>虚拟头节点，双指针</p></blockquote><p>2、<a href="https://leetcode.cn/problems/partition-list">单链表分解</a> <code>middle</code></p><blockquote><p>虚拟头节点，双指针</p></blockquote><p>3、<a href="https://leetcode.cn/problems/merge-k-sorted-lists">合并K个升序链表</a> <code>hard</code></p><blockquote><p>堆的使用，priority_queue</p></blockquote><p>4、寻找单链表的倒数第 <code>k</code> 个节点</p><p>5、寻找单链表的中点</p><p>6、判断单链表是否包含环并找出环起点</p><p>7、判断两个单链表是否相交并找出交点</p><h2 id="2-leecode-刷题笔记">2 LeeCode 刷题笔记</h2><blockquote><p>From 2024年9月20日 待补充</p></blockquote><h3 id="455-分发饼干"><a href="https://leetcode.cn/problems/assign-cookies/">455. 分发饼干</a></h3><blockquote><p>09/22 <a href="https://leetcode.cn/submissions/detail/566906740/">提交</a></p></blockquote><h3 id="岛屿的周长"><a href="https://leetcode.cn/problems/island-perimeter/">岛屿的周长</a></h3><blockquote><p>09/23 <a href="https://leetcode.cn/submissions/detail/567257105/">提交</a></p></blockquote><h3 id="最大连续-1-的个数"><a href="https://leetcode.cn/problems/max-consecutive-ones/">最大连续 1 的个数</a></h3><blockquote><p>09/24 <a href="https://leetcode.cn/submissions/detail/567534692/">提交</a></p></blockquote><h3 id="提莫攻击"><a href="https://leetcode.cn/problems/teemo-attacking/">提莫攻击</a></h3><blockquote><p>09/25 <a href="https://leetcode.cn/submissions/detail/567912182/">提交</a></p></blockquote><h3 id="下一个更大元素-i"><a href="https://leetcode.cn/problems/next-greater-element-i/">下一个更大元素 I</a></h3><blockquote><p>09/26 <a href="https://leetcode.cn/submissions/detail/568196305/">提交</a></p></blockquote><h3 id="键盘行"><a href="https://leetcode.cn/problems/keyboard-row/">键盘行</a></h3><blockquote><p>09/27 <a href="https://leetcode.cn/submissions/detail/568435534/">提交</a></p></blockquote><h3 id="相对名次"><a href="https://leetcode.cn/problems/relative-ranks/">相对名次</a></h3><blockquote><p>09/28 <a href="https://leetcode.cn/submissions/detail/568764757/">提交</a></p></blockquote><h2 id="c-自查表">C++ 自查表</h2><blockquote><p>所有容器：<a href="https://en.cppreference.com/w/cpp/container#Container_adaptors">Containers library - cppreference.com</a></p><p>中文首页：<a href="https://zh.cppreference.com/w/%E9%A6%96%E9%A1%B5">cppreference.com</a></p><p>英文首页：<a href="https://en.cppreference.com/w/">cppreference.com</a></p><p>如果以下英文网页难以理解，可以将 url 中的 en 改为 zh，即中文网页。</p></blockquote><h3 id="算法-algorithm">算法 algorithm</h3><h3 id="c-中的默认随机数引擎-default-random-engine">C++中的默认随机数引擎（default_random_engine）</h3><p>在C++中，<code>default_random_engine</code>是一个生成伪随机数的引擎类，它至少提供了相对随意、非专家或轻量级使用的可接受的引擎行为。这个引擎类是标准库实现中的选择，用于生成伪随机数。</p><p><strong>代码示例</strong></p><p>使用<code>default_random_engine</code>可以非常简单地生成随机数。以下是一个基本的使用示例：</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;random&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    default_random_engine e; <span class="comment">// 随机数引擎</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; ++i)</span><br><span class="line"></span><br><span class="line">    cout &lt;&lt; <span class="built_in">e</span>() &lt;&lt; endl; <span class="comment">// 返回一个无符号整数</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>此代码创建了一个<code>default_random_engine</code>对象，并在循环中调用它来生成随机数。<code>e()</code>调用不接受参数，并返回一个无符号整数。</p><p><strong>生成指定范围内的随机数</strong></p><p>要生成特定范围内的随机数，可以使用随机数分布类，如<code>uniform_int_distribution</code>。以下是一个生成0到9之间随机数的示例：</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function">uniform_int_distribution&lt;<span class="type">unsigned</span>&gt; <span class="title">u</span><span class="params">(<span class="number">0</span>,<span class="number">9</span>)</span></span>; <span class="comment">// 随机数分布类</span></span><br><span class="line"></span><br><span class="line">    default_random_engine e; <span class="comment">// 随机数引擎</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; ++i)</span><br><span class="line"></span><br><span class="line">    cout &lt;&lt; <span class="built_in">u</span>(e) &lt;&lt; endl; <span class="comment">// 注意是u(e)，而不是u(e())</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这段代码首先定义了一个<code>uniform_int_distribution</code>对象<code>u</code>，它的范围是0到9。然后，它创建了一个<code>default_random_engine</code>对象<code>e</code>，并在循环中调用<code>u(e)</code>来生成随机数。</p><p><strong>设置随机数引擎种子</strong></p><p>为了每次运行程序时生成不同的随机数序列，可以设置随机数引擎的种子。通常使用时间作为种子：</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function">uniform_int_distribution&lt;<span class="type">int</span>&gt; <span class="title">u</span><span class="params">(<span class="number">0</span>, <span class="number">9</span>)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="function">default_random_engine <span class="title">e</span><span class="params">(time(<span class="number">0</span>))</span></span>; <span class="comment">// 使用当前时间作为种子</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; ++i)</span><br><span class="line"></span><br><span class="line">cout &lt;&lt; <span class="built_in">u</span>(e) &lt;&lt; <span class="string">&quot; &quot;</span>;</span><br></pre></td></tr></table></figure><p>这段代码使用当前时间作为种子来初始化<code>default_random_engine</code>对象<code>e</code>，然后生成随机数。</p><p><strong>注意事项</strong></p><ul><li>对于给定的发生器，每次运行返回相同的数值序列。如果希望在程序的每次运行中都生成不同的序列，需要设置不同的种子。</li><li>如果在定义时没有设置种子，<code>default_random_engine</code>将使用默认种子。</li><li>如果在循环中使用时间作为种子，由于时间的单位是秒，可能会导致在短时间内生成相同的种子，从而产生相同的随机数序列。</li></ul><p>通过使用<code>default_random_engine</code>和适当的分布类对象，可以更直观地获取随机数，而不需要进行取余等计算。这是C++11中引入的一种新的获取随机数的方法，相比于传统的<code>rand</code>函数，它提供了更好的随机性和灵活性。</p><h3 id="约束-since-c-20">约束 （Since C++20）</h3><p>包含：<a href="https://en.cppreference.com/w/cpp/algorithm/ranges">Constrained algorithms (since C++20) - cppreference.com</a></p><ul><li>Non-modifying sequence operations</li><li>Modifying sequence operations</li><li>Partitioning operations</li><li>Sorting operations</li><li>Binary search operations (on sorted ranges)</li><li>Set operations (on sorted ranges)</li><li>Heap operations</li><li>Minimum/maximum operations</li><li>Permutation operations</li><li>etc…</li></ul><h3 id="顺序容器">顺序容器</h3><h4 id="vector">&lt;vector&gt;</h4><p><strong>头文件</strong>：<code>#include&lt;vector&gt;</code></p><p><strong>描述</strong>：向量容器，类似于数组，存储在连续内存块，可以在末尾快速插入与删除，支持随机访问。</p><p><strong>函数</strong>：<a href="https://en.cppreference.com/w/cpp/container/vector">std::vector - cppreference.com</a></p><p><strong>生成指定范围内的随机数</strong></p><p>要生成特定范围内的随机数，可以使用随机数分布类，如<code>uniform_int_distribution</code>。以下是一个生成0到9之间随机数的示例：</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function">uniform_int_distribution&lt;<span class="type">unsigned</span>&gt; <span class="title">u</span><span class="params">(<span class="number">0</span>,<span class="number">9</span>)</span></span>; <span class="comment">// 随机数分布类</span></span><br><span class="line"></span><br><span class="line">    default_random_engine e; <span class="comment">// 随机数引擎</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; ++i)</span><br><span class="line"></span><br><span class="line">    cout &lt;&lt; <span class="built_in">u</span>(e) &lt;&lt; endl; <span class="comment">// 注意是u(e)，而不是u(e())</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">#### \&lt;list&gt;</span><br><span class="line"></span><br><span class="line">**头文件**：`<span class="meta">#<span class="keyword">include</span><span class="string">&lt;list&gt;</span>`</span></span><br><span class="line"></span><br><span class="line">**描述**：链表容器，双链表类模板，需从表头开始遍历。</span><br><span class="line"></span><br><span class="line">**函数**：[std::list - cppreference.com](https:<span class="comment">//en.cppreference.com/w/cpp/container/list)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#### \&lt;forward_list&gt;</span><br><span class="line"></span><br><span class="line">**头文件**：`<span class="meta">#<span class="keyword">include</span><span class="string">&lt;forward_list&gt;</span>`</span></span><br><span class="line"></span><br><span class="line">**描述**：链表容器，单链表类模板，需从表头开始遍历，与list相比有更低的存储成本。</span><br><span class="line"></span><br><span class="line">**函数**：[std::forward_list - cppreference.com](https:<span class="comment">//en.cppreference.com/w/cpp/container/forward_list)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">### 关联容器</span><br><span class="line"></span><br><span class="line">| [set](https:<span class="comment">//en.cppreference.com/w/cpp/container/set)       | collection of unique keys, sorted by keys (class template)   |</span></span><br><span class="line">| ------------------------------------------------------------ | ------------------------------------------------------------ |</span><br><span class="line">| [map](https:<span class="comment">//en.cppreference.com/w/cpp/container/map)       | collection of key-value pairs, sorted by keys, keys are unique (class template) |</span></span><br><span class="line">| [multiset](https:<span class="comment">//en.cppreference.com/w/cpp/container/multiset) | collection of keys, sorted by keys (class template)          |</span></span><br><span class="line">| [multimap](https:<span class="comment">//en.cppreference.com/w/cpp/container/multimap) | collection of key-value pairs, sorted by keys (class template) |</span></span><br><span class="line"></span><br><span class="line">&gt;multi 即支持元素重复，map 即 key 映射 value，set 即 仅包含 value。</span><br><span class="line"></span><br><span class="line">### 无序关联容器</span><br><span class="line"></span><br><span class="line">| [unordered_set](https:<span class="comment">//en.cppreference.com/w/cpp/container/unordered_set)(C++11) | collection of unique keys, hashed by keys (class template)   |</span></span><br><span class="line">| ------------------------------------------------------------ | ------------------------------------------------------------ |</span><br><span class="line">| [unordered_map](https:<span class="comment">//en.cppreference.com/w/cpp/container/unordered_map)(C++11) | collection of key-value pairs, hashed by keys, keys are unique (class template) |</span></span><br><span class="line">| [unordered_multiset](https:<span class="comment">//en.cppreference.com/w/cpp/container/unordered_multiset)(C++11) | collection of keys, hashed by keys (class template)          |</span></span><br><span class="line">| [unordered_multimap](https:<span class="comment">//en.cppreference.com/w/cpp/container/unordered_multimap)(C++11) | collection of key-value pairs, hashed by keys (class template) |</span></span><br><span class="line"></span><br><span class="line">&gt; 无序关联容器基于哈希实现，平均复杂度 <span class="built_in">O</span>(<span class="number">1</span>)，最坏 <span class="built_in">O</span>(n)。</span><br><span class="line"></span><br><span class="line">### 适配器容器</span><br><span class="line"></span><br><span class="line">| [stack](https:<span class="comment">//en.cppreference.com/w/cpp/container/stack)   | adapts a container to provide stack (LIFO data structure) (class template) |</span></span><br><span class="line">| ------------------------------------------------------------ | ------------------------------------------------------------ |</span><br><span class="line">| [queue](https:<span class="comment">//en.cppreference.com/w/cpp/container/queue)   | adapts a container to provide queue (FIFO data structure) (class template) |</span></span><br><span class="line">| [priority_queue](https:<span class="comment">//en.cppreference.com/w/cpp/container/priority_queue) | adapts a container to provide priority queue (class template) |</span></span><br><span class="line">| [flat_set](https:<span class="comment">//en.cppreference.com/w/cpp/container/flat_set)(C++23) | adapts a container to provide a collection of unique keys, sorted by keys (class template) |</span></span><br><span class="line">| [flat_map](https:<span class="comment">//en.cppreference.com/w/cpp/container/flat_map)(C++23) | adapts two containers to provide a collection of key-value pairs, sorted by unique keys (class template) |</span></span><br><span class="line">| [flat_multiset](https:<span class="comment">//en.cppreference.com/w/cpp/container/flat_multiset)(C++23) | adapts a container to provide a collection of keys, sorted by keys (class template) |</span></span><br><span class="line">| [flat_multimap](https:<span class="comment">//en.cppreference.com/w/cpp/container/flat_multimap)(C++23) | adapts two containers to provide a collection of key-value pairs, sorted by keys (class template) |</span></span><br><span class="line"></span><br><span class="line">#### \&lt;queue\&gt;</span><br><span class="line"></span><br><span class="line">**头文件**：`<span class="meta">#<span class="keyword">include</span><span class="string">&lt;queue&gt;</span>`</span></span><br><span class="line"></span><br><span class="line">**描述**：队列容器，FIFO特点，不允许顺序遍历。</span><br><span class="line"></span><br><span class="line">**函数**：[std::queue - cppreference.com](https:<span class="comment">//en.cppreference.com/w/cpp/container/queue)</span></span><br><span class="line"></span><br><span class="line">- <span class="built_in">empty</span>(): 判断容器是否空</span><br><span class="line">- <span class="built_in">size</span>(): 返回实际元素个数</span><br><span class="line">- <span class="built_in">frot</span>(): 返回队头元素</span><br><span class="line">- <span class="built_in">back</span>(): 返回队尾元素</span><br><span class="line">- <span class="built_in">push</span>(elem): elem 进队</span><br><span class="line">- <span class="built_in">pop</span>(): 元素出队</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#### \&lt;priority_queue\&gt;</span><br><span class="line"></span><br><span class="line">```cpp</span><br><span class="line"><span class="keyword">template</span>&lt;</span><br><span class="line">    <span class="keyword">class</span> T,</span><br><span class="line">    <span class="keyword">class</span> Container = std::vector&lt;T&gt;,</span><br><span class="line">    <span class="keyword">class</span> Compare = std::less&lt;<span class="keyword">typename</span> Container::value_type&gt;</span><br><span class="line">&gt; <span class="keyword">class</span> priority_queue;</span><br></pre></td></tr></table></figure><p><strong>头文件</strong>：<code>#include&lt;priority_queue&gt;</code></p><p><strong>描述</strong>：优先队列容器，任意顺序进入队列，出队操作将以出队优先级为准（默认最大元素出队）。</p><p><strong>函数</strong>：<a href="https://en.cppreference.com/w/cpp/container/priority_queue">std::priority_queue - cppreference.com</a></p><ul><li>empty()</li><li>size()</li><li>push(elem)</li><li>top(): 返回队头元素</li><li>pop(): 元素出队</li></ul><blockquote><p><code>&lt; &gt;</code> 指定模板参数。默认情况下，<code>std::priority_queue</code>使用<code>std::less</code>作为其比较对象，这意味着队列会按照元素的升序排列（即，最大的元素被视为最高优先级），通常<code>std::priority_queue</code>默认是最大堆。</p></blockquote><blockquote><p>这里有一个很难理解的地方，使用Compare时，会以Compare为True作为最高优先级，But 堆的内部实现是vector，每次出堆时实际上是<strong>堆顶元素和最后一个元素互换</strong>，我们可以理解为<strong>将其沉入数组末端</strong>。那么实际上假如没有出堆这一个过程而是继续将数据缓存在vector中，所有元素出堆后，形成的数组就是升序的。只是由于<strong>出堆这一个操作让我们看起来是一个反序的排列</strong>，而实际上元素出堆在这里相当于<strong>逆序输出</strong>。</p><p>所以这里的最高优先级，可以理解为，<strong>高优先级的放在前列</strong>，但是实际上每次<strong>出堆都会弹出尾部</strong>，所以<strong>Compare变现为逆序</strong>。</p></blockquote><h3 id="lambda-表达式-since-c-11">Lambda 表达式（since C++11）</h3><p><a href="https://en.cppreference.com/w/cpp/language/lambda">Lambda expressions (since C++11) - cppreference.com</a></p><h4 id="非泛型lambda表达式">非泛型Lambda表达式</h4><h5 id="基本语法">基本语法</h5><p>没有显式模板参数列表的Lambda表达式的基本语法如下：</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">[capture](parameters) <span class="function"><span class="keyword">mutable</span> <span class="title">noexcept</span><span class="params">(expression)</span> -&gt; return_type </span>&#123;  </span><br><span class="line">    <span class="comment">// 函数体  </span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><strong>capture</strong>：捕获列表，指定Lambda表达式体内部可以访问的外部变量。捕获可以是值捕获（通过<code>=</code>或<code>&amp;</code>指定）或引用捕获（仅通过<code>&amp;</code>指定）。</li><li><strong>parameters</strong>：参数列表，与普通函数相同，但在这个上下文中是可选的。</li><li><strong>mutable</strong>、<strong>noexcept</strong>、<strong>-&gt; return_type</strong>：这些都是可选的。<code>mutable</code>允许在Lambda表达式体内修改捕获的变量（如果它们是值捕获的）。<code>noexcept</code>指定Lambda表达式是否抛出异常。<code>-&gt; return_type</code>指定Lambda表达式的返回类型，如果Lambda表达式体中有返回语句且编译器无法从返回语句推断出返回类型，则需要显式指定。</li></ul><h5 id="例子">例子</h5><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;algorithm&gt;</span>  </span></span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">    std::vector&lt;<span class="type">int</span>&gt; vec = &#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>&#125;;  </span><br><span class="line">  </span><br><span class="line">    <span class="comment">// 使用非泛型Lambda表达式对vector中的每个元素加1  </span></span><br><span class="line">    std::<span class="built_in">transform</span>(vec.<span class="built_in">begin</span>(), vec.<span class="built_in">end</span>(), vec.<span class="built_in">begin</span>(),  </span><br><span class="line">        [](<span class="type">int</span> x) &#123; <span class="keyword">return</span> x + <span class="number">1</span>; &#125;);  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> n : vec) &#123;  </span><br><span class="line">        std::cout &lt;&lt; n &lt;&lt; <span class="string">&#x27; &#x27;</span>;  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="comment">// 输出: 2 3 4 5 6  </span></span><br><span class="line">  </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>比如这个例子，Lambda表达式<code>[](int x) &#123; return x + 1; &#125;</code>是一个非泛型Lambda表达式，在这里的作用等同于：</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">bool</span> <span class="title">cmp</span><span class="params">(<span class="type">int</span> x)</span></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> x + <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line">std::<span class="built_in">transform</span>(vec.<span class="built_in">begin</span>(), vec.<span class="built_in">end</span>(), vec.<span class="built_in">begin</span>(),cmp);  </span><br></pre></td></tr></table></figure><h5 id="作用">作用</h5><p>非泛型Lambda表达式在C++中不能直接用来“定义”一个具有全局或命名空间作用域的函数，因为Lambda表达式本质上是匿名函数对象，它们通常在需要函数对象、回调函数或类似机制的地方使用。</p><p>但你可以将Lambda表达式赋值给一个<strong>函数指针</strong>（Lambda [capture list]为空其参数类型和返回类型与函数指针兼容），或者赋值给一个<code>std::function</code>对象。</p><p>如：</p><ul><li><strong>赋值函数指针</strong></li></ul><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;functional&gt;</span>  </span></span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">    <span class="comment">// 使用std::function来存储Lambda表达式  </span></span><br><span class="line">    std::function&lt;<span class="type">int</span>(<span class="type">int</span>, <span class="type">int</span>)&gt; add = [](<span class="type">int</span> x, <span class="type">int</span> y) &#123; <span class="keyword">return</span> x + y; &#125;;  </span><br><span class="line">    <span class="comment">// 也可以用auto来自动推导Lambda表达式 Since C++11</span></span><br><span class="line">  <span class="keyword">auto</span> add = [](<span class="type">int</span> x, <span class="type">int</span> y) &#123; <span class="keyword">return</span> x + y; &#125;; </span><br><span class="line">    <span class="comment">// 调用Lambda表达式  </span></span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;The sum is: &quot;</span> &lt;&lt; <span class="built_in">add</span>(<span class="number">2</span>, <span class="number">3</span>) &lt;&lt; std::endl;  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><strong>作为参数传递函数</strong></li></ul><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;algorithm&gt;</span>  </span></span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">printVector</span><span class="params">(<span class="type">const</span> std::vector&lt;<span class="type">int</span>&gt;&amp; vec, std::function&lt;<span class="type">void</span>(<span class="type">int</span>)&gt; printer)</span> </span>&#123;  </span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> n : vec) &#123;  </span><br><span class="line">        <span class="built_in">printer</span>(n);  </span><br><span class="line">    &#125;  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">    std::vector&lt;<span class="type">int</span>&gt; vec = &#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>&#125;;  </span><br><span class="line">  </span><br><span class="line">    <span class="comment">// 将Lambda表达式作为参数传递给printVector  </span></span><br><span class="line">    <span class="built_in">printVector</span>(vec, [](<span class="type">int</span> n) &#123; std::cout &lt;&lt; n &lt;&lt; <span class="string">&#x27; &#x27;</span>; &#125;);  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这段代码首先定义了一个<code>uniform_int_distribution</code>对象<code>u</code>，它的范围是0到9。然后，它创建了一个<code>default_random_engine</code>对象<code>e</code>，并在循环中调用<code>u(e)</code>来生成随机数。</p><p><strong>设置随机数引擎种子</strong></p><p>为了每次运行程序时生成不同的随机数序列，可以设置随机数引擎的种子。通常使用时间作为种子：</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function">uniform_int_distribution&lt;<span class="type">int</span>&gt; <span class="title">u</span><span class="params">(<span class="number">0</span>, <span class="number">9</span>)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="function">default_random_engine <span class="title">e</span><span class="params">(time(<span class="number">0</span>))</span></span>; <span class="comment">// 使用当前时间作为种子</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; ++i)</span><br><span class="line"></span><br><span class="line">cout &lt;&lt; <span class="built_in">u</span>(e) &lt;&lt; <span class="string">&quot; &quot;</span>;</span><br></pre></td></tr></table></figure><p>这段代码使用当前时间作为种子来初始化<code>default_random_engine</code>对象<code>e</code>，然后生成随机数。</p><p><strong>注意事项</strong></p><ul><li>对于给定的发生器，每次运行返回相同的数值序列。如果希望在程序的每次运行中都生成不同的序列，需要设置不同的种子。</li><li>如果在定义时没有设置种子，<code>default_random_engine</code>将使用默认种子。</li><li>如果在循环中使用时间作为种子，由于时间的单位是秒，可能会导致在短时间内生成相同的种子，从而产生相同的随机数序列。</li></ul><p>通过使用<code>default_random_engine</code>和适当的分布类对象，可以更直观地获取随机数，而不需要进行取余等计算。这是C++11中引入的一种新的获取随机数的方法，相比于传统的<code>rand</code>函数，它提供了更好的随机性和灵活性。</p><h4 id="约束-since-c-20">约束 （Since C++20）</h4><p>包含：<a href="https://en.cppreference.com/w/cpp/algorithm/ranges">Constrained algorithms (since C++20) - cppreference.com</a></p><ul><li>Non-modifying sequence operations</li><li>Modifying sequence operations</li><li>Partitioning operations</li><li>Sorting operations</li><li>Binary search operations (on sorted ranges)</li><li>Set operations (on sorted ranges)</li><li>Heap operations</li><li>Minimum/maximum operations</li><li>Permutation operations</li><li>etc…</li></ul><h3 id="顺序容器">顺序容器</h3><h4 id="vector">&lt;vector&gt;</h4><p><strong>头文件</strong>：<code>#include&lt;vector&gt;</code></p><p><strong>描述</strong>：向量容器，类似于数组，存储在连续内存块，可以在末尾快速插入与删除，支持随机访问。</p><p><strong>函数</strong>：<a href="https://en.cppreference.com/w/cpp/container/vector">std::vector - cppreference.com</a></p><h4 id="string">&lt;string&gt;</h4><p><strong>头文件</strong>：<code>#include&lt;string&gt;</code></p><p><strong>描述</strong>：字符串容器，除开字符串操作外，还包含序列容器操作。</p><p><strong>函数</strong>：<a href="https://en.cppreference.com/w/cpp/string/basic_string">std::basic_string - cppreference.com</a></p><h4 id="deque">&lt;deque&gt;</h4><p><strong>头文件</strong>：<code>#include&lt;deque&gt;</code></p><p><strong>描述</strong>：双端队列容器，块中地址连续，块间地址不连续，从首尾快速插入与删除，支持随机访问。</p><blockquote><p>分配空间比vector速度块，因为重新分配空间后原有元素不需要复制</p></blockquote><p><strong>函数</strong>：<a href="https://en.cppreference.com/w/cpp/container/deque">std::deque - cppreference.com</a></p><h4 id="list">&lt;list&gt;</h4><p><strong>头文件</strong>：<code>#include&lt;list&gt;</code></p><p><strong>描述</strong>：链表容器，双链表类模板，需从表头开始遍历。</p><p><strong>函数</strong>：<a href="https://en.cppreference.com/w/cpp/container/list">std::list - cppreference.com</a></p><h4 id="forward-list">&lt;forward_list&gt;</h4><p><strong>头文件</strong>：<code>#include&lt;forward_list&gt;</code></p><p><strong>描述</strong>：链表容器，单链表类模板，需从表头开始遍历，与list相比有更低的存储成本。</p><p><strong>函数</strong>：<a href="https://en.cppreference.com/w/cpp/container/forward_list">std::forward_list - cppreference.com</a></p><h3 id="关联容器">关联容器</h3><table><thead><tr><th><a href="https://en.cppreference.com/w/cpp/container/set">set</a></th><th>collection of unique keys, sorted by keys (class template)</th></tr></thead><tbody><tr><td><a href="https://en.cppreference.com/w/cpp/container/map">map</a></td><td>collection of key-value pairs, sorted by keys, keys are unique (class template)</td></tr><tr><td><a href="https://en.cppreference.com/w/cpp/container/multiset">multiset</a></td><td>collection of keys, sorted by keys (class template)</td></tr><tr><td><a href="https://en.cppreference.com/w/cpp/container/multimap">multimap</a></td><td>collection of key-value pairs, sorted by keys (class template)</td></tr></tbody></table><blockquote><p>multi 即支持元素重复，map 即 key 映射 value，set 即 仅包含 value。</p></blockquote><h3 id="无序关联容器">无序关联容器</h3><table><thead><tr><th><a href="https://en.cppreference.com/w/cpp/container/unordered_set">unordered_set</a>(C++11)</th><th>collection of unique keys, hashed by keys (class template)</th></tr></thead><tbody><tr><td><a href="https://en.cppreference.com/w/cpp/container/unordered_map">unordered_map</a>(C++11)</td><td>collection of key-value pairs, hashed by keys, keys are unique (class template)</td></tr><tr><td><a href="https://en.cppreference.com/w/cpp/container/unordered_multiset">unordered_multiset</a>(C++11)</td><td>collection of keys, hashed by keys (class template)</td></tr><tr><td><a href="https://en.cppreference.com/w/cpp/container/unordered_multimap">unordered_multimap</a>(C++11)</td><td>collection of key-value pairs, hashed by keys (class template)</td></tr></tbody></table><blockquote><p>无序关联容器基于哈希实现，平均复杂度 O(1)，最坏 O(n)。</p></blockquote><h3 id="适配器容器">适配器容器</h3><table><thead><tr><th><a href="https://en.cppreference.com/w/cpp/container/stack">stack</a></th><th>adapts a container to provide stack (LIFO data structure) (class template)</th></tr></thead><tbody><tr><td><a href="https://en.cppreference.com/w/cpp/container/queue">queue</a></td><td>adapts a container to provide queue (FIFO data structure) (class template)</td></tr><tr><td><a href="https://en.cppreference.com/w/cpp/container/priority_queue">priority_queue</a></td><td>adapts a container to provide priority queue (class template)</td></tr><tr><td><a href="https://en.cppreference.com/w/cpp/container/flat_set">flat_set</a>(C++23)</td><td>adapts a container to provide a collection of unique keys, sorted by keys (class template)</td></tr><tr><td><a href="https://en.cppreference.com/w/cpp/container/flat_map">flat_map</a>(C++23)</td><td>adapts two containers to provide a collection of key-value pairs, sorted by unique keys (class template)</td></tr><tr><td><a href="https://en.cppreference.com/w/cpp/container/flat_multiset">flat_multiset</a>(C++23)</td><td>adapts a container to provide a collection of keys, sorted by keys (class template)</td></tr><tr><td><a href="https://en.cppreference.com/w/cpp/container/flat_multimap">flat_multimap</a>(C++23)</td><td>adapts two containers to provide a collection of key-value pairs, sorted by keys (class template)</td></tr></tbody></table><h4 id="queue">&lt;queue&gt;</h4><p><strong>头文件</strong>：<code>#include&lt;queue&gt;</code></p><p><strong>描述</strong>：队列容器，FIFO特点，不允许顺序遍历。</p><p><strong>函数</strong>：<a href="https://en.cppreference.com/w/cpp/container/queue">std::queue - cppreference.com</a></p><ul><li>empty(): 判断容器是否空</li><li>size(): 返回实际元素个数</li><li>frot(): 返回队头元素</li><li>back(): 返回队尾元素</li><li>push(elem): elem 进队</li><li>pop(): 元素出队</li></ul><h4 id="priority-queue">&lt;priority_queue&gt;</h4><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;</span><br><span class="line">    <span class="keyword">class</span> <span class="title class_">T</span>,</span><br><span class="line">    <span class="keyword">class</span> <span class="title class_">Container</span> = std::vector&lt;T&gt;,</span><br><span class="line">    <span class="keyword">class</span> Compare = std::less&lt;<span class="keyword">typename</span> Container::value_type&gt;</span><br><span class="line">&gt; <span class="keyword">class</span> priority_queue;</span><br></pre></td></tr></table></figure><p><strong>头文件</strong>：<code>#include&lt;priority_queue&gt;</code></p><p><strong>描述</strong>：优先队列容器，任意顺序进入队列，出队操作将以出队优先级为准（默认最大元素出队）。</p><p><strong>函数</strong>：<a href="https://en.cppreference.com/w/cpp/container/priority_queue">std::priority_queue - cppreference.com</a></p><ul><li>empty()</li><li>size()</li><li>push(elem)</li><li>top(): 返回队头元素</li><li>pop(): 元素出队</li></ul><blockquote><p><code>&lt; &gt;</code> 指定模板参数。默认情况下，<code>std::priority_queue</code>使用<code>std::less</code>作为其比较对象，这意味着队列会按照元素的升序排列（即，最大的元素被视为最高优先级），通常<code>std::priority_queue</code>默认是最大堆。</p></blockquote><blockquote><p>这里有一个很难理解的地方，使用Compare时，会以Compare为True作为最高优先级，But 堆的内部实现是vector，每次出堆时实际上是<strong>堆顶元素和最后一个元素互换</strong>，我们可以理解为<strong>将其沉入数组末端</strong>。那么实际上假如没有出堆这一个过程而是继续将数据缓存在vector中，所有元素出堆后，形成的数组就是升序的。只是由于<strong>出堆这一个操作让我们看起来是一个反序的排列</strong>，而实际上元素出堆在这里相当于<strong>逆序输出</strong>。</p><p>所以这里的最高优先级，可以理解为，<strong>高优先级的放在前列</strong>，但是实际上每次<strong>出堆都会弹出尾部</strong>，所以<strong>Compare变现为逆序</strong>。</p></blockquote><h2 id="lambda-表达式-since-c-11">Lambda 表达式（since C++11）</h2><p><a href="https://en.cppreference.com/w/cpp/language/lambda">Lambda expressions (since C++11) - cppreference.com</a></p><h3 id="非泛型lambda表达式">非泛型Lambda表达式</h3><h4 id="基本语法">基本语法</h4><p>没有显式模板参数列表的Lambda表达式的基本语法如下：</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">[capture](parameters) <span class="function"><span class="keyword">mutable</span> <span class="title">noexcept</span><span class="params">(expression)</span> -&gt; return_type </span>&#123;  </span><br><span class="line">    <span class="comment">// 函数体  </span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><strong>capture</strong>：捕获列表，指定Lambda表达式体内部可以访问的外部变量。捕获可以是值捕获（通过<code>=</code>或<code>&amp;</code>指定）或引用捕获（仅通过<code>&amp;</code>指定）。</li><li><strong>parameters</strong>：参数列表，与普通函数相同，但在这个上下文中是可选的。</li><li><strong>mutable</strong>、<strong>noexcept</strong>、<strong>-&gt; return_type</strong>：这些都是可选的。<code>mutable</code>允许在Lambda表达式体内修改捕获的变量（如果它们是值捕获的）。<code>noexcept</code>指定Lambda表达式是否抛出异常。<code>-&gt; return_type</code>指定Lambda表达式的返回类型，如果Lambda表达式体中有返回语句且编译器无法从返回语句推断出返回类型，则需要显式指定。</li></ul><h4 id="例子">例子</h4><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;algorithm&gt;</span>  </span></span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">    std::vector&lt;<span class="type">int</span>&gt; vec = &#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>&#125;;  </span><br><span class="line">  </span><br><span class="line">    <span class="comment">// 使用非泛型Lambda表达式对vector中的每个元素加1  </span></span><br><span class="line">    std::<span class="built_in">transform</span>(vec.<span class="built_in">begin</span>(), vec.<span class="built_in">end</span>(), vec.<span class="built_in">begin</span>(),  </span><br><span class="line">        [](<span class="type">int</span> x) &#123; <span class="keyword">return</span> x + <span class="number">1</span>; &#125;);  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> n : vec) &#123;  </span><br><span class="line">        std::cout &lt;&lt; n &lt;&lt; <span class="string">&#x27; &#x27;</span>;  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="comment">// 输出: 2 3 4 5 6  </span></span><br><span class="line">  </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>比如这个例子，Lambda表达式<code>[](int x) &#123; return x + 1; &#125;</code>是一个非泛型Lambda表达式，在这里的作用等同于：</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">bool</span> <span class="title">cmp</span><span class="params">(<span class="type">int</span> x)</span></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> x + <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line">std::<span class="built_in">transform</span>(vec.<span class="built_in">begin</span>(), vec.<span class="built_in">end</span>(), vec.<span class="built_in">begin</span>(),cmp);  </span><br></pre></td></tr></table></figure><h4 id="作用">作用</h4><p>非泛型Lambda表达式在C++中不能直接用来“定义”一个具有全局或命名空间作用域的函数，因为Lambda表达式本质上是匿名函数对象，它们通常在需要函数对象、回调函数或类似机制的地方使用。</p><p>但你可以将Lambda表达式赋值给一个<strong>函数指针</strong>（Lambda [capture list]为空其参数类型和返回类型与函数指针兼容），或者赋值给一个<code>std::function</code>对象。</p><p>如：</p><ul><li><strong>赋值函数指针</strong></li></ul><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;functional&gt;</span>  </span></span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">    <span class="comment">// 使用std::function来存储Lambda表达式  </span></span><br><span class="line">    std::function&lt;<span class="type">int</span>(<span class="type">int</span>, <span class="type">int</span>)&gt; add = [](<span class="type">int</span> x, <span class="type">int</span> y) &#123; <span class="keyword">return</span> x + y; &#125;;  </span><br><span class="line">    <span class="comment">// 也可以用auto来自动推导Lambda表达式 Since C++11</span></span><br><span class="line">  <span class="keyword">auto</span> add = [](<span class="type">int</span> x, <span class="type">int</span> y) &#123; <span class="keyword">return</span> x + y; &#125;; </span><br><span class="line">    <span class="comment">// 调用Lambda表达式  </span></span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;The sum is: &quot;</span> &lt;&lt; <span class="built_in">add</span>(<span class="number">2</span>, <span class="number">3</span>) &lt;&lt; std::endl;  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><strong>作为参数传递函数</strong></li></ul><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;algorithm&gt;</span>  </span></span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">printVector</span><span class="params">(<span class="type">const</span> std::vector&lt;<span class="type">int</span>&gt;&amp; vec, std::function&lt;<span class="type">void</span>(<span class="type">int</span>)&gt; printer)</span> </span>&#123;  </span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> n : vec) &#123;  </span><br><span class="line">        <span class="built_in">printer</span>(n);  </span><br><span class="line">    &#125;  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">    std::vector&lt;<span class="type">int</span>&gt; vec = &#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>&#125;;  </span><br><span class="line">  </span><br><span class="line">    <span class="comment">// 将Lambda表达式作为参数传递给printVector  </span></span><br><span class="line">    <span class="built_in">printVector</span>(vec, [](<span class="type">int</span> n) &#123; std::cout &lt;&lt; n &lt;&lt; <span class="string">&#x27; &#x27;</span>; &#125;);  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 算法笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LeetCode </tag>
            
            <tag> algorithm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CCF-CSP 刷题记录</title>
      <link href="/2024/09/19/LeetCode/CCF-CSP/"/>
      <url>/2024/09/19/LeetCode/CCF-CSP/</url>
      
        <content type="html"><![CDATA[<h1 id="ccf-csp-刷题记录">CCF-CSP 刷题记录</h1><blockquote><p>From 2024年9月21日</p></blockquote><h1 id="基础算法">基础算法</h1><h2 id="排序">排序</h2><h3 id="快速排序">快速排序</h3><p>快排函数定义即</p><ul><li>结束条件</li><li>随机划分轴</li><li>调整轴两侧顺序</li><li>递归执行左右部分</li></ul><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;random&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">quickSort</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; vec, <span class="type">int</span> left, <span class="type">int</span> right)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 结束条件</span></span><br><span class="line">    <span class="keyword">if</span> (left &gt;= right) </span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 随机选择 pivot</span></span><br><span class="line">    random_device rd;  </span><br><span class="line">    <span class="function">default_random_engine <span class="title">eng</span><span class="params">(rd())</span></span>;</span><br><span class="line">    <span class="function">uniform_int_distribution&lt;<span class="type">int</span>&gt; <span class="title">distr</span><span class="params">(left, right)</span></span>;</span><br><span class="line">    <span class="type">int</span> pivotIndex = <span class="built_in">distr</span>(eng);</span><br><span class="line">    <span class="type">int</span> pivot = vec[pivotIndex];</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Partition</span></span><br><span class="line">    <span class="type">int</span> i = left, j = right;</span><br><span class="line">    <span class="keyword">while</span> (i &lt;= j) &#123;</span><br><span class="line">        <span class="keyword">while</span> (vec[i] &lt; pivot) i++;</span><br><span class="line">        <span class="keyword">while</span> (vec[j] &gt; pivot) j--;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (i &lt;= j) &#123;</span><br><span class="line">            <span class="built_in">swap</span>(vec[i], vec[j]);</span><br><span class="line">            i++;</span><br><span class="line">            j--;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 递归排序左右部分</span></span><br><span class="line">    <span class="keyword">if</span> (left &lt; j)</span><br><span class="line">        <span class="built_in">quickSort</span>(vec, left, j);</span><br><span class="line">    <span class="keyword">if</span> (i &lt; right)</span><br><span class="line">        <span class="built_in">quickSort</span>(vec, i, right);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> num = <span class="number">0</span>, temp = <span class="number">0</span>;</span><br><span class="line">    vector&lt;<span class="type">int</span>&gt; res;</span><br><span class="line">    cin &gt;&gt; num;</span><br><span class="line">    <span class="keyword">while</span> (num-- &gt; <span class="number">0</span>) &#123;</span><br><span class="line">        cin &gt;&gt; temp;</span><br><span class="line">        res.<span class="built_in">push_back</span>(temp);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">quickSort</span>(res, <span class="number">0</span>, res.<span class="built_in">size</span>() - <span class="number">1</span>);</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> key : res) &#123;</span><br><span class="line">        cout &lt;&lt; key &lt;&lt; <span class="string">&quot; &quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    cout &lt;&lt; endl;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="归并排序">归并排序</h3><p>分治思想（n个数的排序，总共 <code>logn</code> 层，每层复杂度 <code>O(n)</code>）</p><ul><li>确定分界点 <code>mid = (l+r) &gt;&gt; 1</code></li><li>递归排序 left、right</li><li>有序数组归并（※）</li></ul><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">1e5</span> + <span class="number">10</span>;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> a[N], temp[N];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">merge_sort</span><span class="params">(<span class="type">int</span> q[], <span class="type">int</span> l, <span class="type">int</span> r)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(l &gt;= r) <span class="keyword">return</span>;</span><br><span class="line">    <span class="type">int</span> mid = (l+r) &gt;&gt; <span class="number">1</span>;</span><br><span class="line">    <span class="built_in">merge_sort</span>(q, l, mid), <span class="built_in">merge_sort</span>(q, mid+<span class="number">1</span> , r);</span><br><span class="line">    <span class="type">int</span> k = <span class="number">0</span>, i = l, j = mid + <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">while</span>(i &lt;= mid &amp;&amp; j &lt;= r)&#123;</span><br><span class="line">        <span class="keyword">if</span>(q[i] &lt;= q[j]) temp[k++] = q[i++];</span><br><span class="line">        <span class="keyword">else</span> temp[k++] = q[j++];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">while</span>(i &lt;= mid) temp[k++] = q[i++];</span><br><span class="line">    <span class="keyword">while</span>(j &lt;= r) temp[k++] = q[j++];</span><br><span class="line">    <span class="keyword">for</span>(i = l, j = <span class="number">0</span>; i &lt;= r; i++, j++) q[i] = temp[j];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="type">int</span> n, i = <span class="number">0</span>;</span><br><span class="line">    cin &gt;&gt; n;</span><br><span class="line">    <span class="keyword">while</span>(i &lt; n)&#123;</span><br><span class="line">        cin &gt;&gt; a[i];</span><br><span class="line">        i++;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">merge_sort</span>(a, <span class="number">0</span>, n<span class="number">-1</span>);</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> key : a)&#123;</span><br><span class="line">        <span class="keyword">if</span>(i == <span class="number">0</span>) <span class="keyword">break</span>;</span><br><span class="line">        i --;</span><br><span class="line">        cout &lt;&lt; key &lt;&lt; <span class="string">&quot; &quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>逆序对</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">1e5</span> + <span class="number">10</span>;</span><br><span class="line"><span class="type">int</span> a[N], temp[N];</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">long</span> <span class="type">long</span> <span class="type">int</span> <span class="title">merge_sort</span><span class="params">(<span class="type">int</span> vec[], <span class="type">int</span> l, <span class="type">int</span> r)</span></span>&#123;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span>(l &gt;= r) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    <span class="type">int</span> mid = (l + r) &gt;&gt; <span class="number">1</span>;</span><br><span class="line">    <span class="comment">// 在这一步就会将 mid 左右均排好序</span></span><br><span class="line">    <span class="type">long</span> <span class="type">long</span> <span class="type">int</span> num = <span class="built_in">merge_sort</span>(vec, l, mid) + <span class="built_in">merge_sort</span>(vec, mid+<span class="number">1</span>, r);</span><br><span class="line">    </span><br><span class="line">    <span class="type">int</span> k = <span class="number">0</span>, i = l, j = mid + <span class="number">1</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span>(i &lt;= mid &amp;&amp; j &lt;= r)&#123;</span><br><span class="line">        <span class="keyword">if</span>(vec[i] &gt; vec[j]) &#123;</span><br><span class="line">            temp[k++] = vec[j++];</span><br><span class="line">            <span class="comment">// 所以在左右排好序的情况下，mid ~ i 之间的数都是比 vec[j] 大的数，所以逆序对就是mid - i + 1</span></span><br><span class="line">            num += mid - i + <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> temp[k++] = vec[i++];</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span>(i&lt;=mid) temp[k++] = vec[i++];</span><br><span class="line">    <span class="keyword">while</span>(j&lt;=r) temp[k++] = vec[j++];</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span>(i=l,j=<span class="number">0</span>; i &lt;= r; i++,j++) vec[i] = temp[j];</span><br><span class="line">    <span class="keyword">return</span> num;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="type">int</span> n, i=<span class="number">0</span>;</span><br><span class="line">    cin &gt;&gt; n;</span><br><span class="line">    <span class="keyword">while</span>(i &lt; n)&#123;</span><br><span class="line">        cin &gt;&gt; a[i++];</span><br><span class="line">    &#125;</span><br><span class="line">    cout &lt;&lt; <span class="built_in">merge_sort</span>(a,<span class="number">0</span>,n<span class="number">-1</span>);</span><br><span class="line">    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="二分">二分</h2><p>二分理念的一个比较重要点在于它不是狭义的要有序数列，而是只要满足左侧条件和右侧条件的递归问题就可以支撑用二分方法来解决。</p><ul><li>结束条件（如 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mo>=</mo><mo>=</mo><mi>r</mi></mrow><annotation encoding="application/x-tex">l == r</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span></span><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span></span></span></span>）</li><li>计算 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mi>i</mi><mi>d</mi></mrow><annotation encoding="application/x-tex">mid</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">m</span><span class="mord mathdefault">i</span><span class="mord mathdefault">d</span></span></span></span></li><li>递归/循环 重复更新left、right、mid 直到找到目标</li></ul><p>二分查找数字区间</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">1e5</span> + <span class="number">10</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">binary_search_left</span><span class="params">(<span class="type">int</span> q[],<span class="type">int</span> key, <span class="type">int</span> l, <span class="type">int</span> r)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(l==r)&#123;<span class="keyword">if</span>(q[l] == key) <span class="keyword">return</span> l; <span class="keyword">return</span> <span class="number">-1</span>;&#125;</span><br><span class="line">    <span class="type">int</span> mid = (l+r) &gt;&gt; <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">if</span>(q[mid] &gt;= key) <span class="keyword">return</span> <span class="built_in">binary_search_left</span>(q, key, l, mid);</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">return</span> <span class="built_in">binary_search_left</span>(q, key, mid+<span class="number">1</span>, r);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">binary_search_right</span><span class="params">(<span class="type">int</span> q[],<span class="type">int</span> key, <span class="type">int</span> l, <span class="type">int</span> r)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(l == r)&#123;<span class="keyword">if</span>(q[l] == key) <span class="keyword">return</span> l; <span class="keyword">return</span> <span class="number">-1</span>;&#125;</span><br><span class="line">    <span class="type">int</span> mid = (l+r+<span class="number">1</span>) &gt;&gt; <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">if</span>(q[mid] &gt; key) <span class="keyword">return</span> <span class="built_in">binary_search_right</span>(q, key, l, mid<span class="number">-1</span>);</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">return</span> <span class="built_in">binary_search_right</span>(q, key, mid, r);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="type">int</span> num_1, num_2;</span><br><span class="line">    cin &gt;&gt; num_1 &gt;&gt; num_2;</span><br><span class="line">    <span class="type">int</span> arr[num_1];</span><br><span class="line">    <span class="type">int</span> i = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span>(i &lt; num_1)&#123;</span><br><span class="line">        cin &gt;&gt; arr[i];</span><br><span class="line">        i ++;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> key;</span><br><span class="line">    <span class="keyword">while</span>(num_2--&gt;<span class="number">0</span>)&#123;</span><br><span class="line">        cin &gt;&gt; key;</span><br><span class="line">        cout &lt;&lt; <span class="built_in">binary_search_left</span>(arr, key, <span class="number">0</span>, num_1<span class="number">-1</span>) &lt;&lt; <span class="string">&quot; &quot;</span> &lt;&lt; <span class="built_in">binary_search_right</span>(arr, key, <span class="number">0</span>, num_1<span class="number">-1</span>) &lt;&lt; endl;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>二分用非递归也可以实现，空间损耗会更少，比如这个三次方根的题目。</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;cmath&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iomanip&gt;</span> <span class="comment">// 用于设置输出精度  </span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;  </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">double</span> <span class="title">pow3_f</span><span class="params">(<span class="type">double</span> num)</span></span>&#123;  </span><br><span class="line">    <span class="keyword">return</span> num * num * num;  </span><br><span class="line">&#125;  </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">double</span> <span class="title">cube</span><span class="params">(<span class="type">double</span> num)</span></span>&#123;  </span><br><span class="line">    <span class="type">double</span> left, right, mid;  </span><br><span class="line">    <span class="keyword">if</span>(num &lt; <span class="number">1.0f</span>)&#123;</span><br><span class="line">        left = num;</span><br><span class="line">        right = <span class="number">1.0f</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span>&#123;</span><br><span class="line">        left = <span class="number">0</span>;  </span><br><span class="line">        right = num;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">while</span> (right - left &gt; <span class="number">1e-7</span>) &#123;  </span><br><span class="line">        mid = (left + right) / <span class="number">2.0</span>;  </span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">pow3_f</span>(mid) &lt; num) &#123;  </span><br><span class="line">            left = mid;  </span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;  </span><br><span class="line">            right = mid;  </span><br><span class="line">        &#125;  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="keyword">return</span> mid;  </span><br><span class="line">&#125;  </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;  </span><br><span class="line">    <span class="type">double</span> num;  </span><br><span class="line">    cin &gt;&gt; num;</span><br><span class="line">    <span class="keyword">if</span>(num &lt; <span class="number">0</span>)&#123;</span><br><span class="line">        num = <span class="number">0.0f</span> - num;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;-%.6f&quot;</span>, <span class="built_in">cube</span>(num));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;%.6f&quot;</span>, <span class="built_in">cube</span>(num));</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="高精度">高精度</h2><p>一般情况是 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><msup><mn>0</mn><mn>6</mn></msup></mrow><annotation encoding="application/x-tex">10^6</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">6</span></span></span></span></span></span></span></span></span></span></span> 的大数与一个较小的数进行计算。</p><p>我们需要对<strong>C/C++基础数字类型的取值范围</strong>有一个概念。</p><table><thead><tr><th>类型</th><th>取值范围</th><th>位数</th></tr></thead><tbody><tr><td>unsigned  int</td><td>0～4294967295</td><td>10位</td></tr><tr><td>int</td><td>2147483648～2147483647</td><td>10位</td></tr><tr><td>unsigned long</td><td>0～4294967295</td><td>10位</td></tr><tr><td>long</td><td>2147483648～2147483647</td><td>10位</td></tr><tr><td>long long</td><td>-9223372036854775808~9223372036854775807</td><td>19位</td></tr><tr><td>unsigned long long</td><td>0~1844674407370955161</td><td>19位</td></tr></tbody></table><p>存储在数组，逆序存储，方便进位。实际计算是一个模拟计算的过程。</p><h3 id="加法">加法</h3><ol><li><strong>使用字符串输入</strong>：<ul><li>使用字符串来读取输入，这样可以处理任意长度的数字。</li></ul></li><li><strong>逐位相加</strong>：<ul><li>从个位开始逐位相加，处理每一位的进位。</li></ul></li><li><strong>处理进位</strong>：<ul><li>在每次相加后，计算进位 <code>carry</code> 并在下一位加法中使用。</li></ul></li><li><strong>输出结果</strong>：<ul><li>将结果存储在一个向量中，并在最后反向输出以得到正确的顺序。</li></ul></li></ol><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;string&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;algorithm&gt;</span> <span class="comment">// 包含 reverse 函数</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    string a, b;</span><br><span class="line">    cin &gt;&gt; a &gt;&gt; b;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 反转字符串以便从个位开始处理</span></span><br><span class="line">    <span class="built_in">reverse</span>(a.<span class="built_in">begin</span>(), a.<span class="built_in">end</span>());</span><br><span class="line">    <span class="built_in">reverse</span>(b.<span class="built_in">begin</span>(), b.<span class="built_in">end</span>());</span><br><span class="line"></span><br><span class="line">    vector&lt;<span class="type">int</span>&gt; res;</span><br><span class="line">    <span class="type">int</span> carry = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">auto</span> maxSize = <span class="built_in">max</span>(a.<span class="built_in">size</span>(), b.<span class="built_in">size</span>());</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (std::string::size_type i = <span class="number">0</span>; i &lt; maxSize; ++i) &#123;</span><br><span class="line">        <span class="type">int</span> digitA = (i &lt; a.<span class="built_in">size</span>()) ? a[i] - <span class="string">&#x27;0&#x27;</span> : <span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> digitB = (i &lt; b.<span class="built_in">size</span>()) ? b[i] - <span class="string">&#x27;0&#x27;</span> : <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        <span class="type">int</span> sum = digitA + digitB + carry;</span><br><span class="line">        carry = sum / <span class="number">10</span>;</span><br><span class="line">        res.<span class="built_in">push_back</span>(sum % <span class="number">10</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 如果最后还有进位</span></span><br><span class="line">    <span class="keyword">if</span> (carry) &#123;</span><br><span class="line">        res.<span class="built_in">push_back</span>(carry);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 输出结果（反转回正确的顺序）</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = res.<span class="built_in">size</span>() - <span class="number">1</span>; i &gt;= <span class="number">0</span>; --i) &#123;</span><br><span class="line">        cout &lt;&lt; res[i];</span><br><span class="line">    &#125;</span><br><span class="line">    cout &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="减法">减法</h3><ol><li><strong>输入处理</strong>：<ul><li><strong>字符串输入</strong>：由于大数可能超出标准整数类型的范围，通常以字符串形式读取输入。</li><li><strong>确保被减数大于减数</strong>：为了简化计算，通常先比较两个数的大小，确保被减数大于或等于减数。如果不是，则交换二者并标记结果为负。</li></ul></li><li><strong>逐位计算</strong>：<ul><li><strong>反转字符串</strong>：为了从低位（个位）开始计算，通常先将字符串反转。</li><li><strong>逐位相减</strong>：从低位到高位逐位相减。如果当前位的被减数小于减数，则需要从更高位借位。</li></ul></li><li><strong>借位处理</strong>：<ul><li><strong>借位调整</strong>：如果当前位不够减，需要从更高的一位借 1，这样当前位加 10。注意借位后，下一位的被减数要减 1。</li><li><strong>处理借位</strong>：确保所有位都正确处理借位，避免出现负数。</li></ul></li><li><strong>结果处理</strong>：<ul><li><strong>去除前导零</strong>：计算完成后，可能会有前导零，需要从结果中去除。</li><li><strong>处理负号</strong>：如果在最初交换了两个数，结果应加上负号。</li></ul></li><li><strong>输出结果</strong>：<ul><li><strong>特殊情况</strong>：如果结果为零，直接输出 <code>0</code>。</li><li><strong>输出字符串</strong>：将结果从高位到低位输出，形成最终的结果字符串。</li></ul></li></ol><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;string&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;algorithm&gt;</span> <span class="comment">// 包含 reverse 函数</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    string a, b, temp;</span><br><span class="line">    cin &gt;&gt; a &gt;&gt; b;</span><br><span class="line">    <span class="type">bool</span> isNeg = <span class="literal">false</span>;</span><br><span class="line">    <span class="comment">// 剪枝</span></span><br><span class="line">    <span class="keyword">if</span>(a == b) &#123;cout &lt;&lt; <span class="number">0</span>; <span class="keyword">return</span> <span class="number">0</span>;&#125;</span><br><span class="line">    <span class="comment">// 确保 a 是较大的数</span></span><br><span class="line">    <span class="keyword">if</span> (a.<span class="built_in">size</span>() &lt; b.<span class="built_in">size</span>() || (a.<span class="built_in">size</span>() == b.<span class="built_in">size</span>() &amp;&amp; a &lt; b)) &#123;</span><br><span class="line">        isNeg = <span class="literal">true</span>;</span><br><span class="line">        temp = a;</span><br><span class="line">        a = b;</span><br><span class="line">        b = temp;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">reverse</span>(a.<span class="built_in">begin</span>(), a.<span class="built_in">end</span>());</span><br><span class="line">    <span class="built_in">reverse</span>(b.<span class="built_in">begin</span>(), b.<span class="built_in">end</span>());</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> del = <span class="number">0</span>, digitA, digitB, dif;</span><br><span class="line">    vector&lt;<span class="type">unsigned</span> <span class="type">int</span>&gt; res;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// a &gt; b </span></span><br><span class="line">    <span class="keyword">for</span> (string::size_type i = <span class="number">0</span>; i &lt; a.<span class="built_in">size</span>(); i++) &#123;</span><br><span class="line">        digitA = a[i] - <span class="string">&#x27;0&#x27;</span>;</span><br><span class="line">        digitB = i &lt; b.<span class="built_in">size</span>() ? b[i] - <span class="string">&#x27;0&#x27;</span> : <span class="number">0</span>;</span><br><span class="line">        digitA -= del;</span><br><span class="line">        del = <span class="number">0</span>;</span><br><span class="line">        dif = digitA - digitB;</span><br><span class="line">        <span class="keyword">if</span> (dif &lt; <span class="number">0</span>) &#123;</span><br><span class="line">            del = <span class="number">1</span>;</span><br><span class="line">            dif += <span class="number">10</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        res.<span class="built_in">push_back</span>(dif);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (isNeg) cout &lt;&lt; <span class="string">&#x27;-&#x27;</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 需要忽略前导0</span></span><br><span class="line">    <span class="type">bool</span> isZero = <span class="literal">true</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = res.<span class="built_in">size</span>() - <span class="number">1</span>; i &gt;= <span class="number">0</span>; --i) &#123;</span><br><span class="line">        <span class="keyword">if</span> (isZero &amp;&amp; res[i] == <span class="number">0</span>) <span class="keyword">continue</span>;</span><br><span class="line">        isZero = <span class="literal">false</span>;</span><br><span class="line">        cout &lt;&lt; res[i];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 如果结果是0</span></span><br><span class="line">    <span class="keyword">if</span> (isZero) cout &lt;&lt; <span class="string">&#x27;0&#x27;</span>;</span><br><span class="line"></span><br><span class="line">    cout &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LeetCode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Self-Learning 序</title>
      <link href="/2024/09/19/Self-Learning/Self-Learning/"/>
      <url>/2024/09/19/Self-Learning/Self-Learning/</url>
      
        <content type="html"><![CDATA[<h1 id="序">序</h1><p>计算机领域是一个庞大的领域，有无数的分支难以全部掌握，每个分支领域都可以说是无止境。所以对于踏入这个领域的人来说，一个明确且清晰的学习规划是非常重要的。我在本科的时候自学和学习走过很多弯路，也躺平过。经过这几年的反思，希望在研究生期间能稳定的发展下去，能在这个浩如烟海的领域里学习更为细致且深入的内容。</p><blockquote><p>由我个人感觉，学习的效率和结果会因为多种因素出现很大的不同，包括但不限于信息壁垒、方法与技巧等</p></blockquote><p>所以我准备写一系列的自学笔记，既是对自己的记录与推进，也是对大家的分享和帮助。人生在世，普通人需要担心的无非就两个，就业与学业。我会从这两个来讲自己的经历与笔记。</p><h1 id="就业">就业</h1><blockquote><p>由于时间和精力的限制，我只会讲解与自己相关的东西，如果方向与我不一样，也可以参考参考，毕竟大家也许很多地方都会有类似的考虑与疑惑。</p></blockquote><p>对于经历过秋招和春招的我来说，就业不是一个让人迷茫的词。在我看来，准备就业其实只有一个方向，那就是丰富你的简历，在HR眼中也只能看到你的简历。所以我从研一开始就计划着丰富自己的简历，我的一切准备也是从丰富简历开始。接下来的讲解没有先后顺序，他们都同样重要；同样的，我也是在同时准备以下这些内容。</p><h2 id="比赛">比赛</h2><h2 id="项目">项目</h2><h2 id="技术">技术</h2><h3 id="c-：-cs106l-standard-c-programming"><strong>C++：</strong> CS106L: Standard C++ Programming</h3><a href="/2024/09/19/CS-Class/CS106L/" title="CS106L 学习记录">CS106L 学习记录</a><h3 id="操作系统：-cs162-operating-system"><strong>操作系统：</strong> CS162: Operating System</h3><h3 id="分布式计算：-cmu-15-418-stanford-cs149-parallel-computing">**分布式计算： **CMU 15-418/Stanford CS149: Parallel Computing</h3><h3 id="ai-system">AI System</h3><p><a href="https://chenzomi12.github.io/">课程内容大纲 — AI System (chenzomi12.github.io)</a></p><h2 id="算法">算法</h2><p>我为什么专门把<strong>算法</strong>拎出来，或许它其实应该包含在<strong>技术</strong>里，但是我把它排除在外是因为当前大部分公司对于我们计算机岗位的笔试都是考察你的算法功底。我们将找工作做一个排序，那么就是 <strong>简历</strong>-》<strong>笔试</strong>-》<strong>面试</strong>。那么我也提过，比赛和项目其实就是丰富<strong>简历</strong>，而<strong>技术</strong>就是让你通过面试，而<strong>算法</strong>是你进入面试的门槛。</p><h3 id="ccf-csp">CCF-CSP</h3><a href="/2024/09/19/LeetCode/CCF-CSP/" title="CCF-CSP 刷题记录">CCF-CSP 学习记录</a><h3 id="leetcode">LeetCode</h3><a href="/2024/09/19/LeetCode/LeetCodeBook/" title="算法笔记">LeetCode刷题</a><h1 id="学业">学业</h1><h3 id="论文">论文</h3><p><a href="https://space.bilibili.com/1567748478/channel/collectiondetail?sid=32744">跟李沐学AI</a></p><h3 id="基础">基础</h3><h2 id="参考">参考</h2><p><a href="https://csdiy.wiki/">CS自学指南 (csdiy.wiki)</a></p><p><a href="https://labuladong.online/algo/home/">labuladong的算法笔记</a></p>]]></content>
      
      
      <categories>
          
          <category> 自学 </category>
          
          <category> 笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> notes </tag>
            
            <tag> self learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>如何使用 pytorch 创建一个神经网络</title>
      <link href="/2024/07/01/DL-notes/create_neural_network_post/"/>
      <url>/2024/07/01/DL-notes/create_neural_network_post/</url>
      
        <content type="html"><![CDATA[<h1 id="构建神经网络">构建神经网络</h1><h2 id="1-导入所需包">1 导入所需包</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms</span><br></pre></td></tr></table></figure><h2 id="2-检查gpu是否可用">2 检查GPU是否可用</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">device = (</span><br><span class="line">    <span class="string">&quot;cuda&quot;</span></span><br><span class="line">    <span class="keyword">if</span> torch.cuda.is_available()</span><br><span class="line">    <span class="keyword">else</span> <span class="string">&quot;mps&quot;</span></span><br><span class="line">    <span class="keyword">if</span> torch.backends.mps.is_available()</span><br><span class="line">    <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span></span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Using <span class="subst">&#123;device&#125;</span> device&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(torch.__version__)</span><br><span class="line"><span class="built_in">print</span>(torch.cuda.is_available())</span><br><span class="line"><span class="built_in">print</span>(torch.version.cuda)</span><br></pre></td></tr></table></figure><pre><code>Using cuda device2.2.2+cu121True12.1</code></pre><p>如果发现GPU不可用，可能是因为torch版本问题（比如我），应该去下载GPU版本。</p><ul><li>在CUDA官网找到合适版本的<a href="https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html">cuda</a>，一般是根据系统平台和显卡版本来选择所需CUDA</li><li>查看安装完成是否</li></ul><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">版本，看CUDA的版本，比如我的是cuda_11.2</span></span><br><span class="line">nvcc --version</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">驱动，看Driver Version</span></span><br><span class="line">nvidia-smi</span><br></pre></td></tr></table></figure><ul><li>去PyTorch官网找到合适版本的<a href="https://pytorch.org/get-started/previous-versions/">PyTorch</a>，一般是根据开发环境来选择，然后复制所给的Commond去shell下安装即可</li></ul><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">比如我的命令就是</span></span><br><span class="line">pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cu121</span><br></pre></td></tr></table></figure><h2 id="3-定义我们的神经网络">3 定义我们的神经网络</h2><p>pytorch里面一切自定义操作基本上都是继承<code>nn.Module</code>类来实现的，你可以先不去深入了解这个类，但是要知道，我们一般都是通过继承和重构<code>nn.Module</code>来定义我们的神经网络。我们一般重构<code>__init__</code>和<code>forward</code>这两个方法。根据PyTorch官网的说法：<code>__init__</code>初始化神经网络层；<code>forward</code>层之间的数据操作，也是整个网络的核心。<code>__init__</code>只会定义层，而<code>forward</code>负责将层连接起来。实际上类的初始化参数一般是一些<code>固有属性</code>，我们可以将一些带有训练参数的层放在<code>__init__</code>，而没有训练参数的层是可以加入到<code>forward</code>里面的，或者说我们将没有训练参数的层看作是层之间的数据操作。</p><p>当然直接这么说，肯定不是很清晰，我们来看一个官网给的例子：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">NeuralNetwork</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.flatten = nn.Flatten()</span><br><span class="line">        self.linear_relu_stack = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">28</span>*<span class="number">28</span>, <span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">10</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.flatten(x)</span><br><span class="line">        logits = self.linear_relu_stack(x)</span><br><span class="line">        <span class="keyword">return</span> logits</span><br><span class="line">model = NeuralNetwork().to(device) <span class="comment"># 将网络移入device，并打印结构</span></span><br><span class="line"><span class="built_in">print</span>(model)</span><br></pre></td></tr></table></figure><pre><code>NeuralNetwork(  (flatten): Flatten(start_dim=1, end_dim=-1)  (linear_relu_stack): Sequential(    (0): Linear(in_features=784, out_features=512, bias=True)    (1): ReLU()    (2): Linear(in_features=512, out_features=512, bias=True)    (3): ReLU()    (4): Linear(in_features=512, out_features=10, bias=True)  ))</code></pre><p>我们用torchviz可以将神经网络进行一个简单的可视化，当然很多参数可选，这里不一一列举</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">pip install torchviz</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torchviz <span class="keyword">import</span> make_dot</span><br><span class="line">X = torch.rand(<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>, device=device) <span class="comment"># 需要对神经网络进行数据输入，才是一个完整的网络</span></span><br><span class="line">y = model(X)</span><br><span class="line">output = make_dot(y.mean(), params=<span class="built_in">dict</span>(model.named_parameters())) <span class="comment"># 开始绘制</span></span><br><span class="line">output.<span class="built_in">format</span> = <span class="string">&quot;png&quot;</span></span><br><span class="line">output.directory = <span class="string">&quot;.&quot;</span></span><br><span class="line">output.render(<span class="string">&quot;torchviz&quot;</span>, view=<span class="literal">True</span>) <span class="comment"># 会在相对路径下保存一个可视化后的图片并打开</span></span><br></pre></td></tr></table></figure><pre><code>'torchviz.png'</code></pre><p>看起来可能会比较复杂，也看不懂，我们得需要学会看懂神经网络的结构才能看懂结构图。当然，还有诸如draw_convnet、NNSVG、netron等可视化工具会更加优秀。</p><h2 id="4-神经网络模型层">4 神经网络模型层</h2><p>想要构建一个神经网络并进行训练和预测，我们需要去认识神经网络的构成。假定你已经了解过感知机、人工神经网络的基本概念，那么现在就是来了解一下神经网络的模型层。</p><p>我们直接分解一下官网所给出的这个模型，这是一个简单的前馈神经网络（Feedforward Neural Network），我们先不去了解它的作用，一点点的分解它，看看它最终实现了什么。</p><p>我们先根据官网所说的，取一个大小为 28x28 的 3 张图像的样本小批量作为输入（我们一般将数据的第一个维度看作批量维度并保留）：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">input_image = torch.rand(<span class="number">3</span>,<span class="number">28</span>,<span class="number">28</span>)</span><br><span class="line"><span class="built_in">print</span>(input_image.size())</span><br></pre></td></tr></table></figure><pre><code>torch.Size([3, 28, 28])</code></pre><h3 id="4-1-nn-flatten">4.1 nn.Flatten</h3><p>虽然是PyTorch的<code>nn.Flatten</code>，但是Flatten层是神经网络中常见的组成部分。在神经网络的训练和预测过程中，输入数据通常需要经过一系列的处理和转换。在这个过程中，Flatten层能够将多维的输入数据转化为一维的线性形式，以便于神经网络的进一步处理。模型中的<code>nn.Flatten</code>，将我们所输入的2D 28*28 图像转换为一个包含 784 个像素值的连续数组，也就是和它表面的意思一样展平这个高维数组。</p><blockquote><p>（<code>nn.Flatten()</code>默认参数是<code>start_dim=1</code>和<code>end_dim=-1</code>，如果你想展平所有维度，可以通过设置<code>start_dim=0</code>来实现）</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">flatten = nn.Flatten()</span><br><span class="line">flat_image = flatten(input_image) <span class="comment"># 将输入的图像展平</span></span><br><span class="line"><span class="built_in">print</span>(flat_image.size())</span><br></pre></td></tr></table></figure><pre><code>torch.Size([3, 784])</code></pre><p>在卷积神经网络（CNN）中，<code>Flatten</code>层可以将卷积层提取到的特征图展平，便于进一步的特征处理或分类，也便于输入到全连接层（全连接层通常需要一维的输入，后面会讲到）。在构建复杂网络时，<code>Flatten</code>层可以帮助不同类型的层之间进行连接。总的来说，<code>Flatten</code>层起到了桥梁的作用，使得卷积神经网络的层次结构更加灵活和易于设计，并且确保了从卷积层到全连接层的数据传递顺畅，维持了网络的整体性能和效率。</p><h3 id="4-2-nn-linear">4.2 nn.Linear</h3><p><code>nn.Linear</code>应该耳熟能详，我们称之为线性层（Linear Layer），也可以称为全连接层（Fully Connected Layer）或密集层（Dense Layer）。线性层是一个使用其存储的权重和偏差对输入应用线性变换的模块，也就是对输入数据进行线性变换。线性层对数据的处理方式基本上可以表示为：</p><p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mi>W</mi><mi>x</mi><mo>+</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">y = Wx + b </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">b</span></span></span></span></span></p><p>，其中 W 是权重矩阵，b 是偏置。向量都是可学习的参数。在神经网络的训练和预测过程中，<code>Linear</code>层的作用是将输入数据通过一组权重进行线性变换，然后添加一个偏置项。简单来说，它能够将输入特征映射到输出特征，从而实现对数据的线性组合和转换。如下图是一个单隐藏层的多层感知机（Multilayer Perceptron），一般称为MLP，隐藏层和输出层均是由线性层和激活函数组成：</p><p><img src="/images/DL-pics/create_neural_network_files/MLP.png#pic_center" alt="png"></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义一个线性层，将28*28维度的向量转换为20维度的向量</span></span><br><span class="line">layer1 = nn.Linear(in_features=<span class="number">28</span>*<span class="number">28</span>, out_features=<span class="number">20</span>)</span><br><span class="line">hidden1 = layer1(flat_image) </span><br><span class="line"><span class="built_in">print</span>(hidden1.size())</span><br></pre></td></tr></table></figure><pre><code>torch.Size([3, 20])</code></pre><blockquote><p>在这个例子中，in_features=28*28表示输入特征的维度，out_features=20表示输出特征的维度。nn.Linear层会自动初始化权重和偏置，并在训练过程中通过<strong>反向传播算法</strong>进行调整。简单理解就是，该线性层的输入是784维，而输出是20维。</p></blockquote><h3 id="4-3-nn-relu">4.3 nn.ReLU</h3><p>ReLU函数，全称Rectified Linear Unit，是人工神经网络中常用的一种激活函数.<br>讲到这里，我们就讲讲常见的激活函数及其作用。</p><h4 id="sigmoid-激活函数">Sigmoid 激活函数</h4><p><strong>数学表达式：</strong></p><p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>σ</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>x</mi></mrow></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">\sigma(x) = \frac{1}{1 + e^{-x}} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">σ</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.09077em;vertical-align:-0.7693300000000001em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.697331em;"><span style="top:-2.989em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathdefault mtight">x</span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.7693300000000001em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p><p><strong>作用：</strong></p><ul><li>将输入映射到 (0, 1) 之间。</li><li>常用于输出层，尤其是在二分类问题中，输出概率值。</li></ul><p><strong>优点：</strong></p><ul><li>输出范围在 (0, 1) 之间，可以解释为概率。</li><li>平滑梯度，有助于梯度下降。</li></ul><p><strong>缺点：</strong></p><ul><li>容易导致梯度消失问题。</li><li>输出不是零中心的，会影响网络的训练效率。</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-x))</span><br><span class="line"></span><br><span class="line">x = np.linspace(-<span class="number">10</span>, <span class="number">10</span>, <span class="number">100</span>)</span><br><span class="line">y = sigmoid(x)</span><br><span class="line"></span><br><span class="line">plt.plot(x, y)</span><br><span class="line">plt.title(<span class="string">&quot;Sigmoid Activation Function&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;x&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Sigmoid(x)&quot;</span>)</span><br><span class="line">plt.grid()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/images/DL-pics/create_neural_network_files/create_neural_network_20_0.png#pic_center" alt="png"></p><h4 id="tanh-激活函数">Tanh 激活函数</h4><p><strong>数学表达式：</strong></p><p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>tanh</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><msup><mi>e</mi><mi>x</mi></msup><mo>−</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>x</mi></mrow></msup></mrow><mrow><msup><mi>e</mi><mi>x</mi></msup><mo>+</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>x</mi></mrow></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">tanh</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.217661em;vertical-align:-0.7693300000000001em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.448331em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.590392em;"><span style="top:-2.9890000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">x</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.697331em;"><span style="top:-2.989em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathdefault mtight">x</span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">x</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.771331em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathdefault mtight">x</span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.7693300000000001em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p><p><strong>作用：</strong></p><ul><li>将输入映射到 (-1, 1) 之间。</li><li>常用于隐藏层，提供零中心的输出，有助于训练。</li></ul><p><strong>优点：</strong></p><ul><li>输出是零中心的，梯度消失问题较轻。</li></ul><p><strong>缺点：</strong></p><ul><li>仍然存在梯度消失问题。</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">tanh</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> np.tanh(x)</span><br><span class="line"></span><br><span class="line">x = np.linspace(-<span class="number">10</span>, <span class="number">10</span>, <span class="number">100</span>)</span><br><span class="line">y = tanh(x)</span><br><span class="line"></span><br><span class="line">plt.plot(x, y)</span><br><span class="line">plt.title(<span class="string">&quot;Tanh Activation Function&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;x&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Tanh(x)&quot;</span>)</span><br><span class="line">plt.grid()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/images/DL-pics/create_neural_network_files/create_neural_network_22_0.png#pic_center" alt="png"></p><h4 id="relu-rectified-linear-unit-激活函数">ReLU (Rectified Linear Unit) 激活函数</h4><p><strong>数学表达式：</strong></p><p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>ReLU</mtext><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>max</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{ReLU}(x) = \max(0, x) </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">ReLU</span></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">max</span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span></span></span></p><p><strong>作用：</strong></p><ul><li>将输入小于0的部分设为0，大于0的部分保持不变。</li><li>常用于隐藏层，特别是深度神经网络。</li></ul><p><strong>优点：</strong></p><ul><li>计算简单，收敛速度快。</li><li>减少梯度消失问题。</li></ul><p><strong>缺点：</strong></p><ul><li>输出不是零中心的。</li><li>输入小于0时梯度为零，可能导致“神经元死亡”问题。</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">relu</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> np.maximum(<span class="number">0</span>, x)</span><br><span class="line"></span><br><span class="line">x = np.linspace(-<span class="number">10</span>, <span class="number">10</span>, <span class="number">100</span>)</span><br><span class="line">y = relu(x)</span><br><span class="line"></span><br><span class="line">plt.plot(x, y)</span><br><span class="line">plt.title(<span class="string">&quot;ReLU Activation Function&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;x&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;ReLU(x)&quot;</span>)</span><br><span class="line">plt.grid()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/images/DL-pics/create_neural_network_files/create_neural_network_24_0.png#pic_center" alt="png"></p><h4 id="leaky-relu-激活函数">Leaky ReLU 激活函数</h4><p><strong>数学表达式：</strong></p><p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>Leaky ReLU</mtext><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mrow><mo fence="true">{</mo><mtable rowspacing="0.3599999999999999em" columnalign="left left" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mi>x</mi></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mtext>if </mtext><mi>x</mi><mo>&gt;</mo><mn>0</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>α</mi><mi>x</mi></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mtext>if </mtext><mi>x</mi><mo>≤</mo><mn>0</mn></mrow></mstyle></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex"> \text{Leaky ReLU}(x) = \begin{cases}       x &amp; \text{if } x &gt; 0 \\      \alpha x &amp; \text{if } x \leq 0    \end{cases}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">Leaky ReLU</span></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:3.0000299999999998em;vertical-align:-1.25003em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size4">{</span></span><span class="mord"><span class="mtable"><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.69em;"><span style="top:-3.69em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord mathdefault">x</span></span></span><span style="top:-2.25em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.0037em;">α</span><span class="mord mathdefault">x</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.19em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:1em;"></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.69em;"><span style="top:-3.69em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord text"><span class="mord">if </span></span><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord">0</span></span></span><span style="top:-2.25em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord text"><span class="mord">if </span></span><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≤</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.19em;"><span></span></span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p><p>其中 (\alpha) 通常是一个很小的常数，如 0.01。</p><p><strong>作用：</strong></p><ul><li>解决 ReLU 的“神经元死亡”问题。</li></ul><p><strong>优点：</strong></p><ul><li>输入小于0时仍有较小梯度，避免神经元死亡。</li></ul><p><strong>缺点：</strong></p><ul><li>计算稍复杂。</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">leaky_relu</span>(<span class="params">x, alpha=<span class="number">0.01</span></span>):</span><br><span class="line">    <span class="keyword">return</span> np.where(x &gt; <span class="number">0</span>, x, alpha * x)</span><br><span class="line"></span><br><span class="line">x = np.linspace(-<span class="number">10</span>, <span class="number">10</span>, <span class="number">100</span>)</span><br><span class="line">y = leaky_relu(x)</span><br><span class="line"></span><br><span class="line">plt.plot(x, y)</span><br><span class="line">plt.title(<span class="string">&quot;Leaky ReLU Activation Function&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;x&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Leaky ReLU(x)&quot;</span>)</span><br><span class="line">plt.grid()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/images/DL-pics/create_neural_network_files/create_neural_network_26_0.png#pic_center" alt="png"></p><h4 id="softmax-激活函数">Softmax 激活函数</h4><p><strong>数学表达式：</strong></p><p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>Softmax</mtext><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mfrac><msup><mi>e</mi><msub><mi>x</mi><mi>i</mi></msub></msup><mrow><munder><mo>∑</mo><mi>j</mi></munder><msup><mi>e</mi><msub><mi>x</mi><mi>j</mi></msub></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">\text{Softmax}(x_i) = \frac{e^{x_i}}{\sum_{j} e^{x_j}} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">Softmax</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.46321em;vertical-align:-1.1218180000000002em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.341392em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.16195399999999993em;"><span style="top:-2.40029em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.43581800000000004em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6064620000000001em;"><span style="top:-3.0050700000000004em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3280857142857143em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2818857142857143em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3280857142857143em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.1218180000000002em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p><p><strong>作用：</strong></p><ul><li>将输入向量转换为概率分布，总和为1。</li><li>常用于多分类问题的输出层。</li></ul><p><strong>优点：</strong></p><ul><li>输出可以解释为概率，便于分类。</li></ul><p><strong>缺点：</strong></p><ul><li>计算相对复杂，容易导致数值不稳定。</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">softmax</span>(<span class="params">x</span>):</span><br><span class="line">    e_x = np.exp(x - np.<span class="built_in">max</span>(x))</span><br><span class="line">    <span class="keyword">return</span> e_x / e_x.<span class="built_in">sum</span>(axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">x = np.linspace(-<span class="number">10</span>, <span class="number">10</span>, <span class="number">100</span>)</span><br><span class="line">y = softmax(x)</span><br><span class="line"></span><br><span class="line">plt.plot(x, y)</span><br><span class="line">plt.title(<span class="string">&quot;Softmax Activation Function&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;x&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Softmax(x)&quot;</span>)</span><br><span class="line">plt.grid()</span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="/images/DL-pics/create_neural_network_files/create_neural_network_28_0.png#pic_center" alt="png"></p><h3 id="4-4-nn-sequential">4.4 nn.Sequential</h3><p><code>nn.Sequential</code>是 PyTorch 提供的一个容器模块，它按顺序包含其他子模块，便于构建和管理简单的神经网络结构。通过 nn.Sequential，可以方便地将一系列层（如线性层、激活函数、卷积层等）按顺序堆叠在一起，从而简化模型定义和前向传播的代码。简而言之就是一个包裹的顺序容器。</p><h2 id="5-理解我们的神经网络">5 理解我们的神经网络</h2><p>看完这些，我们再来理解这个官网给的例子：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">NeuralNetwork</span>(nn.Module):</span><br><span class="line">    <span class="comment"># 重构 __init__，定义“固有属性”</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 这一步操作是调用父类 nn.Module 的构造函数，确保继承自 nn.Module 的特性正确初始化</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 定义一个展开层 flatten</span></span><br><span class="line">        self.flatten = nn.Flatten()</span><br><span class="line">        <span class="comment"># 定义一个线性容器，可以简化在forward中的调用</span></span><br><span class="line">        self.linear_relu_stack = nn.Sequential(</span><br><span class="line">            <span class="comment"># 容器内包含一个三层网络</span></span><br><span class="line">            <span class="comment"># 这里的512、10都是研究者根据具体任务和数据集进行调试和优化得到的结果</span></span><br><span class="line">            <span class="comment"># 熟悉的调参</span></span><br><span class="line">            nn.Linear(<span class="number">28</span>*<span class="number">28</span>, <span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">10</span>),</span><br><span class="line">        )</span><br><span class="line">    <span class="comment"># 重构forward，定义前向传播路径</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 在这里定义各个层输入输出的顺序，即层在网络里的位置关系</span></span><br><span class="line">        x = self.flatten(x)</span><br><span class="line">        logits = self.linear_relu_stack(x)</span><br><span class="line">        <span class="keyword">return</span> logits</span><br><span class="line">model = NeuralNetwork().to(device) <span class="comment"># 将网络移入device，并打印结构</span></span><br><span class="line"><span class="built_in">print</span>(model)</span><br></pre></td></tr></table></figure><pre><code>NeuralNetwork(  (flatten): Flatten(start_dim=1, end_dim=-1)  (linear_relu_stack): Sequential(    (0): Linear(in_features=784, out_features=512, bias=True)    (1): ReLU()    (2): Linear(in_features=512, out_features=512, bias=True)    (3): ReLU()    (4): Linear(in_features=512, out_features=10, bias=True)  ))</code></pre><h2 id="6-使用我们的网络">6 使用我们的网络</h2><p>主要步骤如下：</p><ul><li>定义模型</li><li>数据载入</li><li>损失函数和优化</li><li>训练和评估</li><li>预测与可视化</li></ul><p>先导入所需包：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure><h3 id="6-1-定义模型">6.1 定义模型</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义设备，如果有GPU则使用GPU，否则使用CPU</span></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义神经网络模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">NeuralNetwork</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.flatten = nn.Flatten()</span><br><span class="line">        self.linear_relu_stack = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">28</span>*<span class="number">28</span>, <span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">10</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.flatten(x)</span><br><span class="line">        logits = self.linear_relu_stack(x)</span><br><span class="line">        <span class="keyword">return</span> logits</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建神经网络模型实例，并移动到设备上</span></span><br><span class="line">model = NeuralNetwork().to(device)</span><br><span class="line"><span class="built_in">print</span>(model)</span><br></pre></td></tr></table></figure><pre><code>NeuralNetwork(  (flatten): Flatten(start_dim=1, end_dim=-1)  (linear_relu_stack): Sequential(    (0): Linear(in_features=784, out_features=512, bias=True)    (1): ReLU()    (2): Linear(in_features=512, out_features=512, bias=True)    (3): ReLU()    (4): Linear(in_features=512, out_features=10, bias=True)  ))</code></pre><h3 id="6-2-数据载入">6.2 数据载入</h3><p>我们这次用的模型用于简单图像分类问题，所以可以使用<a href="https://paperswithcode.com/dataset/mnist">MNIST数据集</a>，导入用的是PyTorch的<a href="https://pytorch.org/vision/stable/datasets.html">datasets</a>。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 加载MNIST数据集并进行预处理</span></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    <span class="comment"># 对图片的常用操作，将图像数据转换为形状为 (C, H, W) 的张量</span></span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    <span class="comment"># 因为数据集是灰度图像，所以只有单值标准化</span></span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>,), (<span class="number">0.5</span>,))</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载MNIST数据集，并划分训练集和测试集（这里会下载下来）</span></span><br><span class="line">train_dataset = datasets.MNIST(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, transform=transform, download=<span class="literal">True</span>)</span><br><span class="line">test_dataset = datasets.MNIST(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">False</span>, transform=transform)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义批量大小和数据加载器</span></span><br><span class="line">batch_size = <span class="number">64</span></span><br><span class="line">train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line">test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><h3 id="6-3-损失函数和优化">6.3 损失函数和优化</h3><p>损失函数选取交叉熵损失函数（Cross Entropy Loss），它是一种常用的损失函数，能够有效地衡量预测类别和真实类别之间的差异。它能够处理模型输出的logits，并且在计算过程中会自动应用Softmax操作，从而简化代码。</p><p>优化器选取随机梯度下降法（Stochastic Gradient Descent, SGD），它是一种简单而有效的优化方法，特别适用于大规模数据集和模型。结合Momentum算法，SGD优化器可以加速收敛并减小震荡，从而在一定程度上提高训练效率和模型性能。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义损失函数</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line"><span class="comment"># 定义优化器</span></span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.01</span>, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure><h3 id="6-4-训练和评估">6.4 训练和评估</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line"><span class="comment"># 训练的目的是通过多个训练周期和批次的数据，不断调整模型参数以最小化损失函数，从而提高模型的性能。</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">model, train_loader, optimizer, criterion, epochs=<span class="number">5</span></span>):</span><br><span class="line">    <span class="comment"># 某些层（如Dropout和BatchNorm）在训练和评估模式下的行为不同，所以需要显式地设置模型为训练模式。</span></span><br><span class="line">    model.train()</span><br><span class="line">    <span class="comment"># 开始训练循环</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">        <span class="comment"># 初始化损失</span></span><br><span class="line">        running_loss = <span class="number">0.0</span></span><br><span class="line">        <span class="comment"># 遍历训练数据加载器中的每个批次</span></span><br><span class="line">        <span class="keyword">for</span> batch_idx, (data, target) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">            <span class="comment"># 将数据移动到设备上</span></span><br><span class="line">            data, target = data.to(device), target.to(device)</span><br><span class="line">            <span class="comment"># 梯度清零</span></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            <span class="comment"># 前向传播，将数据输入模型进行预测</span></span><br><span class="line">            output = model(data)</span><br><span class="line">            <span class="comment"># 计算损失</span></span><br><span class="line">            loss = criterion(output, target)</span><br><span class="line">            <span class="comment"># 将损失反向传播</span></span><br><span class="line">            loss.backward()</span><br><span class="line">            <span class="comment"># 使用优化器更新参数</span></span><br><span class="line">            optimizer.step() </span><br><span class="line">            <span class="comment"># 累计损失</span></span><br><span class="line">            running_loss += loss.item()</span><br><span class="line">            <span class="keyword">if</span> batch_idx % <span class="number">100</span> == <span class="number">99</span>:    <span class="comment"># 每100个批次打印一次训练状态</span></span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&#x27;Epoch [<span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>/<span class="subst">&#123;epochs&#125;</span>], Step [<span class="subst">&#123;batch_idx+<span class="number">1</span>&#125;</span>/<span class="subst">&#123;<span class="built_in">len</span>(train_loader)&#125;</span>], Loss: <span class="subst">&#123;running_loss/<span class="number">100</span>:<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br><span class="line">                running_loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试模型</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test</span>(<span class="params">model, test_loader</span>):</span><br><span class="line">    <span class="comment"># 进入评估模式，原因与train同理</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="comment"># 累计计数</span></span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    total = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 测试过程不会进行优化，所以no_grad禁用梯度计算可以加快测试速度</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> data, target <span class="keyword">in</span> test_loader:</span><br><span class="line">            data, target = data.to(device), target.to(device)</span><br><span class="line">            <span class="comment"># 前向传播，将数据输入模型进行预测</span></span><br><span class="line">            outputs = model(data)</span><br><span class="line">            <span class="comment"># 获取预测结果</span></span><br><span class="line">            _, predicted = torch.<span class="built_in">max</span>(outputs.data, <span class="number">1</span>)</span><br><span class="line">            total += target.size(<span class="number">0</span>)</span><br><span class="line">            correct += (predicted == target).<span class="built_in">sum</span>().item()</span><br><span class="line">    accuracy = <span class="number">100</span> * correct / total</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Test Accuracy: <span class="subst">&#123;accuracy:<span class="number">.2</span>f&#125;</span>%&#x27;</span>)</span><br></pre></td></tr></table></figure><h3 id="6-5-预测与可视化">6.5 预测与可视化</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 可视化预测结果</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">visualize_predictions</span>(<span class="params">model, test_loader, num_images=<span class="number">10</span></span>):</span><br><span class="line">    <span class="comment"># 这里和上面定义的test相似，主要是在执行过程中添加了可视化代码和限制了测试数量</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    images_so_far = <span class="number">0</span></span><br><span class="line">    plt.figure(figsize=(<span class="number">12</span>, <span class="number">8</span>))</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> i, (inputs, labels) <span class="keyword">in</span> <span class="built_in">enumerate</span>(test_loader):</span><br><span class="line">            inputs = inputs.to(device)</span><br><span class="line">            labels = labels.to(device)</span><br><span class="line">            <span class="comment"># 获取每张图的预测结果，并将数据绘制出来进行比对</span></span><br><span class="line">            outputs = model(inputs)</span><br><span class="line">            _, preds = torch.<span class="built_in">max</span>(outputs, <span class="number">1</span>)</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(inputs.size(<span class="number">0</span>)):</span><br><span class="line">                images_so_far += <span class="number">1</span></span><br><span class="line">                ax = plt.subplot(num_images // <span class="number">5</span>, <span class="number">5</span>, images_so_far)</span><br><span class="line">                ax.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">                ax.set_title(<span class="string">f&#x27;Predicted: <span class="subst">&#123;preds[j]&#125;</span> (Label: <span class="subst">&#123;labels[j]&#125;</span>)&#x27;</span>)</span><br><span class="line">                <span class="comment"># imshow用于在绘图窗口中显示图像</span></span><br><span class="line">                ax.imshow(inputs.cpu().data[j].numpy().squeeze(), cmap=<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line">                <span class="keyword">if</span> images_so_far == num_images:</span><br><span class="line">                    model.train()</span><br><span class="line">                    <span class="keyword">return</span></span><br><span class="line">        model.train()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行训练</span></span><br><span class="line">train(model, train_loader, optimizer, criterion)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行测试</span></span><br><span class="line">test(model, test_loader)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化预测结果</span></span><br><span class="line">visualize_predictions(model, test_loader, num_images=<span class="number">10</span>)</span><br><span class="line">plt.suptitle(<span class="string">&#x27;Model Predictions&#x27;</span>)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>NeuralNetwork(  (flatten): Flatten(start_dim=1, end_dim=-1)  (linear_relu_stack): Sequential(    (0): Linear(in_features=784, out_features=512, bias=True)    (1): ReLU()    (2): Linear(in_features=512, out_features=512, bias=True)    (3): ReLU()    (4): Linear(in_features=512, out_features=10, bias=True)  ))Epoch [1/5], Step [100/938], Loss: 1.1812Epoch [1/5], Step [200/938], Loss: 0.4243Epoch [1/5], Step [300/938], Loss: 0.3437Epoch [1/5], Step [400/938], Loss: 0.3305Epoch [1/5], Step [500/938], Loss: 0.2820Epoch [1/5], Step [600/938], Loss: 0.2634Epoch [1/5], Step [700/938], Loss: 0.2482Epoch [1/5], Step [800/938], Loss: 0.2131Epoch [1/5], Step [900/938], Loss: 0.2161Epoch [2/5], Step [100/938], Loss: 0.1853Epoch [2/5], Step [200/938], Loss: 0.1658Epoch [2/5], Step [300/938], Loss: 0.1766Epoch [2/5], Step [400/938], Loss: 0.1507Epoch [2/5], Step [500/938], Loss: 0.1606Epoch [2/5], Step [600/938], Loss: 0.1347Epoch [2/5], Step [700/938], Loss: 0.1407Epoch [2/5], Step [800/938], Loss: 0.1371Epoch [2/5], Step [900/938], Loss: 0.1283Epoch [3/5], Step [100/938], Loss: 0.1027Epoch [3/5], Step [200/938], Loss: 0.1169Epoch [3/5], Step [300/938], Loss: 0.1150Epoch [3/5], Step [400/938], Loss: 0.1077Epoch [3/5], Step [500/938], Loss: 0.0986Epoch [3/5], Step [600/938], Loss: 0.1139Epoch [3/5], Step [700/938], Loss: 0.1110Epoch [3/5], Step [800/938], Loss: 0.0986Epoch [3/5], Step [900/938], Loss: 0.0927Epoch [4/5], Step [100/938], Loss: 0.0908Epoch [4/5], Step [200/938], Loss: 0.0834Epoch [4/5], Step [300/938], Loss: 0.0957Epoch [4/5], Step [400/938], Loss: 0.0742Epoch [4/5], Step [500/938], Loss: 0.0873Epoch [4/5], Step [600/938], Loss: 0.0786Epoch [4/5], Step [700/938], Loss: 0.0901Epoch [4/5], Step [800/938], Loss: 0.0828Epoch [4/5], Step [900/938], Loss: 0.0810Epoch [5/5], Step [100/938], Loss: 0.0682Epoch [5/5], Step [200/938], Loss: 0.0729Epoch [5/5], Step [300/938], Loss: 0.0601Epoch [5/5], Step [400/938], Loss: 0.0684Epoch [5/5], Step [500/938], Loss: 0.0755Epoch [5/5], Step [600/938], Loss: 0.0706Epoch [5/5], Step [700/938], Loss: 0.0733Epoch [5/5], Step [800/938], Loss: 0.0579Epoch [5/5], Step [900/938], Loss: 0.0621Test Accuracy: 97.45%</code></pre><p><img src="/images/DL-pics/create_neural_network_files/create_neural_network_45_1.png#pic_center" alt="png"></p><p>神经网络（尤其是深度神经网络）的一个非常吸引人的特点就是：它们具有很强的通用性，可以通过不同的数据集进行训练，以解决各种不同的任务。我们可以将该模型使用另外的数据集进行训练和测试，仍然有不低的准确率。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义设备，如果有GPU则使用GPU，否则使用CPU</span></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义神经网络模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">NeuralNetwork</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.flatten = nn.Flatten()</span><br><span class="line">        self.linear_relu_stack = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">28</span>*<span class="number">28</span>, <span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">10</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.flatten(x)</span><br><span class="line">        logits = self.linear_relu_stack(x)</span><br><span class="line">        <span class="keyword">return</span> logits</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建神经网络模型实例，并移动到设备上</span></span><br><span class="line">model = NeuralNetwork().to(device)</span><br><span class="line"><span class="built_in">print</span>(model)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载FashionMNIST数据集并进行预处理</span></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>,), (<span class="number">0.5</span>,))</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">train_dataset = datasets.FashionMNIST(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, transform=transform, download=<span class="literal">True</span>)</span><br><span class="line">test_dataset = datasets.FashionMNIST(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">False</span>, transform=transform)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义批量大小和数据加载器</span></span><br><span class="line">batch_size = <span class="number">64</span></span><br><span class="line">train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line">test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数和优化器</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.01</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">model, train_loader, optimizer, criterion, epochs=<span class="number">5</span></span>):</span><br><span class="line">    model.train()</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">        running_loss = <span class="number">0.0</span></span><br><span class="line">        <span class="keyword">for</span> batch_idx, (data, target) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">            data, target = data.to(device), target.to(device)</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            output = model(data)</span><br><span class="line">            loss = criterion(output, target)</span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            running_loss += loss.item()</span><br><span class="line">            <span class="keyword">if</span> batch_idx % <span class="number">100</span> == <span class="number">99</span>:    <span class="comment"># 每100个批次打印一次训练状态</span></span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&#x27;Epoch [<span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>/<span class="subst">&#123;epochs&#125;</span>], Step [<span class="subst">&#123;batch_idx+<span class="number">1</span>&#125;</span>/<span class="subst">&#123;<span class="built_in">len</span>(train_loader)&#125;</span>], Loss: <span class="subst">&#123;running_loss/<span class="number">100</span>:<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br><span class="line">                running_loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试模型</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test</span>(<span class="params">model, test_loader</span>):</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    total = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> data, target <span class="keyword">in</span> test_loader:</span><br><span class="line">            data, target = data.to(device), target.to(device)</span><br><span class="line">            outputs = model(data)</span><br><span class="line">            _, predicted = torch.<span class="built_in">max</span>(outputs.data, <span class="number">1</span>)</span><br><span class="line">            total += target.size(<span class="number">0</span>)</span><br><span class="line">            correct += (predicted == target).<span class="built_in">sum</span>().item()</span><br><span class="line">    accuracy = <span class="number">100</span> * correct / total</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Test Accuracy: <span class="subst">&#123;accuracy:<span class="number">.2</span>f&#125;</span>%&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化预测结果</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">visualize_predictions</span>(<span class="params">model, test_loader, num_images=<span class="number">10</span></span>):</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    images_so_far = <span class="number">0</span></span><br><span class="line">    plt.figure(figsize=(<span class="number">12</span>, <span class="number">8</span>))</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> i, (inputs, labels) <span class="keyword">in</span> <span class="built_in">enumerate</span>(test_loader):</span><br><span class="line">            inputs = inputs.to(device)</span><br><span class="line">            labels = labels.to(device)</span><br><span class="line">            outputs = model(inputs)</span><br><span class="line">            _, preds = torch.<span class="built_in">max</span>(outputs, <span class="number">1</span>)</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(inputs.size(<span class="number">0</span>)):</span><br><span class="line">                images_so_far += <span class="number">1</span></span><br><span class="line">                ax = plt.subplot(num_images // <span class="number">5</span>, <span class="number">5</span>, images_so_far)</span><br><span class="line">                ax.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">                ax.set_title(<span class="string">f&#x27;Predicted: <span class="subst">&#123;preds[j]&#125;</span> (Label: <span class="subst">&#123;labels[j]&#125;</span>)&#x27;</span>)</span><br><span class="line">                ax.imshow(inputs.cpu().data[j].numpy().squeeze(), cmap=<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line">                <span class="keyword">if</span> images_so_far == num_images:</span><br><span class="line">                    model.train()</span><br><span class="line">                    <span class="keyword">return</span></span><br><span class="line">        model.train()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行训练</span></span><br><span class="line">train(model, train_loader, optimizer, criterion)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行测试</span></span><br><span class="line">test(model, test_loader)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化预测结果</span></span><br><span class="line">visualize_predictions(model, test_loader, num_images=<span class="number">10</span>)</span><br><span class="line">plt.suptitle(<span class="string">&#x27;Model Predictions&#x27;</span>)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>NeuralNetwork(  (flatten): Flatten(start_dim=1, end_dim=-1)  (linear_relu_stack): Sequential(    (0): Linear(in_features=784, out_features=512, bias=True)    (1): ReLU()    (2): Linear(in_features=512, out_features=512, bias=True)    (3): ReLU()    (4): Linear(in_features=512, out_features=10, bias=True)  ))Epoch [1/5], Step [100/938], Loss: 1.1111Epoch [1/5], Step [200/938], Loss: 0.6047Epoch [1/5], Step [300/938], Loss: 0.5097Epoch [1/5], Step [400/938], Loss: 0.4919Epoch [1/5], Step [500/938], Loss: 0.4808Epoch [1/5], Step [600/938], Loss: 0.4519Epoch [1/5], Step [700/938], Loss: 0.4558Epoch [1/5], Step [800/938], Loss: 0.4473Epoch [1/5], Step [900/938], Loss: 0.4138Epoch [2/5], Step [100/938], Loss: 0.3960Epoch [2/5], Step [200/938], Loss: 0.3889Epoch [2/5], Step [300/938], Loss: 0.4075Epoch [2/5], Step [400/938], Loss: 0.3719Epoch [2/5], Step [500/938], Loss: 0.3819Epoch [2/5], Step [600/938], Loss: 0.3858Epoch [2/5], Step [700/938], Loss: 0.3838Epoch [2/5], Step [800/938], Loss: 0.3564Epoch [2/5], Step [900/938], Loss: 0.3616Epoch [3/5], Step [100/938], Loss: 0.3488Epoch [3/5], Step [200/938], Loss: 0.3507Epoch [3/5], Step [300/938], Loss: 0.3522Epoch [3/5], Step [400/938], Loss: 0.3363Epoch [3/5], Step [500/938], Loss: 0.3375Epoch [3/5], Step [600/938], Loss: 0.3445Epoch [3/5], Step [700/938], Loss: 0.3378Epoch [3/5], Step [800/938], Loss: 0.3208Epoch [3/5], Step [900/938], Loss: 0.3163Epoch [4/5], Step [100/938], Loss: 0.3189Epoch [4/5], Step [200/938], Loss: 0.3005Epoch [4/5], Step [300/938], Loss: 0.3071Epoch [4/5], Step [400/938], Loss: 0.3240Epoch [4/5], Step [500/938], Loss: 0.3147Epoch [4/5], Step [600/938], Loss: 0.2946Epoch [4/5], Step [700/938], Loss: 0.3150Epoch [4/5], Step [800/938], Loss: 0.3024Epoch [4/5], Step [900/938], Loss: 0.3152Epoch [5/5], Step [100/938], Loss: 0.2723Epoch [5/5], Step [200/938], Loss: 0.2969Epoch [5/5], Step [300/938], Loss: 0.2963Epoch [5/5], Step [400/938], Loss: 0.2835Epoch [5/5], Step [500/938], Loss: 0.2910Epoch [5/5], Step [600/938], Loss: 0.2990Epoch [5/5], Step [700/938], Loss: 0.2990Epoch [5/5], Step [800/938], Loss: 0.3039Epoch [5/5], Step [900/938], Loss: 0.3005Test Accuracy: 87.60%</code></pre><p><img src="/images/DL-pics/create_neural_network_files/create_neural_network_47_1.png" alt="png"></p><h2 id="7-优化与调参">7 优化与调参</h2><p>显然，同样的模型对于不同的数据集的适配程度是不一样的。对于MNIST数据集准确率可以达到97%，但对于FashionMNIST只能达到86%。所以我们可以来探索一下为什么会有这样的偏差，以及如何优化该模型才能让FashionMNIST也可以达到90%以上的准确率。</p><h3 id="7-1-可能的问题">7.1 可能的问题</h3><ol><li>数据集的复杂性</li></ol><ul><li>MNIST 数据集：包含手写数字的灰度图像（0-9），这些图像相对简单，特征明显，模式较少。</li><li>FashionMNIST 数据集：包含服装物品的灰度图像（例如 T 恤、裤子、鞋子等），这些图像的特征更加复杂，类别之间的差异较小。</li></ul><ol start="2"><li>模型的复杂性</li></ol><ul><li>我们使用的是一个简单的全连接神经网络，它可能足以在 MNIST 数据集上达到高准确率，但在处理更复杂的 FashionMNIST 数据集时会表现不佳。</li></ul><ol start="3"><li>超参数调整：</li></ol><ul><li>我们的模型可能需要在不同的数据集上进行不同的超参数调整。例如，学习率、批量大小、正则化参数等可能需要重新调整以适应 FashionMNIST 的复杂性。</li></ul><ol start="4"><li>数据预处理：</li></ol><ul><li>数据的预处理步骤（如标准化、归一化、数据增强等）对不同的数据集可能有不同的效果。我们可能需要针对 FashionMNIST 数据集尝试不同的预处理方法。</li></ul><h3 id="7-2-解决方案">7.2 解决方案</h3><h4 id="7-2-1-增加神经网络层数和神经元容量">7.2.1 增加神经网络层数和神经元容量</h4><p>通过增加模型的容量，模型能够学习更多的特征。如下，我们添加两个线性层进一步提高模型对复杂特征的处理能力。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">NeuralNetwork_v1</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.flatten = nn.Flatten()</span><br><span class="line">        self.linear_relu_stack = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">28</span>*<span class="number">28</span>, <span class="number">1024</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">1024</span>, <span class="number">1024</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">1024</span>, <span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.flatten(x)</span><br><span class="line">        logits = self.linear_relu_stack(x)</span><br><span class="line">        <span class="keyword">return</span> logits</span><br></pre></td></tr></table></figure><p>可能因为线性层在处理FashionMNIST数据集时，难以处理和学习更多的特征，在添加了线性层后预测准确率没有明显的提高，仍然是87%左右。所以需要尝试其他的方法。</p><h4 id="7-2-2-使用卷积神经网络-cnn">7.2.2  使用卷积神经网络（CNN）</h4><p>FashionMNIST 数据集涉及到服装和配件的图像分类，每个图像都是单通道的灰度图像，分辨率为 28x28 像素。尽管 FashionMNIST 数据集相对于真实世界的图像数据集如 CIFAR-10 或 ImageNet 来说较为简单，但仍然涉及到一定程度的空间特征。且如纹理图案、形状轮廓等复杂特征，线性层更加难以处理和识别。所以在局部相关性和空间结构处理占优的情况下，使用卷积神经网络（CNN）来处理是更优的选择。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ConvNeuralNetwork</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(ConvNeuralNetwork, self).__init__()</span><br><span class="line">        <span class="comment"># 卷积层定义</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">32</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">32</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.conv3 = nn.Conv2d(<span class="number">64</span>, <span class="number">128</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 扁平化层</span></span><br><span class="line">        self.flatten = nn.Flatten()</span><br><span class="line">        <span class="comment"># 全连接层</span></span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">128</span>*<span class="number">3</span>*<span class="number">3</span>, <span class="number">512</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">512</span>, <span class="number">512</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">512</span>, <span class="number">10</span>)</span><br><span class="line">        <span class="comment"># Dropout 正则化层，用于随机丢弃一定比例的神经元，防止过拟合</span></span><br><span class="line">        self.dropout = nn.Dropout(<span class="number">0.4</span>)</span><br><span class="line">        <span class="comment"># 批量归一化层，对每个卷积层的输出进行归一化，有助于加速收敛和提高模型泛化能力</span></span><br><span class="line">        self.batch_norm1 = nn.BatchNorm2d(<span class="number">32</span>)</span><br><span class="line">        self.batch_norm2 = nn.BatchNorm2d(<span class="number">64</span>)</span><br><span class="line">        self.batch_norm3 = nn.BatchNorm2d(<span class="number">128</span>)</span><br><span class="line">        <span class="comment"># 批量归一化层，对全连接层的输出进行归一化</span></span><br><span class="line">        self.batch_norm_fc1 = nn.BatchNorm1d(<span class="number">512</span>)</span><br><span class="line">        self.batch_norm_fc2 = nn.BatchNorm1d(<span class="number">512</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 卷积层和激活函数</span></span><br><span class="line">        <span class="comment"># 依次进行卷积、ReLU 激活函数、批量归一化和最大池化操作</span></span><br><span class="line"></span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = F.relu(self.batch_norm1(x))</span><br><span class="line">        x = F.max_pool2d(x, <span class="number">2</span>)</span><br><span class="line">        </span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        x = F.relu(self.batch_norm2(x))</span><br><span class="line">        x = F.max_pool2d(x, <span class="number">2</span>)</span><br><span class="line">        </span><br><span class="line">        x = self.conv3(x)</span><br><span class="line">        x = F.relu(self.batch_norm3(x))</span><br><span class="line">        x = F.max_pool2d(x, <span class="number">2</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 进行全连接和正则化</span></span><br><span class="line">        x = self.flatten(x)</span><br><span class="line">        x = self.fc1(x)</span><br><span class="line">        x = F.relu(self.batch_norm_fc1(x))</span><br><span class="line">        x = self.dropout(x)</span><br><span class="line">        </span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        x = F.relu(self.batch_norm_fc2(x))</span><br><span class="line">        x = self.dropout(x)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 输出层</span></span><br><span class="line">        logits = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> logits</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建CNN实例，并移动到设备上</span></span><br><span class="line">model_v1 = ConvNeuralNetwork().to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数和优化器</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(model_v1.parameters(), lr=<span class="number">0.01</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># train_loader、test_loader 在之前已初始化</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_v1</span>(<span class="params">model, train_loader, optimizer, criterion, epochs=<span class="number">5</span></span>):</span><br><span class="line">    model.train()</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">        running_loss = <span class="number">0.0</span></span><br><span class="line">        <span class="keyword">for</span> batch_idx, (data, target) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">            data, target = data.to(device), target.to(device)</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            output = model(data)</span><br><span class="line">            loss = criterion(output, target)</span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            running_loss += loss.item()</span><br><span class="line">            <span class="keyword">if</span> batch_idx % <span class="number">100</span> == <span class="number">99</span>:    <span class="comment"># 每100个批次打印一次训练状态</span></span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&#x27;Epoch [<span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>/<span class="subst">&#123;epochs&#125;</span>], Step [<span class="subst">&#123;batch_idx+<span class="number">1</span>&#125;</span>/<span class="subst">&#123;<span class="built_in">len</span>(train_loader)&#125;</span>], Loss: <span class="subst">&#123;running_loss/<span class="number">100</span>:<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br><span class="line">                running_loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取类别名称</span></span><br><span class="line">class_names_v1 = train_dataset.classes</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置中文字体</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;font.sans-serif&#x27;</span>] = [<span class="string">&#x27;SimHei&#x27;</span>]  <span class="comment"># 使用SimHei字体</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;axes.unicode_minus&#x27;</span>] = <span class="literal">False</span>  <span class="comment"># 解决负号&#x27;-&#x27;显示为方块的问题</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">visualize_predictions_v1</span>(<span class="params">model, dataloader, class_names</span>):</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    num_classes = <span class="built_in">len</span>(class_names)</span><br><span class="line">    confusion_matrix = np.zeros((num_classes, num_classes), dtype=np.int32)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> inputs, labels <span class="keyword">in</span> dataloader:</span><br><span class="line">            inputs, labels = inputs.to(device), labels.to(device)  <span class="comment"># 将数据移动到设备上</span></span><br><span class="line">            outputs = model(inputs)</span><br><span class="line">            _, preds = torch.<span class="built_in">max</span>(outputs, <span class="number">1</span>)</span><br><span class="line">            <span class="keyword">for</span> t, p <span class="keyword">in</span> <span class="built_in">zip</span>(labels.view(-<span class="number">1</span>), preds.view(-<span class="number">1</span>)):</span><br><span class="line">                confusion_matrix[t.long(), p.long()] += <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 归一化混淆矩阵</span></span><br><span class="line">    confusion_matrix = confusion_matrix.astype(<span class="string">&#x27;float&#x27;</span>) / confusion_matrix.<span class="built_in">sum</span>(axis=<span class="number">1</span>)[:, np.newaxis]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 绘图</span></span><br><span class="line">    fig, ax = plt.subplots(figsize=(<span class="number">10</span>, <span class="number">8</span>))</span><br><span class="line">    im = ax.imshow(confusion_matrix, interpolation=<span class="string">&#x27;nearest&#x27;</span>, cmap=plt.cm.Blues)</span><br><span class="line">    ax.figure.colorbar(im, ax=ax)</span><br><span class="line">    ax.<span class="built_in">set</span>(xticks=np.arange(confusion_matrix.shape[<span class="number">1</span>]),</span><br><span class="line">           yticks=np.arange(confusion_matrix.shape[<span class="number">0</span>]),</span><br><span class="line">           xticklabels=class_names, yticklabels=class_names,</span><br><span class="line">           title=<span class="string">&#x27;归一化混淆矩阵&#x27;</span>,</span><br><span class="line">           ylabel=<span class="string">&#x27;真实标签&#x27;</span>,</span><br><span class="line">           xlabel=<span class="string">&#x27;预测标签&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 旋转标签并设置对齐</span></span><br><span class="line">    plt.setp(ax.get_xticklabels(), rotation=<span class="number">45</span>, ha=<span class="string">&quot;right&quot;</span>,</span><br><span class="line">             rotation_mode=<span class="string">&quot;anchor&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 遍历数据维度并创建文本注释</span></span><br><span class="line">    fmt = <span class="string">&#x27;.2f&#x27;</span></span><br><span class="line">    thresh = confusion_matrix.<span class="built_in">max</span>() / <span class="number">2.</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(confusion_matrix.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(confusion_matrix.shape[<span class="number">1</span>]):</span><br><span class="line">            ax.text(j, i, <span class="built_in">format</span>(confusion_matrix[i, j], fmt),</span><br><span class="line">                    ha=<span class="string">&quot;center&quot;</span>, va=<span class="string">&quot;center&quot;</span>,</span><br><span class="line">                    color=<span class="string">&quot;white&quot;</span> <span class="keyword">if</span> confusion_matrix[i, j] &gt; thresh <span class="keyword">else</span> <span class="string">&quot;black&quot;</span>)</span><br><span class="line">    fig.tight_layout()</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行训练</span></span><br><span class="line">train_v1(model_v1, train_loader, optimizer, criterion)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行测试</span></span><br><span class="line">test(model_v1, test_loader)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化预测结果</span></span><br><span class="line">visualize_predictions_v1(model_v1, test_loader, class_names_v1)</span><br><span class="line">plt.suptitle(<span class="string">&#x27;Model Predictions_v1&#x27;</span>)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>Epoch [1/5], Step [100/938], Loss: 0.7875Epoch [1/5], Step [200/938], Loss: 0.4787Epoch [1/5], Step [300/938], Loss: 0.4455Epoch [1/5], Step [400/938], Loss: 0.3960Epoch [1/5], Step [500/938], Loss: 0.3657Epoch [1/5], Step [600/938], Loss: 0.3557Epoch [1/5], Step [700/938], Loss: 0.3363Epoch [1/5], Step [800/938], Loss: 0.3495Epoch [1/5], Step [900/938], Loss: 0.3140Epoch [2/5], Step [100/938], Loss: 0.2842Epoch [2/5], Step [200/938], Loss: 0.3065Epoch [2/5], Step [300/938], Loss: 0.2671Epoch [2/5], Step [400/938], Loss: 0.2750Epoch [2/5], Step [500/938], Loss: 0.2874Epoch [2/5], Step [600/938], Loss: 0.2722Epoch [2/5], Step [700/938], Loss: 0.2639Epoch [2/5], Step [800/938], Loss: 0.2840Epoch [2/5], Step [900/938], Loss: 0.2630Epoch [3/5], Step [100/938], Loss: 0.2359Epoch [3/5], Step [200/938], Loss: 0.2461Epoch [3/5], Step [300/938], Loss: 0.2350Epoch [3/5], Step [400/938], Loss: 0.2337Epoch [3/5], Step [500/938], Loss: 0.2453Epoch [3/5], Step [600/938], Loss: 0.2247Epoch [3/5], Step [700/938], Loss: 0.2354Epoch [3/5], Step [800/938], Loss: 0.2351Epoch [3/5], Step [900/938], Loss: 0.2333Epoch [4/5], Step [100/938], Loss: 0.2045Epoch [4/5], Step [200/938], Loss: 0.2206Epoch [4/5], Step [300/938], Loss: 0.2161Epoch [4/5], Step [400/938], Loss: 0.2125Epoch [4/5], Step [500/938], Loss: 0.2003Epoch [4/5], Step [600/938], Loss: 0.2060Epoch [4/5], Step [700/938], Loss: 0.1919Epoch [4/5], Step [800/938], Loss: 0.2012Epoch [4/5], Step [900/938], Loss: 0.2138Epoch [5/5], Step [100/938], Loss: 0.1789Epoch [5/5], Step [200/938], Loss: 0.1724Epoch [5/5], Step [300/938], Loss: 0.1737Epoch [5/5], Step [400/938], Loss: 0.1883Epoch [5/5], Step [500/938], Loss: 0.1921Epoch [5/5], Step [600/938], Loss: 0.1982Epoch [5/5], Step [700/938], Loss: 0.2055Epoch [5/5], Step [800/938], Loss: 0.1865Epoch [5/5], Step [900/938], Loss: 0.1930Test Accuracy: 91.53%</code></pre><p><img src="/images/DL-pics/create_neural_network_files/create_neural_network_51_1.png" alt="png"></p><pre><code>&lt;Figure size 640x480 with 0 Axes&gt;</code></pre><h4 id="7-2-3-调整超参数">7.2.3 调整超参数</h4><p>超参数是模型训练过程中需要预先设定的参数，学习率、批次大小和迭代次数等都称之为超参数。通过调整这些超参数，我们可以提高模型的性能和准确性。</p><ul><li>学习率是最重要的超参数之一。我们可以尝试不同的学习率，观察其对模型性能的影响：</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">learning_rates = [<span class="number">0.1</span>, <span class="number">0.01</span>, <span class="number">0.001</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> lr <span class="keyword">in</span> learning_rates:</span><br><span class="line">    optimizer = optim.SGD(model.parameters(), lr=lr)</span><br><span class="line">    <span class="comment"># 训练模型并记录性能</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><ul><li>当然我们还可以通过学习率调度器在训练过程中动态调整学习率</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=<span class="number">10</span>, gamma=<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="comment"># 训练模型</span></span><br><span class="line">    scheduler.step()</span><br><span class="line"></span><br></pre></td></tr></table></figure><ul><li>批量大小会影响训练的稳定性和速度：</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">batch_sizes = [<span class="number">32</span>, <span class="number">64</span>, <span class="number">128</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> batch_size <span class="keyword">in</span> batch_sizes:</span><br><span class="line">    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># 训练模型并记录性能</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><ul><li>不同的优化器可能会对模型的收敛速度和最终性能产生影响。我们可以尝试不同的优化器，如SGD和Adam：</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">optimizers = &#123;</span><br><span class="line">    <span class="string">&#x27;SGD&#x27;</span>: optim.SGD(model.parameters(), lr=<span class="number">0.01</span>),</span><br><span class="line">    <span class="string">&#x27;Adam&#x27;</span>: optim.Adam(model.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="参考">参考</h1><ul><li><a href="https://pytorch.ac.cn/tutorials/beginner/basics/buildmodel_tutorial.html">PyTorch-构建神经网络</a></li><li><a href="https://pytorch-cn.readthedocs.io/zh/latest/package_references/torch-nn/">PyTorch中文文档</a></li><li><a href="https://zh.d2l.ai/chapter_multilayer-perceptrons/mlp.html">d2L-多层感知机</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
          <category> 笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DL-notes </tag>
            
            <tag> DL </tag>
            
            <tag> notes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DL-notes:目录</title>
      <link href="/2024/05/07/DL-notes/DL-notes/"/>
      <url>/2024/05/07/DL-notes/DL-notes/</url>
      
        <content type="html"><![CDATA[<h2 id="目录">目录</h2><h3 id="swig-0"><a href="/2024/09/28/DL-notes/pages/HwAlg/" title="常见算法">1 常见算法</a></h3><h2 id="参考">参考</h2><p><a href="https://datawhalechina.github.io/leedl-tutorial/#/">李宏毅深度学习教程LeeDL-Tutorial</a><br><a href="https://nndl.github.io/">邱锡鹏神经网络与深度学习NNDL</a></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
          <category> 笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DL-notes </tag>
            
            <tag> DL </tag>
            
            <tag> notes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ML-notes:SVM</title>
      <link href="/2024/05/07/ML-notes/pages/svm/"/>
      <url>/2024/05/07/ML-notes/pages/svm/</url>
      
        <content type="html"><![CDATA[<h1 id="6-svm">6 SVM</h1>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> notes </tag>
            
            <tag> ML-notes </tag>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>单细胞数据分析</title>
      <link href="/2024/04/24/single-cell/usingScanpy/"/>
      <url>/2024/04/24/single-cell/usingScanpy/</url>
      
        <content type="html"><![CDATA[<h1 id="单细胞数据分析">单细胞数据分析</h1><p>自2013年被选为年度方法以来，单细胞技术已经足够成熟，可以为复杂的研究问题提供答案。随着单细胞分析技术的发展，从单细胞分析中收集的数据也显著增加，导致处理这些庞大而复杂的数据集的计算挑战。</p><h2 id="单细胞rna测序-数据分析">单细胞RNA测序-数据分析</h2><p>scanpy 是一个用于分析单细胞转录组(single cell rna sequencing) 数据的python库，文章2018发表在<a href="https://genomebiology.biomedcentral.com/"><em>Genome Biology</em></a>。它和seurat几乎大差不差，但是以Python的生态，完全可以认为其具有更大的扩展潜力。</p><h3 id="安装环境-scanpy单细胞测序学习-环境配置">安装环境（<a href="https://yingbio.cn/archives/scseq-scanpy-install">Scanpy单细胞测序学习-环境配置</a>）</h3><h3 id="公共单细胞数据集">公共单细胞数据集</h3><p>10X Genomics免费提供的外周血单核细胞(PBMC)数据集</p><p><a href="https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html">Preprocessing and clustering 3k PBMCs (legacy workflow) — scanpy-tutorials 0.1.dev50+g06018e6 documentation</a></p><p>scanpy提供的公开数据集</p><p><a href="https://scanpy.readthedocs.io/en/stable/api/datasets.html">Datasets — scanpy</a></p><h3 id="开始">开始</h3><h4 id="1-载入包">1. 载入包</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 载入包</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> scanpy <span class="keyword">as</span> sc</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置日志等级: errors (0), warnings (1), info (2), hints (3)</span></span><br><span class="line">sc.settings.verbosity = <span class="number">3</span>             </span><br><span class="line">sc.logging.print_header()</span><br><span class="line">sc.settings.set_figure_params(dpi=<span class="number">80</span>, facecolor=<span class="string">&#x27;white&#x27;</span>)</span><br></pre></td></tr></table></figure><blockquote><p>scanpy==1.10.1 anndata==0.10.7 umap==0.5.5 numpy==1.23.1 scipy==1.13.0 pandas==2.2.2 scikit-learn==1.4.2 statsmodels==0.14.1 igraph==0.11.4 pynndescent==0.5.12</p></blockquote><h4 id="2-载入数据集">2. 载入数据集</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># sc载入数据集</span></span><br><span class="line">adata = sc.datasets.pbmc3k()</span><br><span class="line"><span class="comment"># 本地载入数据集</span></span><br><span class="line">results_file = <span class="string">&#x27;D:\scanpy\write\pbmc3k.h5ad&#x27;</span> </span><br><span class="line"><span class="built_in">help</span>(sc.read_10x_mtx)</span><br><span class="line">adata = sc.read_10x_mtx(</span><br><span class="line">    <span class="string">&#x27;D:/scanpy/data/filtered_feature_bc_matrix&#x27;</span>,  <span class="comment"># the directory with the `.mtx` file</span></span><br><span class="line">    var_names=<span class="string">&#x27;gene_symbols&#x27;</span>,                  <span class="comment"># use gene symbols for the variable names (variables-axis index)</span></span><br><span class="line">    cache=<span class="literal">True</span>) </span><br><span class="line">adata.var_names_make_unique()  <span class="comment"># this is unnecessary if using `var_names=&#x27;gene_ids&#x27;` in `sc.read_10x_mtx`</span></span><br></pre></td></tr></table></figure><h4 id="3-查看数据">3. 查看数据</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">adata</span><br></pre></td></tr></table></figure><img src="D:\code\SapientialM\source\_posts\single-cell\assets\image-20240423155809136.png" alt="image-20240423155809136" style="zoom:67%;" /><p>AnnData object with n_obs × n_vars = 2700 × 32738 意思是这是一个AnnData对象，n_obs 即有2700个细胞样本，n_vars 即有32738个基因序列。</p><h5 id="数据对象-anndata-annotated-data">数据对象 （<a href="https://anndata.readthedocs.io/en/latest/index.html">anndata - Annotated data</a>）</h5><p>Scanpy 构建的对象叫做 AnnData 对象：</p><p>标准的AnnData对象主要包括以下几个部分（如果你要调用这些属性，只需要直接添加到后面即可，比如adata.obs、adata.X等）：</p><ul><li>.X: 存储基因表达矩阵，行代表基因，列代表细胞，也就是<strong>显示的 n_obs x n_vars</strong>。</li><li>.obs: <strong>观测值数据</strong>，存储细胞相关的注释信息，例如细胞类型、样本信息等。</li><li>.var: <strong>特征和高可变数据</strong>，存储基因相关的注释信息，例如基因名称、基因类型等。</li><li>.uns: <strong>非结构化数据</strong>，存储与数据分析相关的信息，例如数据预处理参数、可视化参数等；可以包含一些在分析数据时我们得到的一些有价值的信息。</li><li>.obsm: <strong>细胞的附加数据</strong>，例如细胞的空间位置、转录组拆分信息等；也就是我们进行进一步处理后得到的细胞数据。</li><li>.varm: <strong>基因的附加数据</strong>，例如基因的表达模式、变异信息等；也就是我们处理得到的基因级别的元数据。</li><li>.layers: <strong>各种类型的基因表达矩阵</strong>，例如原始表达矩阵、归一化表达矩阵等；我们可能拥有不同形式的原始核心数据，也许一种是规范化的，或者不是。这些可以存储在 AnnData 的不同layer中。</li><li>varp、obsp: <strong>基因、细胞映射关系的附加数据</strong>，我没有在相关文档找到varp的详细说明，但应该类似于varm、obsm，拥有 n_obs x n_obs 和 n_var x n_var 大小的矩阵，用于数据分析国产中得到的一些映射关系的信息，比如A基因与B基因之间的关系，A细胞与B细胞之间的关系。</li></ul><blockquote><p>p即Pairwise annotation，m即Multi-dimensional annotation，obs即observations，var即variables/ features，uns即Unstructured annotation，具体的数据解释可参考：<a href="https://anndata.readthedocs.io/en/latest/generated/anndata.AnnData.html">anndata.AnnData — anndata 0.1.dev50+g0a768fc documentation</a></p></blockquote><p>整个AnnData对象如下所示结构，具体意思就是，根据var即基因信息作为属性列包含了 var 表、X表、varm表、varp表，同样的obs作为属性列包含了obs表、X表、obsm表、obsp表</p><img src="https://anndata.readthedocs.io/en/latest/_static/anndata_schema.svg" width="50%" height="50%"><h5 id="数据核心">数据核心</h5><p>单细胞转录组的核心就是一个cell X gene的二维表，</p><h4 id="4-查看数据">4. 查看数据</h4><h3 id="参考资料">参考资料</h3><p>推荐一本书：</p><p><a href="https://www.bookstack.cn/read/pyda-2e-zh/README.md">《利用 Python 进行数据分析 · 第 2 版》  · BookStack</a></p><p>参考资料：</p><p><a href="https://www.youtube.com/watch?v=_tP6vCwZfuY">Single Cell data analysis tutorial on PBMC dataset using scanpy - Part1</a></p><p><a href="https://github.com/ramadatta/Youtube/tree/main/scanpy/PBMC_data">Youtube/scanpy/PBMC_data at main · ramadatta/Youtube (github.com)</a></p><p><a href="https://mp.weixin.qq.com/s/c_3NjoJyZkSv1XjIYu-V_g">【基于python的单细胞分析】使用scVI实现批次效应校正</a></p><p><a href="https://mp.weixin.qq.com/s/ekJ0gyMqnchx5_6U4WoPHQ">【基于python的单细胞分析】如何进行细胞类型注释</a></p><p><a href="https://mp.weixin.qq.com/s/u5fkFnTe_eDe1F2RTkdtLQ">基于Scanpy的单细胞数据质控、聚类、标注</a></p><p><a href="https://blog.51cto.com/u_14782715/5082964"><strong>scanpy 单细胞分析包图文详解 01 | 深入理解 AnnData 数据结构</strong></a></p><p><a href="https://www.youtube.com/watch?v=NYpwinpPEb0">【陈巍学基因】单细胞RNA测序分析图解读</a></p><p><a href="https://yingbio.cn/archives/scseq-scanpy-install">Scanpy单细胞测序学习-环境配置</a></p><p><a href="https://mp.weixin.qq.com/s/IlG2R7qXCHpOH94cQHRRdA">基于COSG的单细胞数据marker基因鉴定</a></p><p><a href="https://cloud.tencent.com/developer/article/1610396">scanpy教程：预处理与聚类-腾讯云开发者社区-腾讯云 (tencent.com)</a></p><p><a href="https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html">预处理和聚类 3k PBMC（旧工作流） — scanpy-tutorials 0.1.dev50+g06018e6 文档</a></p>]]></content>
      
      
      <categories>
          
          <category> 数据分析 </category>
          
          <category> 单细胞 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> single-cell </tag>
            
            <tag> 人工智能 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ML-notes:人工神经网络</title>
      <link href="/2024/04/16/ML-notes/pages/neural-network/"/>
      <url>/2024/04/16/ML-notes/pages/neural-network/</url>
      
        <content type="html"><![CDATA[<h1 id="5-人工神经网络">5 人工神经网络</h1><p>本章讨论现阶段比较热门的一个<strong>监督学习算法————人工神经网络（artificial Neural Network）</strong></p><p>神经网络是由具有<strong>适应性的简单单元</strong>组成的广泛<strong>并行互连的网络</strong>，它的组织能够模拟<strong>生物神经系统</strong>对真实世界物体所做的交互反应。</p><h2 id="5-1-a-name-5-1-神经元模型-neuron-a">5.1 <a name='5.1'>神经元模型 Neuron</a></h2><p><strong>神经网络中最基本的成分</strong>便是神经元（Neuron）模型，也就是上面说的<strong>适应性简单单元</strong>。在神经网络中，每个神经元都与其他神经元相连，当它“兴奋”时，都会向相连的神经元发送化学物质，改变相连的神经元内的电位；如果神经元电位超过了一个“阈值”（threshold），那么该神经元就会兴奋，所以整个神经网络就是通过兴奋和电位来传播信息。</p><h3 id="5-1-1-a-name-5-1-1-m-p神经元模型-a">5.1.1 <a name='5.1.1'>M-P神经元模型</a></h3><p>1943年一直沿用至今的 <strong>“M-P神经元模型”</strong> 便是对这个过程的抽象。</p><p>在这个模型中，</p><ul><li>神经元收到了来自其他 n 个神经元传递过来的<strong>输入信号</strong> x<sub>i</sub></li><li>而这些输入信号<strong>通过带有权重的连接（connection）</strong>，这些权重又叫连接权（connection weight）。</li><li>然后来到细胞体的<strong>前部分</strong>，它负责计算总输入值（输入信号的<strong>加权求和</strong>，累积电平）</li><li>然后到达<strong>后部分</strong>，<strong>计算总输入值与神经元阈值的差值</strong>，通过<strong>激活函数（activation function）处理</strong>，传递到下一层神经元。</li></ul><div align=center><img src="/images/ML-pics/5.1.png" /></div><blockquote><ul><li>① 𝒙<sub>𝒊</sub> 来自第𝑖个神经元的输入</li><li>② 𝒘<sub>𝒊</sub> 第𝑖 个神经元的连接权重</li><li>③ 𝜽 阈值(threshold)或称为偏置 （bias）</li><li>④ y是输出，也是激活函数</li></ul></blockquote><p>和之前讲的线性模型的分类十分相似，<strong>神经元模型最理想的激活函数</strong>也是阶跃函数。即将神经元输入值与阈值的差值映射为输出值 1 或 0 ，0 表示抑制神经元而 1 表示激活神经元。但阶跃函数不连续，不光滑，故在M-P神经元模型中，也采用Sigmoid函数来近似， <strong>Sigmoid函数</strong>将较大范围内变化的输入值挤压到 (0,1) 输出值范围内，所以也称为<strong>挤压函数（squashing function）</strong>：</p><div align=center><img src="/images/ML-pics/5.2.png" /></div><p>将多个神经元按一定的层次结构连接起来，就得到了神经网络。它是一种<strong>包含多个参数（输入）的模型</strong>，比方说10个神经元两两连接，则有100个参数需要学习（每个神经元有9个连接权以及1个阈值），若将每个神经元都看作一个函数，则整个神经网络就是由这些<strong>函数相互嵌套</strong>而成。</p><blockquote><p>神经元模型与逻辑回归模型求解的优化问题是一致的，都是<strong>线性二分类问题</strong>。本质上来说，M-P神经元模型就等同于一个线性二分类器。</p><ul><li>Sigmoid函数≠Logistic函数</li><li>Logistic函数⊂ Sigmoid函数</li></ul></blockquote><h2 id="5-2-a-name-5-2-感知机-perceptron-与多层网络-a">5.2 <a name='5.2'>感知机（perceptron）与多层网络</a></h2><h3 id="5-2-1-a-name-5-2-1-感知机-a">5.2.1 <a name='5.2.1'>感知机</a></h3><h4 id="概念">概念</h4><p><strong>感知机（Perceptron）</strong> 是由两层神经元组成的一个简单模型。</p><p>只有输出层是M-P神经元，即<strong>只有输出层神经元进行激活函数处理</strong>，也称为<strong>阈值神经单元（threshold logic unit）</strong>；也叫<strong>功能神经元</strong>。</p><p><strong>输入层只是接受外界信号（样本属性）并传递给输出层</strong>（输入层的神经元个数等于样本的属性数目），而<strong>没有激活函数</strong>。</p><div align=center><img src="/images/ML-pics/5.3.png" /></div><p>于是，感知机与之前线性模型中的对数几率回归的思想基本是一样的，<strong>都是通过对属性加权与另一个常数求和</strong>，再使用sigmoid函数将这个输出值压缩到0-1之间，从而解决分类问题。</p><p>不同的是感知机的输出层应该可以有多个神经元，从而可以实现多分类问题，同时两个模型所用的参数估计方法十分不同。</p><h4 id="学习">学习</h4><p>给定数据集，权重 W<sub>i</sub>(i = 1,2…,n)以及阈值 θ 可以通过<strong>学习</strong>得到。阈值 θ 可以看作一个固定的输入为 -1.0 的“哑结点”（dummy node）所对应的连接权重 w<sub>n+1</sub>，这样我们就<strong>可以i将权重与阈值的学习统一为权重学习</strong>。</p><p>感知机的学习规则非常简单，对于训练样例 （x，y），若当前感知机的输出为 y<sup>~</sup>，则感知机权重这样调整:</p><div align=center><img src="/images/ML-pics/L5.1.png" /></div><p>其中 ŋ ∈（0，1）称为学习率（learning rate）。从上图知道，如果预测正确，即样例的 y 与预测值 y<sup>~</sup>相等，则不会调整权重。</p><h4 id="缺陷">缺陷</h4><p>感知机只有输出层神经元进行激活函数处理，也<strong>就是只有这一层功能神经元</strong>，学习能力有限。而 <strong>与、或、非问题都是线性可分（linearly separable）问题</strong>。对于这种线性可分问题，感知机的学习过程一定是收敛从而求得适当的权向量 w = （w1;w2;…;wn+1）;不收敛的话，感知机就会发生<strong>振荡（fluctuation）</strong>。权向量难以稳定，无法求得合适解。</p><p>比如下面的非线性可分问题“异或”就无法解决：</p><div align=center><img src="/images/ML-pics/5.4.png" /></div><blockquote><p>其实就是一个分类问题，当分类边界呈现非线性时感知机就无法解决了</p></blockquote><h4 id="点到超平面距离">点到超平面距离</h4><p>上面有讲到点到超平面，这里讨论一下点到超平面的距离。</p><p><strong>超平面（Hyperplane）</strong>：超平面是n维欧氏空间中余维度等于一的线性子空间，也就是必须是(n-1)维度。<strong>其实相当于是一个n维空间的一种n-1维分界平面。</strong></p><p>我们将数据向量化：</p><ul><li>阈值 θ = w0：</li><li>所以可以将输出向量化如下图：</li></ul><div align=center><img src="/images/ML-pics/L5.2.png" /></div><p>然后我们可以知道<strong>点到超平面(Hyperplane)的距离</strong>:</p><div align=center><img src="/images/ML-pics/L5.3.png" /></div><blockquote><p>也就是 输出的值 det(w<sup>T</sup>x) 除以权值的2-范数 ||w||<sup>2</sup></p></blockquote><h4 id="损失函数">损失函数</h4><p>损失函数的一个自然选择是误分类点的总数。但是这样的损失函数不是参数w和b的连续可导函数，不易优化。</p><p>所以我们将目标选到了<strong>最小化误分类点到分类平面的距离</strong>。得到以下结果：</p><div align=center><img src="/images/ML-pics/L5..4.png" /></div><p>而感知机的目标是使误分类点的个数为0，所以提出了新的概念叫函数间隔，以此简化学习过程：</p><div align=center><img src="/images/ML-pics/L5.2.1_1.png"/></div>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> notes </tag>
            
            <tag> ML-notes </tag>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ML-notes:决策树</title>
      <link href="/2024/04/15/ML-notes/pages/decision-tree/"/>
      <url>/2024/04/15/ML-notes/pages/decision-tree/</url>
      
        <content type="html"><![CDATA[<h1 id="4-决策树">4 决策树</h1><h2 id="4-1-a-name-4-1-基本概念-a">4.1 <a name='4.1'>基本概念</a></h2><h3 id="4-1-1-a-name-4-1-1-举例子-a">4.1.1 <a name='4.1.1'>举例子</a></h3><p>多分类问题实质上通过划分的方法转化为多个二分类问题进行求解。这次我们将讨论另一种被广泛使用的分类算法–<strong>决策树（Decision Tree）</strong>。</p><p>比如 一个相亲——母女对话：</p><ul><li>女儿：多大年纪了？</li><li>母亲：26。</li><li>女儿：长的帅不帅？</li><li>母亲：挺帅的。</li><li>女儿：收入高不？</li><li>母亲：不算很高，中等情况。</li><li>女儿：是公务员不？</li><li>母亲：是，在税务局上班呢。</li><li>女儿：那好，我去见见。</li></ul><blockquote><p>此例子纯属虚构，不代表广大女性同胞的择偶标准。如有雷同纯属巧合。<br>我们就可以通过这段对话，画出一个决策树。</p></blockquote><div align=center><img src="/images/ML-pics/4-2.png" /></div><h3 id="4-1-2-a-name-4-1-2-决策树-a">4.1.2 <a name='4.1.2'>决策树</a></h3><p><strong>决策树（decision tree）</strong>：是构建出的一个基于属性的树形<br>分类器。</p><ul><li>每个非叶节点表示一个特征属性上的测试（分割）（判断）</li><li>每个分支代表这个特征属性在某个值域上的输出（分支）</li><li>每个叶节点存放一个类别（结果）</li></ul><p>使用决策树进行决策的<strong>过程</strong>就是从<strong>根节点开始</strong>，<strong>测试</strong>待分类项中相应的<strong>特征属性</strong>，并按照其值<strong>选择输出分支</strong>，直到<strong>到达叶子节点</strong>，将叶子节点存放的类别作为<strong>决策结果</strong>。</p><h2 id="4-2-a-name-4-2-决策树的构建-a">4.2 <a name='4.2'>决策树的构建</a></h2><p>决策树的构建采用<strong>分治法</strong>的思想（递归）。而结束递归的条件如下：</p><ul><li>① 当前结点样本均属于同一类别，无需划分。（只有一种结果）</li></ul><blockquote><p>Example: 下一个要划分的属性为属性1，显然无论属性为何种，均为 P 类无需划分。</p><div align=center><img src="/images/ML-pics/4-3.png" /></div></blockquote><ul><li>② 当前属性集为空。 （没有下一个结点）</li></ul><blockquote><p>Example: 属性1(B)→属性2(A)→属性3(A) 走完该路径已经无属性往下分</p><div align=center><img src="/images/ML-pics/4-4.png" /></div></blockquote><ul><li>③ 所有样本在当前属性集上取值相同，无法划分。（每个结点只有一条分支）</li></ul><blockquote><p>Example: 属性1 的 B分支下，样本子集中所有样本属性值完全一样，再往下划分就没有意义了。</p><div align=center><img src="/images/ML-pics/4-5.png" /></div></blockquote><ul><li>④ 当前结点包含的样本集合为空，不能划分。</li></ul><blockquote><p>Example: 属性 1 → 属性 2 的分支下只有 A ，无其他子集。</p><div align=center><img src="/images/ML-pics/4-6.png" /></div></blockquote><p><strong>伪代码：</strong></p><div align=center><img src="/images/ML-pics/4-7.png" /></div><h2 id="4-3-a-name-4-3-决策树的核心-a">4.3 <a name='4.3'>决策树的核心</a></h2><p>决策树的核心是：<strong>如何选取最佳划分属性</strong>。</p><p>比如我们举一个极端的例子：</p><div align=center><img src="/images/ML-pics/4-8.png" /></div>很显然，其实根据属性 3 就可以判断出 正负例 了。那加入其他的属性进行判断就显得多余了。<h3 id="4-3-1-a-name-4-3-1-最佳划分-a">4.3.1 <a name='4.3.1'>最佳划分</a></h3><h4 id="最佳划分属性">最佳划分属性</h4><p>我们希望经过属性划分后，不同类样本被更好的分离。</p><ul><li><strong>理想情况</strong>：划分后样本被完美分类。即划分到每个分支的样本都属于同一类。</li><li><strong>实际情况</strong>：不可能完美划分！尽量使得每个分支某一类样本比例尽量高！即尽量提高划分后<strong>子集的纯度（purity）</strong>。</li></ul><h4 id="最佳划分目标">最佳划分目标</h4><ul><li>提升划分后子集的纯度</li><li>降低划分后子集的不纯度</li></ul><p>因此下面便是介绍<strong>量化纯度</strong>的具体方法，决策树最常用的算法有三种：ID3，C4.5和CART。</p><h3 id="4-3-2-a-name-4-3-2-id3-决策树算法-a">4.3.2 <a name='4.3.2'>ID3 决策树算法</a></h3><p>ID3 算法使用信息增益为准则来选择划分属性，“<strong>信息熵</strong>”(information entropy)是度量样本结合纯度的常用指标，所以我们先介绍信息熵。</p><h4 id="信息熵">信息熵</h4><p>假定当前样本集合 D 中第k类样本所占比例为 p<sub>k</sub>，则样本集合D的信息熵定义为：</p><div align=center><img src="/images/ML-pics/4-9.png" /></div><blockquote><p>Ent(D)的值越小，则 D 的纯度越高</p></blockquote><p>假定离散属性 a 有 V 个可能的取值{a<sup>1</sup>,a<sup>2</sup>…,a<sup>V</sup>}。若使用 a 来对样本集 D 进行划分，那么会产生 V 个分支结点，其中 D<sup>v</sup> 表示第 v 个分支结点包含了 D 中所有在属性 a 上取值为 a<sup>v</sup>的样本数。然后呢，我们可以<strong>根据上面的式子计算出 D<sup>v</sup> 的信息熵</strong>，再考虑到不同分支结点包含的样本数的不同，我们还需要对<strong>分支结点赋予权重 |D<sup>v</sup>|/|D|</strong>，即<strong>样本数越多的分支结点的影响越大</strong>，所以就计算出用属性 a 对样本集 D 进行划分所得到的 <strong>信息增益</strong>:</p><div align=center><img src="/images/ML-pics/4-10.png" /></div>信息增益越大，表示使用该属性划分样本集D的效果越好，因此ID3算法在递归过程中，每次选择最大信息增益的属性作为当前的划分属性。也就是每次可以得到最佳划分属性。<h4 id="例子">例子</h4><div align=center><img src="/images/ML-pics/4-11.png" /></div><div align=center><img src="/images/ML-pics/4-12.png" /></div><div align=center><img src="/images/ML-pics/4-13.png" /></div><div align=center><img src="/images/ML-pics/4-14.png" /></div><div align=center><img src="/images/ML-pics/4-15.png" /></div><div align=center><img src="/images/ML-pics/4-16.png" /></div><h3 id="4-3-3-a-name-4-3-3-c4-5-算法-a">4.3.3 <a name='4.3.3'>C4.5 算法</a></h3><h4 id="提出">提出</h4><p><strong>ID3算法存在一个问题，就是偏向于取值数目较多的属性</strong></p><blockquote><p>例如：在学生成绩分类中，考虑学号为一个属性，然而学号基本每个人都有，所以得到得纯度很高，但是对分类毫无用处</p><div align=center><img src="/images/ML-pics/4-17.png" /></div></blockquote><p>因此又提出了 <strong>C4.5算法</strong> 通过 <strong>“增益率”（gain ratio）</strong> 来选择划分属性，来避免这个问题带来的困扰。</p><h4 id="增益率">增益率</h4><p>首先使用ID3算法计算出<strong>信息增益</strong>高于平均水平的候选属性，接着C4.5计算这些候选属性的增益率，<strong>增益率</strong> Gain_ratio 定义为：</p><div align=center><img src="/images/ML-pics/4-18.png" /></div><p>我们称上图的 IV(a) 为属性 a 的 “固有值”。属性 a 的可能取值越多（V 越多），则 IV(a) 通常会越大。</p><p>C4.5 算法不是直接选择增益值最大的候选划分属性，而是类似于一种启发式算法：先从划分属性中找到信息增益高于平均水平的属性，再计算它们的增益率，从增益率内找最高的。</p><h3 id="4-3-4-a-name-4-3-4-cart-算法-a">4.3.4 <a name='4.3.4'>CART 算法</a></h3><p>CART 是另一种决策树算法，使用 <strong>“基尼指数”</strong> 来选择划分属性。采用与 Ent(D) 信息熵计算相同的符号p<sub>k</sub>，数据集 D 的纯度可以用 <strong>基尼值</strong> 来度量：</p><div align=center><img src="/images/ML-pics/4-19.png" /></div>直观的说，Gini(D) 反映了从数据集 D 中随机抽取两个样本，其类别标记不一致的概率。因此，Gini(D) 越小，则数据集 D 的纯度最高。<br><br><p>同样的,基尼值对应信息熵，CART 的<strong>基尼指数</strong>也对应信息增益：</p><div align=center><img src="/images/ML-pics/4-20.png" /></div><blockquote><p>Gini_index 和 Gain_ratio 计算过程一样。CART 和 ID3 算法的区别在于 Gini 和 Gain 的区别。一个是 p<sub>k</sub>log<sub>2</sub>p<sub>k</sub> 一个是 p<sub>k</sub>p<sub>k</sub>’</p></blockquote><p>研究表明: 划分选择的各种准则虽然对决策树的尺寸有较大影响，但对泛化性能的影响很有限。（也就是其实是更改准则对模型的效果影响有限）</p><blockquote><p>例如信息增益与基尼指数产生的结果，仅在约 2% 的情况下不同。</p></blockquote><p>而剪枝方法和程度对决策树泛化性能的影响更为显著。</p><blockquote><p>在数据带噪时甚至可能将泛化性能提升 25%。</p></blockquote><h2 id="4-4-a-name-4-4-剪枝处理-a">4.4 <a name='4.4'>剪枝处理</a></h2><h3 id="4-4-1-a-name-4-4-1-概念-a">4.4.1 <a name='4.4.1'>概念</a></h3><p>从决策树的构造流程中我们可以直观地看出：不管怎么样的训练集，决策树总是能很好地将各个类别分离开来，这时就会遇到之前提到过的问题：过拟合（overfitting），即太依赖于训练样本。<strong>剪枝</strong>（pruning）顾名思义，将多余的分支剪掉，是决策树算法<strong>对付过拟合</strong>的主要手段，剪枝的策略有两种如下：</p><blockquote><ul><li>预剪枝（prepruning）：在构造的过程中先评估，再考虑是否分支。具体是 若当前结点向下划分不能提升决策树泛化性能，则进行裁剪，把结点标记为叶结点，不再向下分支。也就是评估分支与不分支的性能进行比较。</li></ul></blockquote><blockquote><ul><li>后剪枝（post-pruning）：在构造好一颗完整的决策树后，自底向上，评估分支的必要性。具体是 自底向上，判断结点对应的子树被替换为叶结点是否能提升决策树泛化能力，是则进行裁剪（将该结点的子树替换为叶结点）。（如果不懂看这里：其实就是从下向上，看我这一层要是成了叶结点是否会更好，更好则裁剪）</li></ul></blockquote><h3 id="4-4-2-a-name-4-4-2-性能度量-a">4.4.2 <a name='4.4.2'>性能度量</a></h3><p>显然，问题就来了，我怎么知道更好呢？</p><p>之前学的<a href="/2024/04/15/ML-notes/pages/model-evaluation/" title="ML-notes:模型评估与选择">2.模型评估与选择</a>就有用了，其实就是所谓的<strong>性能度量</strong>。之前讲过的 <strong>验证集</strong> 在这里也就有了用处。</p><h4 id="例子">例子</h4><p>书上是使用的留用法，预留一部分数据作为 验证集 来进行性能评估。先看西瓜数据集：</p><div align=center><img src="/images/ML-pics/4-21.png" /></div><p>我们随机将其划分为了两部分，一部分是训练集，一部分是验证集。我们通过<strong>信息增益准则</strong>可以进行属性划分选择，生成一个<strong>未剪枝</strong>的决策树：</p><div align=center><img src="/images/ML-pics/4-22.png" /></div><p><strong>开始了啊！</strong></p><h4 id="预剪枝">预剪枝</h4><div align=center><img src="/images/ML-pics/4-23.png" /></div><p>预剪枝，每次选定属性后，要进行一次评估。图 4.6 就是第一次评估。我首先默认，不作划分的话都是好瓜。对于 ① 脐部，划分前，经过<strong>它所包含的验证集</strong>进行验证，准确率 42.9%，划分后 71.4%，显然需要划分。然后同样的步骤，对于下一层，当然每次划分都需要经过 ① 脐部划分之后再进行，比较验证集精度来进行评估。</p><h4 id="后剪枝">后剪枝</h4><div align=center><img src="/images/ML-pics/4-22.png" /></div><div align=center><img src="/images/ML-pics/4-24.png" /></div><p>后剪枝就直接对 4-2 的未剪枝的决策树进行操作。从纹理层，也就是 ⑥ 开始操作，我们将它剪去，默认分支结果为好瓜，使用<strong>它所包含的验证集</strong>的{1,2,3,14}测试，看模型精度是否会更好。后面的也是一样的了。</p><blockquote><p>注意：每次使用验证集都是直接使用它所包含的验证集的样本，而非所有，因为包含其它分支的样本进行测试对结果无影响。</p></blockquote><h4 id="比较">比较</h4><ul><li>时间开销：<ul><li>预剪枝：训练时间开销降低（不需要生成一次未剪枝的决策树），测试时间开销降低</li><li>后剪枝：训练时间开销增加（需要生成一次未剪枝的决策树），测试时间开销降低</li></ul></li><li>过/欠拟合风险：<ul><li>预剪枝：过拟合风险降低，欠拟合风险增加（没有生成过未剪枝的决策树，剪枝之前得到的信息不全面）</li><li>后剪枝：过拟合风险降低，欠拟合风险基本不变</li></ul></li><li>泛化性能：后剪枝 通常优于 预剪枝</li></ul><h2 id="4-5-a-name-4-5-连续值与缺失值的处理-a">4.5 <a name='4.5'>连续值与缺失值的处理</a></h2><h3 id="4-5-1-a-name-4-5-1-连续值-a">4.5.1 <a name='4.5.1'>连续值</a></h3><p>对于连续值的属性，若每个取值作为一个分支则显得不可行，因此需要进行离散化处理，常用的方法为二分法，基本思想为：给定样本集 D 与连续属性 α ，二分法试图找到一个划分点 t 将样本集 D 在属性 α 上分为 &lt;=t 与 &gt; t (也就是划分出一个区间)。</p><p>二分法：</p><ul><li>n 个属性值可形成 n‐1 个候选划分</li><li>然后即可将它们当做 n‐1 个离散属性值处理</li><li>处理流程：</li></ul><blockquote><ul><li>首先将 α 的所有取值按升序排列，所有<strong>相邻属性的均值作为候选划分点</strong>（n-1个，n为α所有的取值数目）。比如一共五个属性值，则如下：</li></ul><div align=center><img src="/images/ML-pics/line-1.png" /></div><ul><li>计算每一个划分点作为二分点后的信息增益。</li></ul><div align=center><img src="/images/ML-pics/line-2.png" /></div><ul><li>选择最大信息增益的划分点作为最优划分点。</li></ul></blockquote><h3 id="4-5-2-a-name-4-5-2-缺失值-a">4.5.2 <a name='4.5.2'>缺失值</a></h3><p>现实应用中，我们也会遇到属性值“缺失”(missing)的现象。如果不使用有缺失属性的样例训练，那将是巨大的浪费。那么样本的属性缺失该如何划分？</p><p>因此在属性值缺失的情况下需要解决两个问题：</p><ul><li>（1）如何选择划分属性。</li><li>（2）给定划分属性，若某样本在该属性上缺失值，如何划分到具体的分支上。</li></ul><p>我们提出了：<strong>样本赋权，权重划分</strong></p><h4 id="例子">例子</h4><p>我们给出了含缺失值的西瓜数据集：</p><div align=center><img src="/images/ML-pics/4-25.png" /></div><p>假定为样本集 D 中的每一个样本 x 都赋予一个权重 w<sub>x</sub>，同时我们规定，样本集 D 中对于 属性 α 没有缺失值的集合为 <img src="/images/ML-pics/D_.png" /> ，划分到达根节点中时的权重初始化为1，则定义：</p><div align=center><img src="/images/ML-pics/4-26.png" /></div><p>为了<strong>解决问题 （1）</strong>，我们通过在样本集 D 中选取在属性α上<strong>没有缺失值的样本子集</strong>，计算了在该样本子集上的信息增益，<strong>该信息增益推广为等于样本子集占样本集的<u>比重</u>乘以无缺失样本子集划分后<u>信息增益</u></strong>。即：</p><div align=center><img src="/images/ML-pics/4-28.png" /></div><div align=center><img src="/images/ML-pics/4-27.png" /></div><p>然后为了<strong>解决问题（2）</strong>，若该样本子集在属性α上的值<strong>缺失</strong>，则将该样本以不同的权重（即每个分支所含<strong>无缺失样本比例</strong>）划入到所有分支节点中。该样本在分支节点中的权重变为：</p><div align=center><img src="/images/ML-pics/4-29.png" /></div><blockquote><p>也就是说，缺失值的话，会将该样本划入所有子结点，但是不是 以一个单位来划分，而是以权值划分。这是针对训练集的划分。</p></blockquote><h2 id="4-6-a-name-4-6-单变量与多变量决策树-a">4.6 <a name='4.6'>单变量与多变量决策树</a></h2><p>决策树也分单变量与多变量决策树。</p><h3 id="4-6-1-a-name-4-6-1-单变量决策树-a">4.6.1 <a name='4.6.1'>单变量决策树</a></h3><p>单变量决策树：在每个非叶结点仅考虑一个划分属性。</p><p>但是只是轴平行划分，也就是下图右方那样，形成简单的平行于轴的划分边界。</p><div align=center><img src="/images/ML-pics/4-30.png" /></div>当学习任务所对应的分类边界很复杂时，需要非常多段划分才能获得较好的近似，单变量就不再适用：<div align=center><img src="/images/ML-pics/4-31.png" /></div><h3 id="4-6-2-a-name-4-6-2-多变量决策树-a">4.6.2 <a name='4.6.2'>多变量决策树</a></h3><p>于是便有了多变量决策树。</p><p>多变量决策树：每个非叶结点不仅考虑一个属性</p><p>例如“斜决策树” (oblique decision tree) 不是为每个非叶结点寻找最优划分属性，而是建立一个线性分类器。</p><div align=center><img src="/images/ML-pics/4-32.png" /></div><h2 id="4-7-a-name-4-7-思考-a">4.7 <a name='4.7'>思考</a></h2><div align=center><img src="/images/ML-pics/4-33.png" /></div>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> notes </tag>
            
            <tag> ML-notes </tag>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ML-notes:线性模型</title>
      <link href="/2024/04/15/ML-notes/pages/linear-model/"/>
      <url>/2024/04/15/ML-notes/pages/linear-model/</url>
      
        <content type="html"><![CDATA[<h1 id="3-线性模型">3 线性模型</h1><blockquote><p>由于时间原因，这里只讲解部分内容</p></blockquote><h2 id="3-3-a-name-3-3-对数几率回归-a">3.3 <a name='3.3'>对数几率回归</a></h2><p>虽然只讲这个，但是我们还是要提一提一些概念。</p><h3 id="3-3-1-a-name-3-3-1-前情提要-a">3.3.1 <a name='3.3.1'>前情提要</a></h3><ul><li><strong>线性模型</strong>：其实我们很早就已经与它打过交道，比如我们熟知的“最小二乘法”。这就是线性模型的经典算法之一：根据给定的（x，y）点对，求出一条与这些点拟合效果最好的直线y=ax+b。</li><li><strong>线性回归</strong>：就是试图学到一个线性模型尽可能准确地预测新样本的输出值。</li></ul><div align=center><img src="/images/ML-pics/3-1.png" /></div><ul><li><strong>监督学习</strong>：<a href="/2024/04/14/ML-notes/pages/introduction/" title="ML-notes:绪论">1.绪论</a>的方法分类有提到。</li><li><strong>回归与分类</strong>：我们可以通过线性回归的思想来解决一些分类任务，比如二分类问题。</li></ul><div align=center><img src="/images/ML-pics/3-2.png" /></div><blockquote><p>直观上说，可以规定直线上方的点为正样本(Positive) ，直线下方的点为负样本(Negative) 。本质上说，我们是需要把连续实数值转化为离散值的(例如: 𝟎, 𝟏)：</p><ul><li>比如：对于二分类任务，线性模型预测出来的是 连续值 z = wx + b，所以我们需要将 z  转换为 0/1 值，最理想的就是单位阶跃函数：</li></ul><div align=center><img src="/images/ML-pics/3-3.png" /></div></blockquote><blockquote><p>直观就是我们可以使用一个<strong>线性分类器𝒇(𝒙)</strong>，当𝒙为正类样本，𝒇 (𝒙) &gt; 𝟎，反之， 𝒙 为负类样本，则 𝒇 (𝒙) &lt; 𝟎 。</p></blockquote><blockquote><p>当然这只是一种基本分类思想，我们还需要对分类器的好坏进行度量，也就是上一章的<strong>模型评估与选择</strong>。当前的分类器是无法与标签相对应，自然也无法纠正分类错误，毕竟分类器的输出是线性的，而 标签 是离散的。（分类器 输出范围 [-∞，+∞]，而 标签 可以是{1，0}，{1，-1}）</p></blockquote><blockquote><p>然而单位阶跃函数是不好满足这个条件的，所以就提到了 对数几率函数。</p></blockquote><h3 id="3-3-2-a-name-3-3-2-对数几率函数-a">3.3.2 <a name='3.3.2'>对数几率函数</a></h3><p><strong>回归</strong>就是通过输入的属性值得到一个预测值，利用广义线性模型的特征，是否可以通过一个<strong>联系函数</strong>，将预测值转化为离散值从而进行分类呢？线性几率回归正是研究这样的问题。对数几率引入了一个<strong>对数几率函数（logistic function）</strong>,将预测值投影到 0-1 之间，从而将线性回归问题转化为二分类问题。单位阶跃函数不是一个连续函数，我们引入对数几率函数（logistic function）正好可以替代它：</p><div align=center><img src="/images/ML-pics/3-4.png" /></div>从图3.2可以看出，Logistic Function 对数几率函数是一种“Sigmoid”函数，它将 z 值转化为一个接近0 或 1 的值 y，并且输出值在 z = 0 附近变化很陡。<p>若将 y 视为样本作为正例的可能性，那么 1 - y 就是反例的可能性，则实际上这就是使用线性回归模型的预测结果器逼近真实标记的对数几率。因此这个模型称为 <strong>“对数几率回归”（logistic regression）</strong>，也有一些书籍称之为“逻辑回归”。</p><div align=center><img src="/images/ML-pics/3-8.png" /></div><p>两者的比值，我们称之为 <strong>几率 （odds）</strong>，反应了 x 作为正例的相对可能性，而取对数就得到了 <strong>对数几率（log odds 又称 logit）</strong>。</p><div align=center>几率 （odds）<img src="/images/ML-pics/3-6.png" /></div><div align=center>对数几率（log odds 又称 logit）<img src="/images/ML-pics/3-7.png" /></div><h3 id="3-3-3-a-name-3-3-3-最大似然估计-a">3.3.3 <a name='3.3.3'>最大似然估计</a></h3><p>我们可以使用最大似然估计的方法，求得下述对数几率模型的解 w,b。</p><div align=center><img src="/images/ML-pics/3-9.png" /></div>我们利用极大似然的思想构建目标函数：（3.23 正例概率 3.24 反例概率）<div align=center><img src="/images/ML-pics/3-10.png" /></div>通过极大似然法，针对给定数据集{x,y}求解：<blockquote><p>对数变乘为加，且采用了最大化似然（即所有样本出现真实值的概率乘积最大）</p></blockquote><div align=center><img src="/images/ML-pics/3-11.png" /></div><h3 id="3-3-4-a-name-3-3-4-对数几率模型-a">3.3.4 <a name='3.3.4'>对数几率模型</a></h3><div align=center><img src="/images/ML-pics/3-12.png" /></div>所以：<div align=center><img src="/images/ML-pics/3-13.png" /></div><h4 id="3-3-4-1-a-name-3-3-4-1-牛顿法-newton-s-method-a">3.3.4.1 <a name='3.3.4.1'>牛顿法（Newton’s Method）</a></h4><p>牛顿法有两种应用方向，1、目标函数最优化求解， 2、方程的求解（根）</p><p>核心思想是对函数进行泰勒展开。</p><ul><li>方程求解：</li></ul><div align=center><img src="/images/ML-pics/3-15.png" /></div>用牛顿法可以解非线性方程，它是把非线性方程 f(x)=0 线性化的一种近似方法。把f(x)在点x0的某邻域内展开成泰勒级数。<div align=center><img src="/images/ML-pics/3-14.png" /></div>取其线性部分（即泰勒展开的前两项），并令其等于0，即<div align=center><img src="/images/ML-pics/3-16.png" /></div>以此作为非线性方程f(x)=0 的近似方程，若f’(xo)不为0，则其解为<div align=center><img src="/images/ML-pics/3-17.png" /></div>这样，得到牛顿迭代法的一个迭代关系式：<div align=center><img src="/images/ML-pics/3-18.png" /></div><div align=center><img src="/images/ML-pics/3-19.png" /></div><h4 id="3-3-4-2-a-name-3-3-4-2-梯度下降法-a">3.3.4.2 <a name='3.3.4.2'>梯度下降法</a></h4><p>梯度下降（gradient descent）在机器学习中应用十分的广泛，不论是在线性回归还是Logistic回归中，它的主要目的是通过迭代找到目标函数的最小值，或者收敛到最小值。</p><div align=center><img src="/images/ML-pics/3-20.png" /></div><p>我们有个<strong>可微分</strong>的函数，它代表一座山。我们的目标是找到山底。<strong>梯度</strong>的方向是函数变化最快的方向也就是平地上最陡的方向。所以梯度下降的步骤是这样的：</p><ul><li>从一个出发点出发，我们开始求出发点的梯度</li><li>向梯度方向的<strong>负方向</strong>行走一个步长</li><li>重复求取梯度，重复行走步长</li><li>直到走到底</li></ul><h3 id="3-3-5-a-name-3-3-5-总结-a">3.3.5 <a name='3.3.5'>总结</a></h3><div align=center><img src="/images/ML-pics/3-21.png" /></div><ul><li>牛顿法和梯度下降法是求解最优化问题的常见的两种算法。</li><li>前者使用割线逐渐逼近最优解，后者使得目标函数逐渐下降。</li><li>牛顿法的收敛速度快，但是需要二阶导数信息。</li><li>梯度下降法计算速度快，但是需要人工确认步长参数。</li><li>极大似然法：</li></ul><div align=center><img src="/images/ML-pics/3-22.png" /></div>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> notes </tag>
            
            <tag> ML-notes </tag>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ML-notes:模型评估与选择</title>
      <link href="/2024/04/15/ML-notes/pages/model-evaluation/"/>
      <url>/2024/04/15/ML-notes/pages/model-evaluation/</url>
      
        <content type="html"><![CDATA[<h1 id="2-模型评估与选择">2 模型评估与选择</h1><h2 id="1-a-name-1-经验误差与过拟合-a">1. <a name='1'>经验误差与过拟合</a></h2><h3 id="1-1-a-name-1-1-经验-experiences-a">1.1 <a name='1-1'>经验 Experiences</a></h3><blockquote><p>Experience = The data we have for training the machine learning model.</p></blockquote><p>对于特定机器学习任务，<strong>已存在的可利用数据</strong>即是解决该机器学习任务的<strong>经验</strong>。而在这个大数据时代，大数据=丰富经验=训练更好的机器学习模型。</p><h3 id="1-2-a-name-1-2-数据划分-a">1.2 <a name='1-2'>数据划分</a></h3><p>通常我们会对获取到的数据进行数据划分，也就是我们绪论提到的一些学术用语：</p><div align=center><img src="/images/ML-pics/2-1.png" /></div><ul><li><strong>训练集 Training Set</strong>：用来训练模型或确定参数。</li><li><strong>测试集 Testing Set</strong>：测试已经训练好的模型的推广能力。</li><li><strong>验证集 Validation Set</strong>（可选）：是模型训练过程中单独留出的样本集，它可以用于调整模型的超参数和用于对模型的能力进行初步评估。用来做模型选择（Model Selection），即模型的最终优化及选择。</li></ul><h3 id="1-3-a-name-1-3-误差与精度-a">1.3 <a name='1-3'>误差与精度</a></h3><h4 id="误差-error">误差 Error</h4><p>我们将学习器对样本的<strong>实际预测结果与样本的真实值之间的差异</strong>称之为<strong>误差（error）</strong>。而误差包含三类：</p><ul><li><strong>训练误差 training error 或 经验误差 empirical error</strong>：学习器在训练集上的误差。</li><li><strong>泛化误差generalization error</strong>：学习器在新样本的误差，也就是实际误差。</li><li><strong>测试误差Testing Error</strong>：学习器在测试集上的误差，用来近似泛化误差。因为对于实际误差总是变化着的，所以我们一般用测试误差来近似泛化误差。</li></ul><h4 id="经验误差vs-泛化误差">经验误差vs.泛化误差</h4><ul><li><strong>经验误差</strong>：在训练集上的误差，训练误差</li><li><strong>泛化误差</strong>：在新样本上的误差，实际误差</li></ul><blockquote><p>是不是两个误差都越小越好？</p></blockquote><blockquote><p>不，因为会出现过拟合 overfitting</p></blockquote><h3 id="1-4-a-name-1-4-过拟合-overfitting-vs-欠拟合-underfitting-a">1.4 <a name='1-4'>过拟合(overfitting) vs. 欠拟合(underfitting)</a></h3><div align=center><img src="/images/ML-pics/2-2.png" /></div><p>我们以叶子为例子，如图所示，过拟合的话，会认为树叶必须有锯齿，欠拟合的话，认为绿色都是树叶。所以误差过小或过大，都会与预期结果不符。</p><h4 id="我的定义">我的定义</h4><ul><li><strong>过拟合（over-fitting）</strong>：所建立的机器学习模型或者是深度学习模型在训练样本中表现得过于优越，导致在验证集以及测试集中表现不佳。</li><li><strong>欠拟合（under-fitting）</strong>：训练样本被提取的特征较少，导致训练出来的模型不能很好地匹配，导致表现不佳，甚至样本本身都无法高效的识别。</li></ul><blockquote><p>非官方解释</p></blockquote><p><u>所以我们发现 一个模型的评估与选择是多么的重要。</u></p><h2 id="2-a-name-2-模型选择-a">2. <a name='2'>模型选择</a></h2><p>模型选择主要是回答三个问题，也就是下面我们讲的三个问题：</p><ul><li>如何获得测试结果？ -&gt; <strong>2.1 评估方法</strong></li><li>如何评估性能优劣？ -&gt; <strong>2.2 性能度量</strong></li><li>如何判断实质差别？ -&gt; <strong>2.3 比较检验</strong></li></ul><h3 id="2-0-a-name-2-0-典型的机器学习过程-a">2.0 <a name='2-0'>典型的机器学习过程</a></h3><p>我认为通常机器学习分五步：</p><ul><li><strong>得到数据</strong>：机器学习过程中，最重要并且困难的通常都是<strong>清理和预处理数据</strong>。除开直接得到的数据外，一般数据都需要数据挖掘、数据清洗、特征工程、数据转换、特征提取等一系列数据准备工作。</li><li><strong>模型选择</strong>：得到数据后，我们需要针对不同的问题和任务选取恰当的模型，而<strong>模型就是一组函数的集合</strong>。</li><li><strong>确定标准</strong>：选取到适合的模型后，我们需要确定一个衡量标准，也就是我们通常说的 <strong>损失函数（Loss Function）</strong> 来衡量模型函数的好坏。</li><li><strong>得到函数</strong>：最后也是困难的也是这一步 <strong>Pick the “Best” Function</strong>，我们需要进行多次训练，从众多函数中得到 Loss 最小的一个。通常会使用一些 梯度下降法、最小二乘法等技巧来选择。</li><li><strong>测试</strong>：学习得到“最好”的函数后，需要在<strong>新样本上进行测试</strong>，只有在新样本上表现很好，才算是一个“好”的函数。</li></ul><h4 id="调参">调参</h4><h5 id="概念">概念</h5><p>大多数学习算法都有些<strong>参数(parameter) 需要设定</strong>，参数配置不同，学得模型的性能往往有显著差别，这就是通常所说的<strong>参数调节或调参</strong> (parameter tuning)。</p><h5 id="如何调参">如何调参</h5><p>学习算法的很多参数是在实数范围内取值，因此，对每种参数取值都训练出模型来是不可行的。常用的做法是：<strong>对每个参数选定一个范围和步长λ</strong>，这样使得学习的过程变得可行。</p><p>例如：假定算法有 k 个参数，每个参数仅考虑 j 个候选值，这样对每一组训练/测试集就会有 k^j 个模型需考察。</p><blockquote><p>当重新调参之后就要重新训练模型，所以调参的时间代价是很大的。</p></blockquote><h5 id="验证集">验证集</h5><p>我们在<a href="#1-2">数据划分</a>的时候有说到<strong>验证集</strong>，有验证集的模型训练步骤是这样的：先在训练集上训练，验证集上做测试，重复以上步骤选出最好的模型，把训练集和验证集并到一起做训练，在测试集上最后测试。</p><p>也就是：算法参数设定后，要用“训练集+验证集”重新训练最终模型。而测试集，是用来评估最终模型的。</p><h3 id="2-1-a-name-2-1-评估方法-a">2.1 <a name='2-1'>评估方法</a></h3><p>评估方法描述了<strong>如何获得测试结果</strong>。而它的关键便是：怎么获得“测试集”(test set)。一般来说，我们应该保证<strong>测试集应该与训练集“互斥”</strong>。</p><p>所以我们常见的方法如下：</p><ul><li><strong>留出法(hold-out)</strong></li><li><strong>交叉验证法(cross validation)</strong></li><li><strong>自助法(bootstrap)</strong></li></ul><h4 id="2-1-1-a-name-2-1-1-留出法-a">2.1.1 <a name='2-1-1'>留出法</a></h4><div align=center><img src="/images/ML-pics/2-3.png" /></div><p>将数据集 D 划分为<strong>两个互斥的集合</strong>，一个作为训练集 S，一个作为测试集 T，满足 D=S∪T 且 S∩T=∅ ，常见的划分为：大约2/3-4/5的样本用作训练，剩下的用作测试。</p><blockquote><p>留出就是说，数据集的一部分作为训练集，剩下的留给测试集</p></blockquote><p><strong>注意事项：</strong></p><ul><li><strong>保持数据分布一致性</strong></li></ul><blockquote><p>（避免因数据划分过程引入额外的偏差而对最终结果产生影响，例如分类任务中<strong>保持两个数据集（S、T）样本的类别比例相似</strong>，也就是<strong>分层采样</strong>)</p></blockquote><ul><li><strong>多次重复划分</strong></li></ul><blockquote><p>(单次使用留出法得到的估计结果往往不够稳定可靠，需要多次实验)</p></blockquote><ul><li><strong>测试集不能太大、不能太小</strong></li></ul><blockquote><p>(<strong>测试集小</strong>时，评估结果的方差就比较大，<strong>训练集小</strong>时，评估结果的偏差较大。常见值：1/5~1/3，测试集至少应含30个样例<br>[Mitchell, 1997])</p></blockquote><h4 id="2-1-2-a-name-2-1-2-交叉验证法-a">2.1.2 <a name='2-1-2'>交叉验证法</a></h4><div align=center><img src="/images/ML-pics/2-4.png" /></div><p>将数据集D<strong>等分为k份相互不重叠的子集</strong>，每次<strong>取1份子集作为测试集，其余子集作为训练集</strong>。重复k次，直至所有子集都作为测试集进行过一次实验评估，最后取平均值。我们也称之为 <strong>k 折交叉认证</strong>。</p><p><strong>同样为保证数据分布一致性</strong>，即为减少因样本划分不同而引入的差别，一般会随机使用不同划分重复p次，比如10次10折交叉验证。</p><h4 id="2-1-3-a-name-2-1-3-自助法-a">2.1.3 <a name='2-1-3'>自助法</a></h4><p>自助法比较好理解，<strong>又称可放回采样</strong>。和我们学的概率论的可放回抽取类似：</p><p>假设有m个样本，每次采一个作为测试集，采到的概率是1/m，采不到的概率是(1-1/m)，采m次还采不到，概率为 (1-1/m) 的m次方。</p><div align=center><img src="/images/ML-pics/2-5.png" /></div>那么我们可以计算得到，其实最终约有 36.8% 的样本是不在训练集中的。这种方法取出的测试样本在数据集D中比例一般在25%~36.8%之间。<blockquote><p>它常用于数据集较小或难以有效划分训练/测试集情况。因为<strong>数据分布有所改变，会引入估计误差</strong>，初始数据量比较足够时，前两种方法更常用。</p></blockquote><h3 id="2-2-a-name-2-2-性能度量-a">2.2 <a name='2-2'>性能度量</a></h3><p><strong>性能度量(performance measure)</strong> 是衡量模型泛化能力的评价标准，反映了任务需求。使用不同的性能度量往往会导致不同的评判结果。</p><p>比如 回归(regression) 任务常用<strong>均方误差</strong>衡量性能：</p><div align=center><img src="/images/ML-pics/2-6.png" /></div><h4 id="2-2-1-a-name-2-2-1-错误率与精度-a">2.2.1 <a name='2-2-1'>错误率与精度</a></h4><p>对于样例 D：</p><ul><li><strong>错误率（error rate）</strong>：被错误分类的样本在总样本中的比例。</li></ul><div align=center><img src="/images/ML-pics/2-7.png" /></div><ul><li><strong>精度（accuracy）</strong>：被正确分类的样本在总样本中的比例，即（1 – 错误率）</li></ul><div align=center><img src="/images/ML-pics/2-8.png" /></div><blockquote><p>西瓜为例，<strong>错误率是</strong>有多少比例的瓜被判别错误。若关心“挑出的西瓜中有多少比例是好瓜”，或者“所有好瓜中有多少比例被挑了出来”，错误率就不够用了，需要使用其他性能度量。</p></blockquote><blockquote><p>有些需求在信息检索、Web搜索等应用中经常出现，例如在信息检索中，我们经常会关心“<strong>检索出的信息中有多少比例是用户感兴趣的</strong>”“<strong>用户感兴趣的信息中有多少被检索出来了</strong>”.<strong>“ 查准率”(precision)与“查全率”(recal)是更为适用于此类需求的性能度量.</strong></p></blockquote><h4 id="2-2-2-a-name-2-2-2-查全率-查准率-a">2.2.2 <a name='2-2-2'>查全率、查准率</a></h4><div align=center><img src="/images/ML-pics/2-9.png" /></div><ul><li>解释：<ul><li>FN: Fault Negative Example 错反例（错认为是反例）</li><li>FP: Fault Positive Example 错正例（错认为是正例）</li><li>其他的反之亦然</li></ul></li><li><strong>查准率、准确率(Precision)</strong>：预测为正的样例中有多少是真正的正样例（你认为的正例，有百分之多少是对的）<ul><li><strong>P = TP/(TP+FP)</strong></li></ul></li><li><strong>查全率、召回率(Recall)</strong>：样本中的正例有多少被预测正确（正例样本有百分之多少被你预测到了）<ul><li><strong>R = TP/(TP+FN)</strong></li></ul></li><li><strong>精度（accuracy）</strong>：被正确分类的样本在总样本中的比例（所有样本中预测正确的占百分之多少）<ul><li><strong>A=(TP+TN)/(TP+FN+TN+FP)</strong></li></ul></li></ul><h4 id="a-name-p-r-p-r曲线-a"><a name='p-r'><strong>P-R曲线</strong></a></h4><ul><li><p><strong>问题</strong></p><ul><li>查准率和查全率是一对矛盾的度量</li><li><strong>查全率</strong>是希望将好瓜尽可能多地挑选出来，所以选的瓜越多其实查全率也就越大。</li><li><strong>查准率</strong>是选出的好瓜里面有多少是真正的好瓜，只选有把握的瓜查准率也就越大。</li><li>但是这么看来，保证查准率的话，难免会漏掉好瓜，查全率得不到保证。</li></ul></li><li><p><strong>P-R曲线</strong></p><ul><li>“P-R曲线”正是描述查准/查全率之间关系的曲线</li><li><strong>定义</strong>：根据学习器的预测结果按正例可能性大小（与阈值之差）对样例进行排序，并逐个把样本作为正例计算P和R，得到很多个P和R，形成P-R图。</li></ul>  <div align=center>  <img src="/images/ML-pics/2-10.png" />  </div><ul><li>一般来说，曲线下的面积是很难进行估算的，所以衍生出了 <strong>“平衡点”（Break-Event Point，简称BEP）</strong>，即当P=R时的取值，平衡点的取值越高，性能更优。</li><li>当然 BEP 其实也太简化了一点，所以我们引入了新的度量方法：<strong>F-Measure</strong>。即 F 度量。</li></ul></li><li><p><strong>F-Measure</strong></p><ul><li>常用的是 <strong>F1 度量</strong>：(基于 P R 的调和平均值)</li></ul>  <div align=center>  <img src="/images/ML-pics/2-11.png" />  </div><ul><li>然而有时候我们对查全率和查准率的要求有偏差，这个时候我们就需要修改它们的权重，所以又引入了<strong>F<sub>β</sub> 度量</strong>：（基于 P R 的加权平均值）</li></ul>  <div align=center>  <img src="/images/ML-pics/2-12.png" />  </div><blockquote><p>β &gt; 1 时 R 查全率 有更大权重，β &lt; 1 时 P 查准率 有更大权重。β = 1 时，则为 F1 度量。</p></blockquote></li></ul><h4 id="a-name-m-m-macro-宏观-vs-micro-微观-a"><a name='m-m'>macro 宏观 vs micro 微观</a></h4><p>有时候我们会有多个二分类混淆矩阵，例如：多次训练或者在多个数据集上训练，那么估算全局性能的方法有两种，分为<strong>宏观和微观</strong>。</p><p>简单理解：</p><ul><li><strong>宏观</strong>就是先算出每个混淆矩阵的P值和R值，然后取得平均P值macro-P和平均R值macro-R，再算出F<sub>β</sub>或F<sub>1</sub>，</li><li><strong>微观</strong>则是计算出混淆矩阵的平均TP、FP、TN、FN，接着进行计算P、R，进而求出F<sub>β</sub>或F<sub>1</sub>。</li></ul><div align=center><img src="/images/ML-pics/2-13.png" /></div><h4 id="a-name-r-a-roc-与-aoc-a"><a name='r-a'>ROC 与 AOC</a></h4><p>如上所述：学习器对测试样本的<strong>评估结果</strong>一般为<strong>一个实值或概率</strong>，然后将这个预测值与一个 <strong>分类阈值(threshold)</strong> 比较，若大于阈值则分为正类，否则为反类，也就完成了分类。</p><blockquote><p>学习器也就是分类器。</p></blockquote><h5 id="概念">概念</h5><p>根据这个实值或概率预测结果，可将测试样本进行排序，“最可能”是正例的排在最前面，“最不可能”是正例的排在最后面。这种分类过程相当于在这个排序中以某个 <strong>“截断点”(cut point)</strong> 将样本分为两部分。根据任务需求采用<strong>不同截断点</strong>：更<strong>重视“查准率”</strong>，选择<strong>靠前</strong>的位置截断；更<strong>重视“查全率”</strong>，则从<strong>靠后</strong>的位置截断。</p><blockquote><p>其实通俗来说的话，就是分类器会将测试样本评估，并给它们评分，然后按照评分排序，之后根据你侧重 P 还是 R 来设置一个截断点作为下次预测的判断依据。比如 你设置截断点 75分以上是 优秀，75分以下是 差，那就是我要求比较低；我设置 90分以上是 优秀，90分以下是 差，那就是我要求比较高。这是同一个道理。</p></blockquote><p>因此，排序本身的质量好坏，就体现了 “一般情况下”泛化性能的好坏。</p><p>而 <strong>ROC曲线</strong> 正是从这个角度出发来研究学习器的泛化性能，ROC曲线与P-R曲线十分类似，都是按照排序的顺序逐一按照正例预测，不同的是ROC曲线以“真正例率”（True Positive Rate，简称TPR）为横轴，纵轴为“假正例率”（False Positive Rate，简称FPR），<strong>ROC偏重研究基于测试样本评估值的排序好坏</strong>。</p><blockquote><p>TPR = R ，TPR 和 FPR 可以看作 正例成功率 和 反例失败率，也就是 真正例TP占真正例的比例，假正例FP占真反例的比例。</p></blockquote><div align=center><img src="/images/ML-pics/2-14.png" /></div><div align=center><img src="/images/ML-pics/2-15.png" /></div>简单分析图像，可以得知：当FN=0时，TN也必须0（TPR = FPR = 1），反之也成立，我们可以画一个队列，试着使用不同的截断点（即阈值）去分割队列，来分析曲线的形状，（0,0）表示将所有的样本预测为负例，（1,1）则表示将所有的样本预测为正例，（0,1）表示正例全部出现在负例之前的理想情况，（1,0）则表示负例全部出现在正例之前的最差情况。<br><br><h5 id="绘图">绘图</h5><p>现实中的任务通常都是有限个测试样本，因此只能绘制出近似ROC曲线。绘制方法：首先根据测试样本的评估值对测试样本排序，接着按照以下规则进行绘制:</p><blockquote><p>其实就是绘制 最大阈值时 的点 和 最小阈值时 的点，及其每个样本作为阈值的 点，构成一个离散点集，得到近似ROC曲线。</p></blockquote><div align=center><img src="/images/ML-pics/2-16.png" /></div><h5 id="评估">评估</h5><p>同样地，进行模型的性能比较时，若一个学习器A的ROC曲线被另一个学习器B的ROC曲线<strong>完全包住</strong>（所有点更靠近（1，1）），则称B的性能优于A。</p><p>若A和B的曲线发生了交叉，则谁的曲线下的面积大，谁的性能更优。</p><p>所以就引出 ROC曲线下的面积 我们定义为<strong>AUC（Area Uder ROC Curve）</strong>，<strong>不同于P-R的是，这里的AUC是可估算的</strong>，即AOC曲线下每一个小矩形的面积之和。</p><div align=center><img src="/images/ML-pics/2-17.png" /></div><p>易知：AUC越大，证明排序的质量越好，AUC为1时，证明所有正例排在了负例的前面，AUC为0时，所有的负例排在了正例的前面，这两种情况其实都属于最理想的情况。（不过 AUC 为 0 肯定是 分类出问题了）</p><div align=center><img src="/images/ML-pics/2-18.png" /></div><h4 id="2-2-3-a-name-2-2-3-代价敏感错误率与代价曲线-a">2.2.3 <a name='2-2-3'>代价敏感错误率与代价曲线</a></h4><p>上面的方法中，将学习器的犯错同等对待，但在现实生活中，将正例预测成假例与将假例预测成正例的代价常常是不一样的，例如：好瓜预测为坏瓜，顶多损失了好瓜；而坏瓜预测为好瓜，吃了会中毒，这是不一样的代价。</p><p>所以以二分类为例，由此引入了“代价矩阵”（cost matrix）。</p><div align=center><img src="/images/ML-pics/2-19.png" /></div><p>在非均等错误代价下，我们希望的是最小化“总体代价”，这样“代价敏感”的错误率（2.5.1节介绍）为：</p><div align=center><img src="/images/ML-pics/2-20.png" /></div>同样对于ROC曲线，在非均等错误代价下，演变成了“代价曲线”，在这里就不介绍了。<h3 id="2-3-a-name-2-3-比较检验-a">2.3 <a name='2-3'>比较检验</a></h3><p>在某种度量下取得评估结果后，我们是否可以直接比较以评判优劣？</p><p>不是这样的：</p><ul><li>测试性能不等于泛化性能</li><li>测试性能随着测试集的变化而变化</li><li>很多机器学习算法本身有一定的随机性</li></ul><p>我们的机器学习，也只是所谓的 “概率近似正确”，而不是真正的泛化。</p><p>我们可以根据数据集以及模型任务的特征，选择出最合适的评估和性能度量方法来计算出学习器的“测试误差”。但由于“测试误差”受到很多因素的影响，测试集的不同也会影响算法的评估，同时测试误差是作为泛化误差的近似，并不能代表学习器真实的泛化性能。所以我们提出了 比较检验 来对单个或多个学习器在不同或相同测试集上的性能度量结果做比较。</p><h4 id="2-3-1-a-name-2-3-1-假设检验-a">2.3.1 <a name='2-3-1'>假设检验</a></h4><p><strong>“假设”指的是对样本总体的分布或已知分布中某个参数值的一种猜想</strong>，例如：假设总体服从泊松分布，或假设正态总体的期望u=u0。回到本篇中，我们可以通过测试获得测试错误率，但直观上测试错误率和泛化错误率相差不会太远，因此可以通过测试错误率来推测泛化错误率的分布，<strong>这就是一种假设检验</strong>。</p><p><strong>统计假设检验(hypothesis test)</strong> 为学习器性能比较提供了重要依据。</p><p>学习器的性能比较常用方法如下：</p><ul><li><h4 id="2-3-1-1-a-name-2-3-1-1-两学习器比较-a">2.3.1.1 <a name='2-3-1-1'><strong>两学习器比较</strong></a></h4><ul><li>交叉验证t 检验(基于成对t 检验)</li></ul>  <div align=center>  <img src="/images/ML-pics/2-21.png" />  </div><blockquote><p>k 折交叉验证中训练集、测试集会产生重叠，可以通过5次2折交叉验证，使用第一次的两对差值计算均值，使用全部的差值对（即10对）计算方差，可以有效的避免这个问题。</p></blockquote><ul><li>McNemar 检验(基于列联表，卡方检验)</li></ul><blockquote><p>主要思想是：若两学习器的性能相同，则A预测正确B预测错误数应等于B预测错误A预测正确数，即e01=e10，且|e01-e10|服从N（1，e01+e10）分布。</p></blockquote></li><li><h4 id="2-3-1-2-a-name-2-3-1-2-多学习器比较-a">2.3.1.2 <a name='2-3-1-2'><strong>多学习器比较</strong></a></h4><ul><li><p>Friedman + Nemenyi</p><ul><li>Friedman检验(基于序值，F检验; 判断”是否都相同”)</li></ul><blockquote><p>F检验可以在多组数据集进行多个学习器性能的比较，基本思想是在同一组数据集上，根据测试结果（例：测试错误率）对学习器的性能进行排序，赋予序值1,2,3…，相同则平分序值.</p></blockquote><blockquote><p>比如：以下是三个算法 ABC 在 四个数据集上的 序值。比如 D1 中，A 最好，B 其次，C 最差。</p></blockquote>  <div align=center>  <img src="/images/ML-pics/2-22.png" />  </div><blockquote><p>若学习器的性能相同，则它们的平均序值应该相同，且第i个算法的平均序值ri服从正态分布N（（k+1）/2，（k+1）(k-1)/12），则有：</p></blockquote>  <div align=center>  <img src="/images/ML-pics/2-23.png" />  </div>  <div align=center>  <img src="/images/ML-pics/2-24.png" />  </div>  * Nemenyi 后续检验(基于序值，进一步判断两两差别)<blockquote><p>Friedman 检验检测出来所有算法都 “相同”，那么就它们的差别不显著，则需要进行 后续检验，Nemenyi 就是一个常用的方法。</p></blockquote><blockquote><p>Nemenyi检验计算出平均序值差别的临界值域，下表是常用的qa值，若两个算法的平均序值差超出了临界值域CD，则相应的置信度1-α拒绝“两个算法性能相同”的假设。</p></blockquote>  <div align=center>  <img src="/images/ML-pics/2-25.png" />  </div>  <div align=center>  <img src="/images/ML-pics/2-26.png" />  </div></li></ul></li></ul><h2 id="3-a-name-3-偏差与方差-a">3 <a name='3'>偏差与方差</a></h2><p>在 <a href="#1-3">1.3 误差与精度</a> 我们讲过误差，那么误差到底包含了哪些因素，或者说，如何从机器学习的角度来解释误差从何而来？</p><p>这里我们就要提到<strong>偏差与方差</strong>。<strong>偏差-方差分解</strong>是解释学习器泛化性能的重要工具。在学习算法中，偏差指的是预测的期望值与真实值的偏差，方差则是每一次预测值与预测值得期望之间的差均方。实际上，偏差体现了学习器预测的准确度，而方差体现了学习器预测的稳定性。</p><div align=center><img src="/images/ML-pics/2-27.png" /></div><p>但是 一般而言，偏差与方差存在冲突，这就是常说的偏差-方差窘境（bias-variance dilamma）：</p><ul><li><strong>训练不足</strong>时，学习器拟合能力不强，偏差主导了泛化错误率（欠拟合）</li><li>随着<strong>训练程度加深</strong>，学习器拟合能力逐渐增强，方差逐 渐主导了泛化错误率</li><li><strong>训练充足</strong>后，学习器的拟合能力很强，方差主导了泛化错误率（过拟合）</li></ul><div align=center><img src="/images/ML-pics/2-28.png" /></div>这也就是绪论所说的拟合的几个情况。]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> notes </tag>
            
            <tag> ML-notes </tag>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ML-notes:绪论</title>
      <link href="/2024/04/14/ML-notes/pages/introduction/"/>
      <url>/2024/04/14/ML-notes/pages/introduction/</url>
      
        <content type="html"><![CDATA[<h1 id="1-绪论">1 绪论</h1><h2 id="1-a-name-1-概念-a">1. <a name='1'>概念</a></h2><p>机器学习是从人工智能中产生的一个重要学科分支，是实现智能化的关键。</p><p><strong>机器学习</strong>是一类算法的总称，这些算法企图从大量历史数据中挖掘出其中隐含的规律，并用于预测或者分类，更具体的说，机器学习可以看作是寻找一个函数，输入是样本数据，输出是期望的结果，只是这个函数过于复杂，以至于不太方便形式化表达。也就是<strong>经典定义</strong>：<u>利用经验改善系统自身的性能。（从 经验到数据 的过程）</u></p><p>在计算机系统中，<strong>经验</strong>通常以<strong>数据</strong>形式存在，因此，机器学习研究的主要内容，是关于在计算机上<strong>从数据中产生模型</strong>的算法，即<strong>学习算法</strong>。</p><blockquote><p>“Learning is any process by which a system improves performance from experience.”<br>– Herbert Simon</p></blockquote><blockquote><p>Machine learning aims to study Approaches which improve the Performance of a machine at a specific Task with Experiences.</p></blockquote><blockquote><p>也就是说 <strong>机器学习 = 任务 + 方法 + 经验 + 性能</strong></p></blockquote><h2 id="2-a-name-2-三个概念之间的关系-a">2. <a name='2'>三个概念之间的关系</a></h2><p>当学到机器学习最容易问的就是 三个概念 之间的关系了：机器学习、深度学习、人工智能。其实关系如下图一样：</p><div align=center><img src="/images/ML-pics/1-1.png" /></div><ul><li><strong>机器学习</strong>是人工智能的一个子领域，是<strong>人工智能的核心</strong></li><li><strong>深度学习</strong>是机器学习的一个子领域，是目前最火的<strong>方向</strong></li><li><strong>机器学习是从数据通往智能的技术途径，是现代人工智能的本质</strong></li></ul><p>由机器学习也延展出很多技术：</p><div align=center><img src="/images/ML-pics/1-2.png" /></div><h2 id="3-a-name-3-taep-a">3. <a name='3'>TAEP</a></h2><h3 id="3-1-a-name-3-1-概念-a">3.1. <a name='3-1'>概念</a></h3><p>通常描述机器学习的应用用 TAEP 来描述：</p><ul><li>任务-Task：机器学习要解决的问题（研究对象）</li><li>方法-Approach：各种机器学习方法（核心内容）</li><li>经验-Experience：训练模型的数据、实例（动力源泉）</li><li>性能-Performance：方法针对任务的性能评估准则（检验指标）</li></ul><h3 id="3-2-a-name-3-2-例子-a">3.2. <a name='3-2'>例子</a></h3><ul><li>T：人脸识别<ul><li>A：线性回归</li><li>E：已标定身份的人脸图片数据</li><li>P：人脸识别准确率</li></ul></li><li>T：象棋博弈<ul><li>A: 人工神经网络</li><li>E: 指令化棋谱</li><li>P: 对随机对手的获胜比率</li></ul></li><li>T：股价预测<ul><li>A: 多项式回归</li><li>E: 不同股票近三年各交易日股价数据</li><li>P: 估价误差（方差）</li></ul></li></ul><h2 id="4-a-name-4-基本任务-a">4. <a name='4'>基本任务</a></h2><p>机器学习有四个基本任务，所有的机器学习的应用与子领域都是这几个基本任务组成的：</p><ul><li>回归（Regression）</li><li>分类（Classification）</li><li>聚类（Clustering）</li><li>表征（Representation）</li></ul><h3 id="4-1-a-name-4-1-回归-regression-a">4.1 <a name='4-1'>回归（Regression）</a></h3><p>回归，是一种分析手段，用于<strong>解决预测问题</strong>。除开逻辑回归外，它一般是用于预测连续型数据，<strong>等价于函数拟合</strong>。可以用于形状分析、表情分析、运动分析等</p><blockquote><p>就像炒菜一样，我们每次都会尝一口，然后根据与理想味道的偏差来加料或者补水，最终得到最理想的结果；下次炒菜的时候，我们就有了这么个模型，可以预测到大概需要的调料数量，得到理想的结果。也是数据（经验）到模型的过程。</p></blockquote><p>它最早源自于<strong>高尔顿和学生皮尔逊</strong>发现的一个神奇的生物遗传现象：如果父母双亲都比较高一些，那么生出的子女身高会低于父母的平均身高；反之，如果父母双亲都比较矮一些，那么生出的子女身高要高于父母平均身高。同样体重也如此，它们总是会向一个标准数值回归。</p><div align=center><img src="/images/ML-pics/1-3.png" /></div><h3 id="4-2-a-name-4-2-分类-classification-与-聚类-clustering-a">4.2 <a name='4-2'>分类（Classification）与 聚类（Clustering）</a></h3><p>分类和聚类一起讲，是因为它们其实非常像，唯一的区别就是一个是有监督学习，一个是无监督学习。（监督指的是学习时样本有无标签）</p><p><strong>分类</strong>是根据一些给定的已知类别标号的样本，训练某种学习机器（即得到某种目标函数），使它能够对未知类别的样本进行分类。</p><p>一般用于：</p><ul><li>分类（图像、视频、文本………）</li><li>识别（语音、人脸、指纹…….）</li><li>检测（行人、车辆、军事目标…….）</li></ul><div align=center><img src="/images/ML-pics/1-4.png" /></div><p><strong>聚类</strong>是给定一些无标签样本，将其分成由类似对象组成的多个类。</p><p>一般用于：</p><ul><li>分割（图像、视频）、背景建模</li><li>数据挖掘、数据恢复</li><li>字典学习（视觉信息，文本）</li></ul><div align=center><img src="/images/ML-pics/1-5.png" /></div><h3 id="4-3-a-name-4-3-表征-representation-a">4.3 <a name='4-3'>表征（Representation）</a></h3><p>表征用于特征提取，是一种将原始数据转换成为更容易被机器学习应用的数据的过程，也就是为前面三个任务解决数据的问题，它一般用于数据重构和信息检索。</p><h2 id="5-a-name-5-方法分类-a">5. <a name='5'>方法分类</a></h2><p>我们可以根据学习形式进行方法分类，就像之前提到分类和聚类的区别一样，<br>我们将学习方法分为：</p><ul><li><strong>有监督学习（Supervised Learning）</strong><ul><li>数据都有明确的标签，根据机器学习产生的模型可以将新数据分到一个明确的类或得到一个预测值。</li><li>典型：支持向量机、贝叶斯分类器、决策树、线性判别分析…….</li></ul></li><li><strong>无监督学习（Unsupervised Learning）</strong><ul><li>数据没有标签，机器学习出的模型是从数据中提取出来的模式（提取决定性特征或者聚类等）</li><li>典型：K均值、Meanshift、主成分分析、典型相关分析……</li></ul></li><li><strong>半监督学习（Semi-supervised Learning）</strong><ul><li>部分数据有明确的标签，根据机器学习产生的模型可以将新数据分到一个明确的类或得到一个预测值。</li><li>典型：图直推学习、超图直推学习……</li></ul></li></ul><h2 id="6-a-name-6-基本术语-a">6. <a name='6'>基本术语</a></h2><p>我们以西瓜书上的例子来讲：</p><blockquote><p>假设我们收集了一批西瓜的数据，例如：（色泽=青绿;根蒂=蜷缩;敲声=浊响)， (色泽=乌黑;根蒂=稍蜷;敲声=沉闷)， (色泽=浅自;根蒂=硬挺;敲声=清脆)……每对括号内是一个西瓜的记录。</p></blockquote><p>那么：</p><ul><li><strong>数据集（dataset）</strong>：所有数据的集合。（所有的西瓜数据）</li><li><strong>示例（instance）或样本（sample）</strong>：每一条数据。（某一个西瓜的数据）</li><li><strong>样例（example）</strong>：有标签的示例。（西瓜数据包括西瓜类型）</li><li><strong>特征（feature）或属性（attribute）</strong>：单个特点。属性即事物本身所固有的性质。特征即一事物异于其他事物的特点。（色泽或根缔或敲声）</li><li><strong>属性空间, 样本空间, 输入空间, 假设空间，版本空间</strong>：都是属性张成的空间，但是里面包含不同的点。</li><li><strong>假设空间</strong>：包含所有可能的假设点的空间（比如所有属性组合的西瓜数据点）。</li><li><strong>版本空间（version space）</strong>： 与训练集一致的假设集合，是通过训练集筛选过的假设空间，它随训练集而变化。</li><li><strong>特征向量</strong>：一条记录在对应空间中对应的坐标向量。（我们可以称每个西瓜记录都是一个特征向量，如（色泽=青绿;根蒂=蜷缩;敲声=浊响)）</li><li><strong>特征空间</strong>：排除线性相关和对模型构建没有益处后的属性，形成的新属性空间。</li><li><strong>维数（dimensionality）</strong>：一个样本的特征数。（该西瓜的例子维数为3）（当维数非常大时，也就是现在说的“维数灾难”）</li><li><strong>训练集（training set）</strong>：所有训练样本的集合，特殊集合。</li><li><strong>测试集（testing set）</strong>：所有测试样本的集合，一般集合。</li><li><strong>泛化（generalization）</strong>：在训练集上训练好的模型在测试集上的效果，即从特殊到一般的效果。</li><li><strong>独立同分布(i.i.d.)</strong><ul><li>每次抽样之间是没有关系的，不会相互影响</li><li>每次抽样，样本都服从同样的一个分布</li></ul></li></ul><h1 id="a-name-kw-勘误表-a"><a name='kw'>勘误表</a></h1><p>用于订正书中的错误内容</p><p><a href="https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/MLbook2016.htm">https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/MLbook2016.htm</a></p><h1 id="a-name-data-数据集-a"><a name='data'>数据集</a></h1><p><a href="http://archive.ics.uci.edu/ml/index.php">http://archive.ics.uci.edu/ml/index.php</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> notes </tag>
            
            <tag> ML-notes </tag>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ML-notes:复习</title>
      <link href="/2024/04/13/ML-notes/pages/review/"/>
      <url>/2024/04/13/ML-notes/pages/review/</url>
      
        <content type="html"><![CDATA[<p>该页仅为复习资料，内含博客链接均通过搜索得到。</p><h1 id="1-线性回归-linear-regression">1. 线性回归 Linear Regression</h1><p><a href="https://www.cnblogs.com/geo-will/p/10468253.html">https://www.cnblogs.com/geo-will/p/10468253.html</a></p><h2 id="要求1：可以按照自己的理解简述线性回归问题">要求1：可以按照自己的理解简述线性回归问题。</h2><blockquote><p>回归分析是一种预测性的建模技术，它研究的是因变量（目标）和自变量（预测器）之间的关系。线性回归是回归问题中的一种，线性回归假设目标值与特征之间线性相关，即满足一个多元一次方程。通过构建损失函数，来求解损失函数最小时的参数w和b。</p></blockquote><h2 id="要求2：可以对简单数据进行计算">要求2：可以对简单数据进行计算。</h2><blockquote><p>最小二乘法与梯度计算</p></blockquote><blockquote><p>见网页</p></blockquote><h2 id="要求3：-可以编程实现线性回归算法">要求3： 可以编程实现线性回归算法。</h2><blockquote><p>见网页</p></blockquote><h1 id="2-逻辑回归-logistic-regression">2. 逻辑回归 Logistic Regression</h1><p><a href="https://www.cnblogs.com/geo-will/p/10468356.html">https://www.cnblogs.com/geo-will/p/10468356.html</a></p><h2 id="要求1：可以按照自己的理解简述逻辑回归问题以及与线性回归问题的区别与联系">要求1：可以按照自己的理解简述逻辑回归问题以及与线性回归问题的区别与联系。</h2><blockquote><p>逻辑斯蒂回归(Logistic Regression) 虽然名字中有回归，但模型最初是为了解决二分类问题。</p><p>线性回归模型帮助我们用最简单的线性方程实现了对数据的拟合，但只实现了回归而无法进行分类。因此LR就是在线性回归的基础上，构造的一种分类模型。LR 通过一个联系函数，将预测值转化为离散值从而进行分类。对数几率引入了一个对数几率函数（logistic function）,将预测值投影到 0-1 之间，从而将线性回归问题转化为二分类问题。</p></blockquote><blockquote><p>跳转详解 <a href="/2024/04/15/ML-notes/pages/linear-model/" title="ML-notes:线性模型">3.线性模型</a></p></blockquote><h2 id="要求2：掌握梯度下降法-牛顿法的基本原理和迭代公式">要求2：掌握梯度下降法、牛顿法的基本原理和迭代公式。</h2><blockquote><p>跳转详解 <a href="/2024/04/15/ML-notes/pages/linear-model/" title="ML-notes:线性模型">3.线性模型</a></p></blockquote><h2 id="要求3：可以编程实现逻辑回归算法">要求3：可以编程实现逻辑回归算法。</h2><blockquote><p>见网页</p></blockquote><h1 id="3-决策树-decision-tree">3. 决策树 Decision Tree</h1><p><a href="https://www.cnblogs.com/geo-will/p/9773621.html">https://www.cnblogs.com/geo-will/p/9773621.html</a></p><h2 id="要求1：可以按照自己的理解简述决策树算法">要求1：可以按照自己的理解简述决策树算法。</h2><blockquote><p>简单而言，决策树是一个多层if-else函数，对对象属性进行多层if-else判断，获取目标属性的类别。由于只使用if-else对特征属性进行判断，所以一般特征属性为离散值，即使为连续值也会先进行区间离散化，如可以采用二分法（bi-partition）。</p></blockquote><h2 id="要求2：可以利用id3-c4-5-和-cart算法对数据进行分类">要求2：可以利用ID3，C4.5 和 CART算法对数据进行分类。</h2><blockquote><p>ID3 使用信息熵 Ent(D) 得到信息增益 Gain(D,a)，衡量划分属性</p></blockquote><div align=center><img src="/images/ML-pics/4-9.png" /></div><div align=center><img src="/images/ML-pics/4-10.png" /></div><blockquote><p>C4.5 使用增益率 Gain_ratio(D,a)，衡量划分属性</p></blockquote><div align=center><img src="/images/ML-pics/4-18.png" /></div><blockquote><p>CART 使用基尼值 Gini(D) 得到 基尼指数 Gini_index(D,a)，衡量划分属性</p></blockquote><div align=center><img src="/images/ML-pics/4-19.png" /></div><div align=center><img src="/images/ML-pics/4-20.png" /></div><blockquote><p>跳转详解 <a href="/2024/04/15/ML-notes/pages/decision-tree/" title="ML-notes:决策树">4.决策树</a></p></blockquote><h2 id="要求3：可以对生成的决策树进行剪枝处理">要求3：可以对生成的决策树进行剪枝处理。</h2><blockquote><p>4.4 剪枝 跳转详解 <a href="/2024/04/15/ML-notes/pages/decision-tree/" title="ML-notes:决策树">4.决策树</a></p></blockquote><blockquote><p>预剪枝 与 后剪枝</p></blockquote><div align=center><img src="/images/ML-pics/4-23.png" /></div><div align=center><img src="/images/ML-pics/4-24.png" /></div><h1 id="4-感知机-perceptron-神经网络的组成单元">4. 感知机 Perceptron —— 神经网络的组成单元</h1><h2 id="要求1：可以按照自己的理解简述感知机模型">要求1：可以按照自己的理解简述感知机模型。</h2><blockquote><p>感知机（Perceptron） ，最简单的感知机是由两层神经元组成的一个简单模型。</p></blockquote><blockquote><p>感知机是一个接收多个输入信号，输出一个信号的简单模型。它是神经网络的组成单元。</p></blockquote><blockquote><p>它的输出层是M-P神经元，即输出层神经元进行激活函数处理，也称为阈值神经单元（threshold logic unit）；也叫功能神经元。</p></blockquote><blockquote><p>输入层接受外界信号（样本属性）并传递给输出层（输入层的神经元个数等于样本的属性数目），而没有激活函数。</p></blockquote><blockquote><p>多层感知机还有一个层在输出层到输入层之间叫隐含层，隐含层类似于输出层，接收上一层的输出，通过激活函数将值传入下层。</p></blockquote><div align=center><img src="/images/ML-pics/5.3.png" /></div><h2 id="要求2：可以利用感知机解决逻辑分类问题-还不是很懂">要求2：可以利用感知机解决逻辑分类问题。(还不是很懂)</h2><div align=center><img src="/images/ML-pics/ex5.1.png" /></div><div align=center><img src="/images/ML-pics/ex5.2.png" /></div><div align=center><img src="/images/ML-pics/ex5.3.png" /></div><div align=center><img src="/images/ML-pics/ex5.4.png" /></div><h1 id="5-神经网络-neural-networks">5. 神经网络 Neural Networks</h1><p><a href="https://blog.csdn.net/qq_32865355/article/details/80260212">https://blog.csdn.net/qq_32865355/article/details/80260212</a></p><p><a href="https://blog.csdn.net/RAO_OO/article/details/77234524">https://blog.csdn.net/RAO_OO/article/details/77234524</a></p><h2 id="要求1：可以按照自己的理解简述神经网络模型-以及与感知机的关系">要求1：可以按照自己的理解简述神经网络模型，以及与感知机的关系。</h2><blockquote><p>神经网络是由具有<strong>适应性的简单单元</strong>组成的广泛<strong>并行互连的网络</strong>，它的组织能够模拟<strong>生物神经系统</strong>对真实世界物体所做的交互反应。<strong>神经网络中最基本的成分</strong>便是神经元（Neuron）模型，也就是上面说的<strong>适应性简单单元</strong>。</p></blockquote><blockquote><p>感知机由两层神经元组成，是最简单形式的前馈式人工神经网络。</p></blockquote><h2 id="要求2：掌握bp算法的基本原理和迭代公式">要求2：掌握BP算法的基本原理和迭代公式。</h2><blockquote><p>误差反向传播算法简称反向传播算法（即BP算法）。使用反向传播算法的多层感知器又称为BP神经网络。BP算法是一个迭代算法，它的基本思想为：（1）先计算每一层的状态和激活值，直到最后一层（即信号是前向传播的）；（2）计算每一层的误差，误差的计算过程是从最后一层向前推进的（这就是反向传播算法名字的由来）；（3）更新参数（目标是误差变小）。迭代前面两个步骤，直到满足停止准则（比如相邻两次迭代的误差的差别很小）。</p></blockquote><h1 id="6-支持向量机-support-vector-machine">6. 支持向量机 Support Vector Machine</h1><p><a href="https://zhuanlan.zhihu.com/p/77750026">https://zhuanlan.zhihu.com/p/77750026</a></p><p><a href="https://zhuanlan.zhihu.com/p/65487578?from_voters_page=true">https://zhuanlan.zhihu.com/p/65487578?from_voters_page=true</a></p><h2 id="要求1：可以按照自己的理解简述支持向量机模型-以及与其他分类算法的区别">要求1：可以按照自己的理解简述支持向量机模型，以及与其他分类算法的区别。</h2><blockquote><p>支持向量机的基本思想是寻找两类样本之间最中间的超平面。支持向量机的目的是使划分平面对于样本的扰动容忍性好。</p></blockquote><blockquote><p>逻辑回归算法是基于全部样本的二分类器：考虑全部样本的平均似然性。</p><p>支持向量机算法是基于部分样本的二分类器：考虑部分靠近边界的支持向量。</p></blockquote><h2 id="要求2：掌握使用拉格朗日乘子法对约束优化问题进行求解-并理解使用拉格朗日乘子法求解svm问题的原因">要求2：掌握使用拉格朗日乘子法对约束优化问题进行求解，并理解使用拉格朗日乘子法求解SVM问题的原因。</h2><blockquote><p>拉格朗日乘子法是求解约束优化问题常用的方法之一，其基本思想是求解与之等价的无约束对偶问题</p></blockquote><blockquote><p>拉格朗日乘子加入到目标函数中，有两个作用</p><ul><li>将约束函数引入到目标函数中，转化为无约束问题，不满足约束条件的解会使得目标函数无穷大，故而无解</li><li>引入拉格朗日乘子另一个最大的作用就是将约束条件与目标函数混在一起，使得我们可以同时计算目标函数的梯度与约束条件的梯度，根据相关的性质从而找到我们想找到的局部最优解或者全局最优解</li></ul></blockquote><blockquote><p>例子求解: <a href="https://blog.csdn.net/on2way/article/details/47729419">https://blog.csdn.net/on2way/article/details/47729419</a></p></blockquote><h2 id="要求3：可以按照自己的理解简述软间隔支持向量机-并分析其与常规支持向量机的关系与区别">要求3：可以按照自己的理解简述软间隔支持向量机，并分析其与常规支持向量机的关系与区别。</h2><blockquote><p>在实际应用中，完全线性可分的样本是很少的，如果遇到了不能够完全线性可分的样本，我们就有了软间隔，相比于硬间隔的苛刻条件，我们允许个别样本点出现在间隔带里面.</p></blockquote><blockquote><p>我们为每个样本引入一个松弛变量 ε，令 ε<sub>i</sub> &gt; 0 ，且：</p></blockquote><div align=center><img src="/images/ML-pics/L6.1.png" /></div><div align=center><img src="/images/ML-pics/P6.1.png" /></div><blockquote><p>和常规的 SVM 一样，软间隔只是多了个约束，SVM 和 软间隔SVM的对偶问题都有相同的目标函数。</p></blockquote><blockquote><p>特点：</p><ul><li>软间隔SVM可以对有outlier的数据分类。</li><li>软间隔SVM对偶模型与SVM对偶模型非常相似，可以用相同算法求解。</li><li>软间隔SVM模型可以看作是最小化hinge损失函数的正则化模型。</li><li>当参数C趋向无穷大时，软间隔SVM退化成普通的SVM。</li></ul></blockquote><h2 id="要求4：-了解smo算法">要求4： 了解SMO算法。</h2><blockquote><p>SMO(Sequential Minimal Optimization)，序列最小优化算法，其核心思想非常简单：每次只优化一个参数，其他参数先固定住，仅求当前这个优化参数的极值。</p></blockquote><h1 id="7-主成分分析-principle-component-analysis">7. 主成分分析 Principle Component Analysis</h1><p><a href="https://blog.csdn.net/zhongkelee/article/details/44064401">https://blog.csdn.net/zhongkelee/article/details/44064401</a></p><h2 id="要求1：可以按照自己的理解简述主成分分析算法">要求1：可以按照自己的理解简述主成分分析算法。</h2><blockquote><p>当信息维度过多时，对每个指标进行分析往往是孤立的，不是综合的，盲目的减少指标也会损失很多信息，因此我们需要在减少分析指标的同时，还要尽量减少指标包含的信息损失，达到对数据的全面分析。主成分分析 PCA 便是这样一种方法。PCA的思想是将n维特征映射到 k 维上（k &lt; n），这k维是全新的正交特征。我们称之为<strong>主成分</strong>，是重新构造出来的k维特征，而不是简单地从n维特征中去除其余n-k维特征，它包含了与去除的特征之间的关系。</p></blockquote><h2 id="要求2：可以简述pca算法的流程">要求2：可以简述PCA算法的流程。</h2><blockquote><p>整个PCA过程貌似及其简单，就是求协方差的特征值和特征向量，然后做数据转换。</p></blockquote><blockquote><p>网页例子 （1 2 3 45）： <a href="https://blog.csdn.net/zhongkelee/article/details/44064401">https://blog.csdn.net/zhongkelee/article/details/44064401</a></p></blockquote><blockquote><ul><li>Step 1: 中心化 <strong>计算均值差</strong></li><li>Step 2: 计算协方差矩阵 <strong>n*n cov</strong> 𝑪 = 𝐜𝐨𝐯 𝑨 = 𝑨𝑨𝐓</li><li>Step 3: 特征值分解</li><li>Step 4: 投影、降维</li></ul></blockquote><h2 id="要求3：核化pca与pca的相同与不同">要求3：核化PCA与PCA的相同与不同。</h2><blockquote><p>PCA是利用特征的协方差矩阵判断变量间的方差一致性，寻找出变量之间的最佳的线性组合，来代替特征，从而达到降维的目的。</p></blockquote><blockquote><p>KPCA利用核化的思想，将样本的空间映射到更高维度的空间，再利用这个更高的维度空间进行线性降维。</p></blockquote><blockquote><p>对于 KPCA 如果样本的维度是k，样本个数是n（n&gt;k），那么首先需要将样本投射到n维空间，这个n维空间是这样计算的：首先计算n个样本间的距离矩阵D（n*n），核函数F，则F(D)就是他的高维空间投射。</p></blockquote><blockquote><p>核函数还不是很懂。</p></blockquote><h1 id="8-线性判别分析-linear-discriminant-analysis">8. 线性判别分析 Linear Discriminant Analysis</h1><p><a href="https://www.cnblogs.com/pinard/p/6244265.html">https://www.cnblogs.com/pinard/p/6244265.html</a></p><h2 id="要求1：可以按照自己的理解简述线性判别分析算法-并分析其与pca之间的联系与区别">要求1：可以按照自己的理解简述线性判别分析算法，并分析其与PCA之间的联系与区别。</h2><blockquote><p>线性鉴别分析的基本思想是将高维的模式样本投影到最佳鉴别矢量空间，以达到抽取分类信息和压缩特征空间维数的效果，投影后保证模式样本在新的子空间有最大的类间距离和最小的类内距离，即模式在该空间中有最佳的可分离性。因此，它是一种有效的特征抽取方法。使用这种方法能够使投影后模式样本的类间散布矩阵最大，并且同时类内散布矩阵最小。就是说，它能够保证投影后模式样本在新的空间中有最小的类内距离和最大的类间距离，即模式在该空间中有最佳的可分离性。</p></blockquote><blockquote><p>LDA是一种监督学习的降维技术，也就是说它的数据集的每个样本是有类别输出的。这点和PCA不同。PCA是不考虑样本类别输出的无监督降维技术。</p></blockquote><blockquote><p>比较：</p><ul><li><p>思想上：</p><ul><li>PCA旨在寻找一组子坐标系（定义一个子空间）使得样本点的方差最大，即信息量保留越多。</li><li>LDA旨在寻找一组子坐标系（定义一个子空间）使得样本点类内散度小，类间散度大（Fisher Criteria）。</li></ul></li><li><p>监督性：</p><ul><li>PCA是无监督学习方法</li><li>LDA是有监督学习方法</li></ul></li><li><p>算法效率</p><ul><li>PCA效率更胜一筹</li></ul></li><li><p>子空间学习（Subspace Learning）角度：</p><ul><li>PCA与LDA都属于线性子空间学习算法（Linear Subspace Learning）。</li><li>目标都是学习一个投影矩阵𝑊 = [𝒘1, ⋯ , 𝒘𝑚]，使得样本在新坐标系上的表示具有相应特性（PCA——样本方差最大，LDA——同类样本高聚合度，不同类样本高扩散度）。</li><li>在样本空间定义一个新的子坐标系（即子空间），其每个列向量定义一个坐标轴，故此类算法均称为子空间学习算法。</li></ul></li><li><p>降维（Dimension Reduction）角度：</p><ul><li>坐标轴数目少，维度也少了</li></ul></li><li><p>特征提取（Feature Extraction）角度：</p><ul><li>样本在新坐标系下的坐标相当于样本的新特征（Feature，or Representation）</li></ul></li></ul></blockquote><h2 id="要求2：可以简述lda算法的流程">要求2：可以简述LDA算法的流程。</h2><div align=center><img src="/images/ML-pics/P8.1.png" /></div><h1 id="9-k-均值聚类-k-means-clustering">9. K-均值聚类 K-means Clustering</h1><p><a href="https://www.cnblogs.com/pinard/p/6164214.html">https://www.cnblogs.com/pinard/p/6164214.html</a><br><a href="https://www.cnblogs.com/zhxuxu/p/9860654.html">https://www.cnblogs.com/zhxuxu/p/9860654.html</a></p><h2 id="要求1：可以按照自己的理解简述k-means算法">要求1：可以按照自己的理解简述K-means算法。</h2><blockquote><p>K-Means算法的思想很简单，对于给定的样本集，按照样本之间的欧式距离大小，将样本集划分为K个簇。让簇内的点尽量紧密的连在一起，而让簇间的距离（簇中心的距离）尽量的大。</p></blockquote><h2 id="要求2：可以简述lloyd算法的流程">要求2：可以简述Lloyd算法的流程。</h2><blockquote><ul><li>Lloyd’s algorithm 过程：<ul><li>（1）首先在数据集中随机选定k个初始点</li><li>（2） 计算k个站点的Voronoi图。</li><li>（3）整合Voronoi图的每个单元格，并计算<strong>质心</strong>。</li><li>（4）然后将每个站点（k）移动到其Voronoi单元的质心。</li><li>Lloyd’s的输入是一个连续的几何区域，而不是一组离散的点。</li></ul></li></ul></blockquote><div align=center><img src="/images/ML-pics/P9.2.png" /></div><blockquote><p>而可以用Lloyd算法来启发式的求解 K-means</p></blockquote><div align=center><img src="/images/ML-pics/P9.1.png" /></div><blockquote><ul><li>原K-Means算法过程：<ul><li>（1）随机初始化k个聚类中心的位置</li><li>（2）计算每一个点到聚类中心的距离，选取最小值分配给k(i)</li><li>（3）移动聚类中心（其实就是对所属它的样本点求平均值，就是它移动是位置）</li><li>（4）重复（2），（3）直到损失函数（也就是所有样本点到其所归属的样本中心的距离的和最小）</li></ul></li></ul></blockquote><blockquote><p>原 K-means 算法是选取距离最小的样本点作为中心，而 Lloyd 来求解则为每次将质点作为新中心。</p></blockquote><blockquote><p>在 K-Means 聚类时，每个聚类簇的质心是隐含数据。假设 K 个初始化质心，即 EM 算法的 E 步；然后计算得到每个样本最近的质心，并把样本聚类到最近的这个质心，即 EM 算法的 M 步。重复这个 E 步和 M 步，直到质心不再变化为止，这样就完成了 K-Means 聚类。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> notes </tag>
            
            <tag> ML-notes </tag>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ML-notes:目录</title>
      <link href="/2024/04/12/ML-notes/ML-notes/"/>
      <url>/2024/04/12/ML-notes/ML-notes/</url>
      
        <content type="html"><![CDATA[<h1 id="ml-notes">ML-notes</h1><p>本科时期学习过的<strong>机器学习基础课程</strong>，现在重拾。本笔记主要针对清华大学出版的《机器学习》教材而作的笔记，只包括部分内容笔记，作者是南京大学的周志华教授。内容包含了我的复习笔记及其一些浅显理解，仅供借鉴。</p><blockquote><p>暂时停更 2024-04</p></blockquote><h2 id="目录-contents">目录 | CONTENTS</h2><h3 id="swig-0"><a href="/2024/04/13/ML-notes/pages/review/" title="ML-notes:复习">0 考试相关复习点</a></h3><h3 id="swig-1"><a href="/2024/04/14/ML-notes/pages/introduction/" title="ML-notes:绪论">1.绪论</a></h3><h3 id="swig-2"><a href="/2024/04/15/ML-notes/pages/model-evaluation/" title="ML-notes:模型评估与选择">2.模型评估与选择</a></h3><h3 id="swig-3"><a href="/2024/04/15/ML-notes/pages/linear-model/" title="ML-notes:线性模型">3.线性模型</a></h3><h3 id="swig-4"><a href="/2024/04/15/ML-notes/pages/decision-tree/" title="ML-notes:决策树">4.决策树</a></h3><h3 id="swig-5"><a href="/2024/04/16/ML-notes/pages/neural-network/" title="ML-notes:人工神经网络">5.神经网络</a></h3><h3 id="6-svm">6.SVM</h3><h3 id="7-pca-lda">7.PCA&amp;LDA</h3><h3 id="8-聚类">8.聚类</h3><h2 id="参考">参考</h2><p>《机器学习》周志华</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> notes </tag>
            
            <tag> ML-notes </tag>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>从agrinJPG到SapientialM</title>
      <link href="/2024/04/12/Introduction/SapientialM-s-blog/"/>
      <url>/2024/04/12/Introduction/SapientialM-s-blog/</url>
      
        <content type="html"><![CDATA[<hr><p>Greetings, dear readers!</p><p>Allow me to introduce myself – I am SapientialM, formerly known as AgrinJPG. While my moniker has changed, my passion for technology and sharing knowledge remains unwavering. During my undergraduate years, you might have stumbled upon my contributions on platforms like CSDN, cnblog, and Gitee, where I actively engaged with the vibrant tech community.</p><p>However, life has its twists and turns. After embarking on a professional journey in Shenzhen, I found myself grappling with challenges and setbacks. Yet, adversity often serves as a catalyst for growth. Thus, I’ve embraced a new chapter in my life by delving into the realm of postgraduate studies.</p><p>Now, as I embark on this fresh academic journey, I’ve resurrected my blog – a digital haven where ideas flourish and insights abound. It feels like a rebirth, a chance to reinvent myself and my pursuits.</p><p>My research focus? It lies at the intersection of artificial intelligence and single-cell sequencing. This burgeoning field holds immense promise, offering novel solutions to intricate biological puzzles and advancing our understanding of cellular dynamics.</p><p>If you share a passion for AI, genomics, or the exciting fusion of both, I extend a warm invitation to connect and converse. Let’s embark on this intellectual odyssey together, exchanging ideas, unraveling mysteries, and charting new frontiers in knowledge.</p><p>Welcome to my blog – a sanctuary for the curious, the inquisitive, and the relentless seekers of wisdom.</p><p>Until next time,<br>SapientialM</p><hr><p>From gpt</p>]]></content>
      
      
      
        <tags>
            
            <tag> 新的开始 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
