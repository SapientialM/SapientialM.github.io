<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>ML-notes:决策树 | SapientialM's Blog |  CM 的学习历程</title><meta name="author" content="SapientialM"><meta name="copyright" content="SapientialM"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="4 决策树4.1 基本概念4.1.1 举例子多分类问题实质上通过划分的方法转化为多个二分类问题进行求解。这次我们将讨论另一种被广泛使用的分类算法–决策树（Decision Tree）。 比如 一个相亲——母女对话：  女儿：多大年纪了？ 母亲：26。  女儿：长的帅不帅？ 母亲：挺帅的。 女儿：收入高不？ 母亲：不算很高，中等情况。 女儿：是公务员不？ 母亲：是，在税务局上班呢。 女儿：那好，我去">
<meta property="og:type" content="article">
<meta property="og:title" content="ML-notes:决策树">
<meta property="og:url" content="https://sapientialm.github.io/2024/04/15/ML-notes/pages/decision-tree/index.html">
<meta property="og:site_name" content="SapientialM&#39;s Blog |  CM 的学习历程">
<meta property="og:description" content="4 决策树4.1 基本概念4.1.1 举例子多分类问题实质上通过划分的方法转化为多个二分类问题进行求解。这次我们将讨论另一种被广泛使用的分类算法–决策树（Decision Tree）。 比如 一个相亲——母女对话：  女儿：多大年纪了？ 母亲：26。  女儿：长的帅不帅？ 母亲：挺帅的。 女儿：收入高不？ 母亲：不算很高，中等情况。 女儿：是公务员不？ 母亲：是，在税务局上班呢。 女儿：那好，我去">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://avatars.githubusercontent.com/u/55795402?v=4">
<meta property="article:published_time" content="2024-04-15T09:08:52.000Z">
<meta property="article:modified_time" content="2024-04-16T05:58:46.320Z">
<meta property="article:author" content="SapientialM">
<meta property="article:tag" content="ML-notes">
<meta property="article:tag" content="ML">
<meta property="article:tag" content="notes">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://avatars.githubusercontent.com/u/55795402?v=4"><link rel="shortcut icon" href="/img/github.jpg"><link rel="canonical" href="https://sapientialm.github.io/2024/04/15/ML-notes/pages/decision-tree/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.12.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.0/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'ML-notes:决策树',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-04-16 13:58:46'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.0.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://avatars.githubusercontent.com/u/55795402?v=4" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">8</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><hr class="custom-hr"/></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="SapientialM's Blog |  CM 的学习历程"><img class="site-icon" src="https://avatars.githubusercontent.com/u/55795402?v=4"/><span class="site-name">SapientialM's Blog |  CM 的学习历程</span></a></span><div id="menus"><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">ML-notes:决策树</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-04-15T09:08:52.000Z" title="发表于 2024-04-15 17:08:52">2024-04-15</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-04-16T05:58:46.320Z" title="更新于 2024-04-16 13:58:46">2024-04-16</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="ML-notes:决策树"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="4-决策树"><a href="#4-决策树" class="headerlink" title="4 决策树"></a>4 决策树</h1><h2 id="4-1-基本概念"><a href="#4-1-基本概念" class="headerlink" title="4.1 基本概念"></a>4.1 <a name='4.1'>基本概念</a></h2><h3 id="4-1-1-举例子"><a href="#4-1-1-举例子" class="headerlink" title="4.1.1 举例子"></a>4.1.1 <a name='4.1.1'>举例子</a></h3><p>多分类问题实质上通过划分的方法转化为多个二分类问题进行求解。这次我们将讨论另一种被广泛使用的分类算法–<strong>决策树（Decision Tree）</strong>。</p>
<p>比如 一个相亲——母女对话：</p>
<ul>
<li>女儿：多大年纪了？</li>
<li>母亲：26。 </li>
<li>女儿：长的帅不帅？</li>
<li>母亲：挺帅的。</li>
<li>女儿：收入高不？</li>
<li>母亲：不算很高，中等情况。</li>
<li>女儿：是公务员不？</li>
<li>母亲：是，在税务局上班呢。</li>
<li>女儿：那好，我去见见。<blockquote>
<p>此例子纯属虚构，不代表广大女性同胞的择偶标准。如有雷同纯属巧合。<br>我们就可以通过这段对话，画出一个决策树。</p>
</blockquote>
<div align=center>
<img src="/img/pics/4-2.png" />
</div></li>
</ul>
<h3 id="4-1-2-决策树"><a href="#4-1-2-决策树" class="headerlink" title="4.1.2 决策树"></a>4.1.2 <a name='4.1.2'>决策树</a></h3><p><strong>决策树（decision tree）</strong>：是构建出的一个基于属性的树形<br>分类器。</p>
<ul>
<li>每个非叶节点表示一个特征属性上的测试（分割）（判断）</li>
<li>每个分支代表这个特征属性在某个值域上的输出（分支）</li>
<li>每个叶节点存放一个类别（结果）</li>
</ul>
<p>使用决策树进行决策的<strong>过程</strong>就是从<strong>根节点开始</strong>，<strong>测试</strong>待分类项中相应的<strong>特征属性</strong>，并按照其值<strong>选择输出分支</strong>，直到<strong>到达叶子节点</strong>，将叶子节点存放的类别作为<strong>决策结果</strong>。</p>
<h2 id="4-2-决策树的构建"><a href="#4-2-决策树的构建" class="headerlink" title="4.2 决策树的构建"></a>4.2 <a name='4.2'>决策树的构建</a></h2><p>决策树的构建采用<strong>分治法</strong>的思想（递归）。而结束递归的条件如下：</p>
<ul>
<li>① 当前结点样本均属于同一类别，无需划分。（只有一种结果）<blockquote>
<p>Example: 下一个要划分的属性为属性1，显然无论属性为何种，均为 P 类无需划分。</p>
<div align=center>
<img src="/img/pics/4-3.png" />
</div></blockquote>
</li>
<li>② 当前属性集为空。 （没有下一个结点）<blockquote>
<p>Example: 属性1(B)→属性2(A)→属性3(A) 走完该路径已经无属性往下分</p>
<div align=center>
<img src="/img/pics/4-4.png" />
</div></blockquote>
</li>
<li>③ 所有样本在当前属性集上取值相同，无法划分。（每个结点只有一条分支）<blockquote>
<p>Example: 属性1 的 B分支下，样本子集中所有样本属性值完全一样，再往下划分就没有意义了。</p>
<div align=center>
<img src="/img/pics/4-5.png" />
</div></blockquote>
</li>
<li>④ 当前结点包含的样本集合为空，不能划分。<blockquote>
<p>Example: 属性 1 → 属性 2 的分支下只有 A ，无其他子集。</p>
<div align=center>
<img src="/img/pics/4-6.png" />
</div></blockquote>
</li>
</ul>
<p><strong>伪代码：</strong></p>
<div align=center>
<img src="/img/pics/4-7.png" />
</div>

<h2 id="4-3-决策树的核心"><a href="#4-3-决策树的核心" class="headerlink" title="4.3 决策树的核心"></a>4.3 <a name='4.3'>决策树的核心</a></h2><p>决策树的核心是：<strong>如何选取最佳划分属性</strong>。</p>
<p>比如我们举一个极端的例子：</p>
<div align=center>
<img src="/img/pics/4-8.png" />
</div>
很显然，其实根据属性 3 就可以判断出 正负例 了。那加入其他的属性进行判断就显得多余了。

<h3 id="4-3-1-最佳划分"><a href="#4-3-1-最佳划分" class="headerlink" title="4.3.1 最佳划分"></a>4.3.1 <a name='4.3.1'>最佳划分</a></h3><h4 id="最佳划分属性"><a href="#最佳划分属性" class="headerlink" title="最佳划分属性"></a>最佳划分属性</h4><p>我们希望经过属性划分后，不同类样本被更好的分离。</p>
<ul>
<li><strong>理想情况</strong>：划分后样本被完美分类。即划分到每个分支的样本都属于同一类。</li>
<li><strong>实际情况</strong>：不可能完美划分！尽量使得每个分支某一类样本比例尽量高！即尽量提高划分后<strong>子集的纯度（purity）</strong>。</li>
</ul>
<h4 id="最佳划分目标"><a href="#最佳划分目标" class="headerlink" title="最佳划分目标"></a>最佳划分目标</h4><ul>
<li>提升划分后子集的纯度</li>
<li>降低划分后子集的不纯度</li>
</ul>
<p>因此下面便是介绍<strong>量化纯度</strong>的具体方法，决策树最常用的算法有三种：ID3，C4.5和CART。</p>
<h3 id="4-3-2-ID3-决策树算法"><a href="#4-3-2-ID3-决策树算法" class="headerlink" title="4.3.2 ID3 决策树算法"></a>4.3.2 <a name='4.3.2'>ID3 决策树算法</a></h3><p>ID3 算法使用信息增益为准则来选择划分属性，“<strong>信息熵</strong>”(information entropy)是度量样本结合纯度的常用指标，所以我们先介绍信息熵。</p>
<h4 id="信息熵"><a href="#信息熵" class="headerlink" title="信息熵"></a>信息熵</h4><p>假定当前样本集合 D 中第k类样本所占比例为 p<sub>k</sub>，则样本集合D的信息熵定义为：</p>
<div align=center>
<img src="/img/pics/4-9.png" />
</div>

<blockquote>
<p>Ent(D)的值越小，则 D 的纯度越高</p>
</blockquote>
<p>假定离散属性 a 有 V 个可能的取值{a<sup>1</sup>,a<sup>2</sup>…,a<sup>V</sup>}。若使用 a 来对样本集 D 进行划分，那么会产生 V 个分支结点，其中 D<sup>v</sup> 表示第 v 个分支结点包含了 D 中所有在属性 a 上取值为 a<sup>v</sup>的样本数。然后呢，我们可以<strong>根据上面的式子计算出 D<sup>v</sup> 的信息熵</strong>，再考虑到不同分支结点包含的样本数的不同，我们还需要对<strong>分支结点赋予权重 |D<sup>v</sup>|&#x2F;|D|<strong>，即</strong>样本数越多的分支结点的影响越大</strong>，所以就计算出用属性 a 对样本集 D 进行划分所得到的 <strong>信息增益</strong>:</p>
<div align=center>
<img src="/img/pics/4-10.png" />
</div>
信息增益越大，表示使用该属性划分样本集D的效果越好，因此ID3算法在递归过程中，每次选择最大信息增益的属性作为当前的划分属性。也就是每次可以得到最佳划分属性。

<h4 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h4><div align=center>
<img src="/img/pics/4-11.png" />
</div>

<div align=center>
<img src="/img/pics/4-12.png" />
</div>

<div align=center>
<img src="/img/pics/4-13.png" />
</div>

<div align=center>
<img src="/img/pics/4-14.png" />
</div>

<div align=center>
<img src="/img/pics/4-15.png" />
</div>

<div align=center>
<img src="/img/pics/4-16.png" />
</div>

<h3 id="4-3-3-C4-5-算法"><a href="#4-3-3-C4-5-算法" class="headerlink" title="4.3.3 C4.5 算法"></a>4.3.3 <a name='4.3.3'>C4.5 算法</a></h3><h4 id="提出"><a href="#提出" class="headerlink" title="提出"></a>提出</h4><p><strong>ID3算法存在一个问题，就是偏向于取值数目较多的属性</strong></p>
<blockquote>
<p>例如：在学生成绩分类中，考虑学号为一个属性，然而学号基本每个人都有，所以得到得纯度很高，但是对分类毫无用处</p>
<div align=center>
<img src="/img/pics/4-17.png" />
</div>
</blockquote>
<p>因此又提出了 <strong>C4.5算法</strong> 通过 <strong>“增益率”（gain ratio）</strong> 来选择划分属性，来避免这个问题带来的困扰。</p>
<h4 id="增益率"><a href="#增益率" class="headerlink" title="增益率"></a>增益率</h4><p>首先使用ID3算法计算出<strong>信息增益</strong>高于平均水平的候选属性，接着C4.5计算这些候选属性的增益率，<strong>增益率</strong> Gain_ratio 定义为：</p>
<div align=center>
<img src="/img/pics/4-18.png" />
</div>

<p>我们称上图的 IV(a) 为属性 a 的 “固有值”。属性 a 的可能取值越多（V 越多），则 IV(a) 通常会越大。</p>
<p>C4.5 算法不是直接选择增益值最大的候选划分属性，而是类似于一种启发式算法：先从划分属性中找到信息增益高于平均水平的属性，再计算它们的增益率，从增益率内找最高的。</p>
<h3 id="4-3-4-CART-算法"><a href="#4-3-4-CART-算法" class="headerlink" title="4.3.4 CART 算法"></a>4.3.4 <a name='4.3.4'>CART 算法</a></h3><p>CART 是另一种决策树算法，使用 <strong>“基尼指数”</strong> 来选择划分属性。采用与 Ent(D) 信息熵计算相同的符号p<sub>k</sub>，数据集 D 的纯度可以用 <strong>基尼值</strong> 来度量：</p>
<div align=center>
<img src="/img/pics/4-19.png" />
</div>
直观的说，Gini(D) 反映了从数据集 D 中随机抽取两个样本，其类别标记不一致的概率。因此，Gini(D) 越小，则数据集 D 的纯度最高。
<br>
<br>

<p>同样的,基尼值对应信息熵，CART 的<strong>基尼指数</strong>也对应信息增益：</p>
<div align=center>
<img src="/img/pics/4-20.png" />
</div>

<blockquote>
<p>Gini_index 和 Gain_ratio 计算过程一样。CART 和 ID3 算法的区别在于 Gini 和 Gain 的区别。一个是 p<sub>k</sub>log<sub>2</sub>p<sub>k</sub> 一个是 p<sub>k</sub>p<sub>k</sub>‘</p>
</blockquote>
<p>研究表明: 划分选择的各种准则虽然对决策树的尺寸有较大影响，但对泛化性能的影响很有限。（也就是其实是更改准则对模型的效果影响有限）</p>
<blockquote>
<p>例如信息增益与基尼指数产生的结果，仅在约 2% 的情况下不同。</p>
</blockquote>
<p>而剪枝方法和程度对决策树泛化性能的影响更为显著。</p>
<blockquote>
<p>在数据带噪时甚至可能将泛化性能提升 25%。</p>
</blockquote>
<h2 id="4-4-剪枝处理"><a href="#4-4-剪枝处理" class="headerlink" title="4.4 剪枝处理"></a>4.4 <a name='4.4'>剪枝处理</a></h2><h3 id="4-4-1-概念"><a href="#4-4-1-概念" class="headerlink" title="4.4.1 概念"></a>4.4.1 <a name='4.4.1'>概念</a></h3><p>从决策树的构造流程中我们可以直观地看出：不管怎么样的训练集，决策树总是能很好地将各个类别分离开来，这时就会遇到之前提到过的问题：过拟合（overfitting），即太依赖于训练样本。<strong>剪枝</strong>（pruning）顾名思义，将多余的分支剪掉，是决策树算法<strong>对付过拟合</strong>的主要手段，剪枝的策略有两种如下：</p>
<blockquote>
<ul>
<li>预剪枝（prepruning）：在构造的过程中先评估，再考虑是否分支。具体是 若当前结点向下划分不能提升决策树泛化性能，则进行裁剪，把结点标记为叶结点，不再向下分支。也就是评估分支与不分支的性能进行比较。</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>后剪枝（post-pruning）：在构造好一颗完整的决策树后，自底向上，评估分支的必要性。具体是 自底向上，判断结点对应的子树被替换为叶结点是否能提升决策树泛化能力，是则进行裁剪（将该结点的子树替换为叶结点）。（如果不懂看这里：其实就是从下向上，看我这一层要是成了叶结点是否会更好，更好则裁剪）</li>
</ul>
</blockquote>
<h3 id="4-4-2-性能度量"><a href="#4-4-2-性能度量" class="headerlink" title="4.4.2 性能度量"></a>4.4.2 <a name='4.4.2'>性能度量</a></h3><p>显然，问题就来了，我怎么知道更好呢？</p>
<p>之前学的<a href="./%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%B8%8E%E9%80%89%E6%8B%A9.md">模型评估</a>就有用了，其实就是所谓的<strong>性能度量</strong>。之前讲过的 <strong>验证集</strong> 在这里也就有了用处。</p>
<h4 id="例子-1"><a href="#例子-1" class="headerlink" title="例子"></a>例子</h4><p>书上是使用的留用法，预留一部分数据作为 验证集 来进行性能评估。先看西瓜数据集：</p>
<div align=center>
<img src="/img/pics/4-21.png" />
</div>

<p>我们随机将其划分为了两部分，一部分是训练集，一部分是验证集。我们通过<strong>信息增益准则</strong>可以进行属性划分选择，生成一个<strong>未剪枝</strong>的决策树：</p>
<div align=center>
<img src="/img/pics/4-22.png" />
</div>

<p><strong>开始了啊！</strong></p>
<h4 id="预剪枝"><a href="#预剪枝" class="headerlink" title="预剪枝"></a>预剪枝</h4><div align=center>
<img src="/img/pics/4-23.png" />
</div>

<p>预剪枝，每次选定属性后，要进行一次评估。图 4.6 就是第一次评估。我首先默认，不作划分的话都是好瓜。对于 ① 脐部，划分前，经过<strong>它所包含的验证集</strong>进行验证，准确率 42.9%，划分后 71.4%，显然需要划分。然后同样的步骤，对于下一层，当然每次划分都需要经过 ① 脐部划分之后再进行，比较验证集精度来进行评估。</p>
<h4 id="后剪枝"><a href="#后剪枝" class="headerlink" title="后剪枝"></a>后剪枝</h4><div align=center>
<img src="/img/pics/4-22.png" />
</div>
<div align=center>
<img src="/img/pics/4-24.png" />
</div>

<p>后剪枝就直接对 4-2 的未剪枝的决策树进行操作。从纹理层，也就是 ⑥ 开始操作，我们将它剪去，默认分支结果为好瓜，使用<strong>它所包含的验证集</strong>的{1,2,3,14}测试，看模型精度是否会更好。后面的也是一样的了。</p>
<blockquote>
<p>注意：每次使用验证集都是直接使用它所包含的验证集的样本，而非所有，因为包含其它分支的样本进行测试对结果无影响。</p>
</blockquote>
<h4 id="比较"><a href="#比较" class="headerlink" title="比较"></a>比较</h4><ul>
<li>时间开销：<ul>
<li>预剪枝：训练时间开销降低（不需要生成一次未剪枝的决策树），测试时间开销降低</li>
<li>后剪枝：训练时间开销增加（需要生成一次未剪枝的决策树），测试时间开销降低</li>
</ul>
</li>
<li>过&#x2F;欠拟合风险：<ul>
<li>预剪枝：过拟合风险降低，欠拟合风险增加（没有生成过未剪枝的决策树，剪枝之前得到的信息不全面）</li>
<li>后剪枝：过拟合风险降低，欠拟合风险基本不变</li>
</ul>
</li>
<li>泛化性能：后剪枝 通常优于 预剪枝</li>
</ul>
<h2 id="4-5-连续值与缺失值的处理"><a href="#4-5-连续值与缺失值的处理" class="headerlink" title="4.5 连续值与缺失值的处理"></a>4.5 <a name='4.5'>连续值与缺失值的处理</a></h2><h3 id="4-5-1-连续值"><a href="#4-5-1-连续值" class="headerlink" title="4.5.1 连续值"></a>4.5.1 <a name='4.5.1'>连续值</a></h3><p>对于连续值的属性，若每个取值作为一个分支则显得不可行，因此需要进行离散化处理，常用的方法为二分法，基本思想为：给定样本集 D 与连续属性 α ，二分法试图找到一个划分点 t 将样本集 D 在属性 α 上分为 &lt;&#x3D;t 与 &gt; t (也就是划分出一个区间)。</p>
<p>二分法：</p>
<ul>
<li>n 个属性值可形成 n‐1 个候选划分</li>
<li>然后即可将它们当做 n‐1 个离散属性值处理</li>
<li>处理流程：<blockquote>
<ul>
<li><p>首先将 α 的所有取值按升序排列，所有<strong>相邻属性的均值作为候选划分点</strong>（n-1个，n为α所有的取值数目）。比如一共五个属性值，则如下：</p>
<div align=center>
<img src="/img/pics/line-1.png" />
</div>
</li>
<li><p>计算每一个划分点作为二分点后的信息增益。</p>
<div align=center>
<img src="/img/pics/line-2.png" />
</div>
</li>
<li><p>选择最大信息增益的划分点作为最优划分点。</p>
</li>
</ul>
</blockquote>
</li>
</ul>
<h3 id="4-5-2-缺失值"><a href="#4-5-2-缺失值" class="headerlink" title="4.5.2 缺失值"></a>4.5.2 <a name='4.5.2'>缺失值</a></h3><p>现实应用中，我们也会遇到属性值“缺失”(missing)的现象。如果不使用有缺失属性的样例训练，那将是巨大的浪费。那么样本的属性缺失该如何划分？</p>
<p>因此在属性值缺失的情况下需要解决两个问题：</p>
<ul>
<li>（1）如何选择划分属性。</li>
<li>（2）给定划分属性，若某样本在该属性上缺失值，如何划分到具体的分支上。</li>
</ul>
<p>我们提出了：<strong>样本赋权，权重划分</strong></p>
<h4 id="例子-2"><a href="#例子-2" class="headerlink" title="例子"></a>例子</h4><p>我们给出了含缺失值的西瓜数据集：</p>
<div align=center>
<img src="/img/pics/4-25.png" />
</div>


<p>假定为样本集 D 中的每一个样本 x 都赋予一个权重 w<sub>x</sub>，同时我们规定，样本集 D 中对于 属性 α 没有缺失值的集合为 <img src="/img/pics/D_.png" /> ，划分到达根节点中时的权重初始化为1，则定义：</p>
<div align=center>
<img src="/img/pics/4-26.png" />
</div>

<p>为了<strong>解决问题 （1）</strong>，我们通过在样本集 D 中选取在属性α上<strong>没有缺失值的样本子集</strong>，计算了在该样本子集上的信息增益，**该信息增益推广为等于样本子集占样本集的<u>比重</u>乘以无缺失样本子集划分后<u>信息增益</u>**。即：</p>
<div align=center>
<img src="/img/pics/4-28.png" />
</div>
<div align=center>
<img src="/img/pics/4-27.png" />
</div>

<p>然后为了<strong>解决问题（2）</strong>，若该样本子集在属性α上的值<strong>缺失</strong>，则将该样本以不同的权重（即每个分支所含<strong>无缺失样本比例</strong>）划入到所有分支节点中。该样本在分支节点中的权重变为：</p>
<div align=center>
<img src="/img/pics/4-29.png" />
</div>

<blockquote>
<p>也就是说，缺失值的话，会将该样本划入所有子结点，但是不是 以一个单位来划分，而是以权值划分。这是针对训练集的划分。</p>
</blockquote>
<h2 id="4-6-单变量与多变量决策树"><a href="#4-6-单变量与多变量决策树" class="headerlink" title="4.6 单变量与多变量决策树"></a>4.6 <a name='4.6'>单变量与多变量决策树</a></h2><p>决策树也分单变量与多变量决策树。</p>
<h3 id="4-6-1-单变量决策树"><a href="#4-6-1-单变量决策树" class="headerlink" title="4.6.1 单变量决策树"></a>4.6.1 <a name='4.6.1'>单变量决策树</a></h3><p>单变量决策树：在每个非叶结点仅考虑一个划分属性。</p>
<p>但是只是轴平行划分，也就是下图右方那样，形成简单的平行于轴的划分边界。</p>
<div align=center>
<img src="/img/pics/4-30.png" />
</div>
当学习任务所对应的分类边界很复杂时，需要非常多段划分才能获得较好的近似，单变量就不再适用：
<div align=center>
<img src="/img/pics/4-31.png" />
</div>

<h3 id="4-6-2-多变量决策树"><a href="#4-6-2-多变量决策树" class="headerlink" title="4.6.2 多变量决策树"></a>4.6.2 <a name='4.6.2'>多变量决策树</a></h3><p>于是便有了多变量决策树。</p>
<p>多变量决策树：每个非叶结点不仅考虑一个属性</p>
<p>例如“斜决策树” (oblique decision tree) 不是为每个非叶结点寻找最优划分属性，而是建立一个线性分类器。</p>
<div align=center>
<img src="/img/pics/4-32.png" />
</div>



<h2 id="4-7-思考"><a href="#4-7-思考" class="headerlink" title="4.7 思考"></a>4.7 <a name='4.7'>思考</a></h2><div align=center>
<img src="/img/pics/4-33.png" />
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://sapientialm.github.io">SapientialM</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://sapientialm.github.io/2024/04/15/ML-notes/pages/decision-tree/">https://sapientialm.github.io/2024/04/15/ML-notes/pages/decision-tree/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://sapientialm.github.io" target="_blank">SapientialM's Blog |  CM 的学习历程</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/ML-notes/">ML-notes</a><a class="post-meta__tags" href="/tags/ML/">ML</a><a class="post-meta__tags" href="/tags/notes/">notes</a></div><div class="post_share"><div class="social-share" data-image="https://avatars.githubusercontent.com/u/55795402?v=4" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/04/16/ML-notes/pages/neural-network/" title="ML-notes:人工神经网络"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">ML-notes:人工神经网络</div></div></a></div><div class="next-post pull-right"><a href="/2024/04/15/ML-notes/pages/linear-model/" title="ML-notes:线性模型"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">ML-notes:线性模型</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/04/12/ML-notes/ML-notes/" title="ML-notes:目录"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-12</div><div class="title">ML-notes:目录</div></div></a></div><div><a href="/2024/04/14/ML-notes/pages/introduction/" title="ML-notes:绪论"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-14</div><div class="title">ML-notes:绪论</div></div></a></div><div><a href="/2024/04/15/ML-notes/pages/linear-model/" title="ML-notes:线性模型"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-15</div><div class="title">ML-notes:线性模型</div></div></a></div><div><a href="/2024/04/16/ML-notes/pages/neural-network/" title="ML-notes:人工神经网络"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-16</div><div class="title">ML-notes:人工神经网络</div></div></a></div><div><a href="/2024/04/15/ML-notes/pages/model-evaluation/" title="ML-notes:模型评估与选择"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-15</div><div class="title">ML-notes:模型评估与选择</div></div></a></div><div><a href="/2024/04/13/ML-notes/pages/review/" title="ML-notes:复习"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-13</div><div class="title">ML-notes:复习</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://avatars.githubusercontent.com/u/55795402?v=4" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">SapientialM</div><div class="author-info__description">SapientialM 的技术博客</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">8</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/SapientialM"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/SapientialM" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#4-%E5%86%B3%E7%AD%96%E6%A0%91"><span class="toc-number">1.</span> <span class="toc-text">4 决策树</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#4-1-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="toc-number">1.1.</span> <span class="toc-text">4.1 基本概念</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-1-%E4%B8%BE%E4%BE%8B%E5%AD%90"><span class="toc-number">1.1.1.</span> <span class="toc-text">4.1.1 举例子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-2-%E5%86%B3%E7%AD%96%E6%A0%91"><span class="toc-number">1.1.2.</span> <span class="toc-text">4.1.2 决策树</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-2-%E5%86%B3%E7%AD%96%E6%A0%91%E7%9A%84%E6%9E%84%E5%BB%BA"><span class="toc-number">1.2.</span> <span class="toc-text">4.2 决策树的构建</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-3-%E5%86%B3%E7%AD%96%E6%A0%91%E7%9A%84%E6%A0%B8%E5%BF%83"><span class="toc-number">1.3.</span> <span class="toc-text">4.3 决策树的核心</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-1-%E6%9C%80%E4%BD%B3%E5%88%92%E5%88%86"><span class="toc-number">1.3.1.</span> <span class="toc-text">4.3.1 最佳划分</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9C%80%E4%BD%B3%E5%88%92%E5%88%86%E5%B1%9E%E6%80%A7"><span class="toc-number">1.3.1.1.</span> <span class="toc-text">最佳划分属性</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9C%80%E4%BD%B3%E5%88%92%E5%88%86%E7%9B%AE%E6%A0%87"><span class="toc-number">1.3.1.2.</span> <span class="toc-text">最佳划分目标</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-2-ID3-%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95"><span class="toc-number">1.3.2.</span> <span class="toc-text">4.3.2 ID3 决策树算法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BF%A1%E6%81%AF%E7%86%B5"><span class="toc-number">1.3.2.1.</span> <span class="toc-text">信息熵</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BE%8B%E5%AD%90"><span class="toc-number">1.3.2.2.</span> <span class="toc-text">例子</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-3-C4-5-%E7%AE%97%E6%B3%95"><span class="toc-number">1.3.3.</span> <span class="toc-text">4.3.3 C4.5 算法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8F%90%E5%87%BA"><span class="toc-number">1.3.3.1.</span> <span class="toc-text">提出</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A2%9E%E7%9B%8A%E7%8E%87"><span class="toc-number">1.3.3.2.</span> <span class="toc-text">增益率</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-4-CART-%E7%AE%97%E6%B3%95"><span class="toc-number">1.3.4.</span> <span class="toc-text">4.3.4 CART 算法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-4-%E5%89%AA%E6%9E%9D%E5%A4%84%E7%90%86"><span class="toc-number">1.4.</span> <span class="toc-text">4.4 剪枝处理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-1-%E6%A6%82%E5%BF%B5"><span class="toc-number">1.4.1.</span> <span class="toc-text">4.4.1 概念</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-2-%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F"><span class="toc-number">1.4.2.</span> <span class="toc-text">4.4.2 性能度量</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BE%8B%E5%AD%90-1"><span class="toc-number">1.4.2.1.</span> <span class="toc-text">例子</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%A2%84%E5%89%AA%E6%9E%9D"><span class="toc-number">1.4.2.2.</span> <span class="toc-text">预剪枝</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%90%8E%E5%89%AA%E6%9E%9D"><span class="toc-number">1.4.2.3.</span> <span class="toc-text">后剪枝</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%AF%94%E8%BE%83"><span class="toc-number">1.4.2.4.</span> <span class="toc-text">比较</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-5-%E8%BF%9E%E7%BB%AD%E5%80%BC%E4%B8%8E%E7%BC%BA%E5%A4%B1%E5%80%BC%E7%9A%84%E5%A4%84%E7%90%86"><span class="toc-number">1.5.</span> <span class="toc-text">4.5 连续值与缺失值的处理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5-1-%E8%BF%9E%E7%BB%AD%E5%80%BC"><span class="toc-number">1.5.1.</span> <span class="toc-text">4.5.1 连续值</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5-2-%E7%BC%BA%E5%A4%B1%E5%80%BC"><span class="toc-number">1.5.2.</span> <span class="toc-text">4.5.2 缺失值</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BE%8B%E5%AD%90-2"><span class="toc-number">1.5.2.1.</span> <span class="toc-text">例子</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-6-%E5%8D%95%E5%8F%98%E9%87%8F%E4%B8%8E%E5%A4%9A%E5%8F%98%E9%87%8F%E5%86%B3%E7%AD%96%E6%A0%91"><span class="toc-number">1.6.</span> <span class="toc-text">4.6 单变量与多变量决策树</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-6-1-%E5%8D%95%E5%8F%98%E9%87%8F%E5%86%B3%E7%AD%96%E6%A0%91"><span class="toc-number">1.6.1.</span> <span class="toc-text">4.6.1 单变量决策树</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-6-2-%E5%A4%9A%E5%8F%98%E9%87%8F%E5%86%B3%E7%AD%96%E6%A0%91"><span class="toc-number">1.6.2.</span> <span class="toc-text">4.6.2 多变量决策树</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-7-%E6%80%9D%E8%80%83"><span class="toc-number">1.7.</span> <span class="toc-text">4.7 思考</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/04/16/ML-notes/pages/neural-network/" title="ML-notes:人工神经网络">ML-notes:人工神经网络</a><time datetime="2024-04-16T00:09:52.000Z" title="发表于 2024-04-16 08:09:52">2024-04-16</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/04/15/ML-notes/pages/decision-tree/" title="ML-notes:决策树">ML-notes:决策树</a><time datetime="2024-04-15T09:08:52.000Z" title="发表于 2024-04-15 17:08:52">2024-04-15</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/04/15/ML-notes/pages/linear-model/" title="ML-notes:线性模型">ML-notes:线性模型</a><time datetime="2024-04-15T05:10:52.000Z" title="发表于 2024-04-15 13:10:52">2024-04-15</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/04/15/ML-notes/pages/model-evaluation/" title="ML-notes:模型评估与选择">ML-notes:模型评估与选择</a><time datetime="2024-04-15T00:36:52.000Z" title="发表于 2024-04-15 08:36:52">2024-04-15</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/04/14/ML-notes/pages/introduction/" title="ML-notes:绪论">ML-notes:绪论</a><time datetime="2024-04-14T05:10:52.000Z" title="发表于 2024-04-14 13:10:52">2024-04-14</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By SapientialM</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.12.0"></script><script src="/js/main.js?v=4.12.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>